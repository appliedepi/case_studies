---
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  error: false
format: 
  html: 
    css: webex.css
    include-after-body: webex.js
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Linking and analysing notification data and laboratory data in R {#multi_disease_lab}

::: {.callout-note appearance="minimal" icon="false"}
**Tool**: R \| **Technical complexity**: Intermediate \|
**Methodological complexity**: Basic \| **Prior knowledge required:** [R
basics](https://epirhandbook.com/en/new_pages/basics.html) (Using
Rstudio; R packages, functions and arguments, using pipes) as well as key tidyverse functions and ggplots) \| **Source:** Applied Epi, with technical support provided by the CDC Global Surveillance, Laboratory, and Data Systems Branch in collaboration with TEPHINET. 
:::

For instructions on how to use our case studies, see our [How-to
Guide](instructions.html). We welcome feedback and suggestions via
[contact\@appliedepi.org](mailto:contact@appliedepi.org). You can also
discuss the case study or related concepts on the [Applied Epi
Community](https://community.appliedepi.org/).

\pagebreak

## Scenario

You are an epidemiologist working in the national surveillance office of Feveria, a very small tropical country. There are three districts within Feveria:

-   **Feveria Central**: an over-populated urban area, with sometimes unreliable
    water and sanitation infrastructure.
-   **Lake Minara**: a lake area with good infrastructure but many
    mosquitoes in the warmer months of the year.
-   **Kasara**: a more sub-urban area on the other side of Feveria Central.

**Map of districts in the country Feveria**

![](/images/multi_disease_lab/map.png){width="70%"}

It is January 2025, and your supervisor would like you to transfer the routine
processing of notifiable disease data from Excel into R, and to conduct some analyses on the data. She wants to know at least:

- How many suspected cases of the different notifiable diseases were reported in 2024, and which was most common?
- What percentage of them ended up being confirmed?
- How many confirmed cases of different notifiable diseases were reported in 2024, and which was most common?
- How were confirmed cases geographically and temporally distributed in Feveria?

She asks that you write code to import, clean, link, and analyse the
following linelists:

-   **2024 notifiable disease surveillance data:** Referred to also as "notification data", this is surveillance data on five notifiable diseases reported by clinics in Feveria: dengue, malaria, cholera, typhoid fever, and yellow fever. These are suspected cases, based on patients' symptoms. Clinicians enter each notification into an online system every weekday. 
-   **2024 laboratory test result data:** This data comes from lab test results, from three major labs in Feveria. These results are for samples taken from those suspected notifiable disease cases mentioned above.

Let's go!

## Objectives

In this case study you will:

1.  Use key R functions to clean data, reshape datasets, link data
    sources, and create new columns using logical conditions to
    prepare data for analysis.
2.  Conduct data inspections and data quality checks at multiple stages of the project and understand their importance for reliable analysis.
3.  Perform basic descriptive analyses to compare disease trends across
    different data sources, before and after linkage.
4.  Interpret differences in results across data sources and understand
    how these reflect the structure and design of the overall
    surveillance system.

## Step 1. Set up

### 1.1 Get started in RStudio

Start by setting up a reproducible and well-organized workflow. This
will make it easy to rerun your analysis whenever needed.

**Tasks:**

-   Set up an RStudio project
-   Set up clear sub-folders where your code, data, and outputs will go
-   Create an R script, or an R Markdown file if you prefer. Make sure
    the script purpose, date, and author are written as comments at the
    top.
-   Extra: Ensure your working language in RStudio is appropriate (e.g.
    English for this exercise)

::: {.callout-tip collapse="true"}
## Click to read a hint

-   Create a folder where all the work in this case study will go. For
    example, create 'multi_disease_lab' on your computer desktop. Create
    your RStudio project to be based in this folder.

-   We suggest creating the following sub-folders: `scripts` (for your
    code), `data` (for your data), and `outputs` (for your analytical
    outputs).
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Create a folder (e.g. 'multi_disease_lab' on your Desktop) for your
work. To create an Rstudio project in your new folder, click
`New Project…` in the top left of your R Studio, then
`Existing Directory`, then `Browse` to select your new folder. For more
information, look at the [R
projects](https://epirhandbook.com/new_pages/r_projects.html) section of
the Epi R Handbook.

Start a new R script by clicking `New File…` in the top left of your R
Studio, then `R Script`. Save it immediately in the appropriate place,
e.g. in a 'scripts' sub-folder of your R Project.

At the top of your new R script, write some essential information like
your name, the purpose of the file, and the date.

Your R locale determines the language and regional settings used for
things like date formats and translations. If your locale is different
from the language you want for your report (e.g., a French locale vs. an
English report), you can change it to English by running
`Sys.setlocale("LC_ALL", "English")`. Include this in your script if
needed, or skip it if your locale is usually appropriate. This is
explained in more detail in the [How-to Guide](pages/instructions.qmd_).
:::

### 1.2 Install/load packages

Next in your R script, you need to install and load the necessary R
packages. This ensures that the functions you need are available for
your analysis.

You will need the following packages: `{rio}` (for importing data),`{skimr}` (for reviewing data), `{janitor}` (for cleaning data), `{lubridate}` (for cleaning dates),  `{epikit}` (for epi-related tasks), `{gtsummary}` (for summary statistics/tests and regression), `{apyramid}` (for age-sex pyramids), `{tidyverse}` (for general data manipulation/science tasks), `{flextable}` (for presentation-ready tables), and `{naniar}` (for evaluating missing data). You will also need the `{remotes}` package to download the data - which we will explain in the download section. 

As you start, your trusted colleague nudges you and whispers "I've heard
that a great way to manage your packages is with the `{pacman}` package".

Over to you!

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use the function `p_load()` from `pacman` for this task. You provide the
function with a list of packages that you want to use. The function will undertake two
steps per package: 1) Check if the package is installed on your
computer, and install it if necessary, then 2) Load the package so it
can be used during this R session.

If you don't already have `pacman` installed, you will need to install
it the "traditional way" first, with `install.packages()`.

Note that the order of packages in your p_load function can be
important. If two packages have the same function names (e.g. `select()`
in the package `MASS` and `select()` in `tidyverse`, which do different
things), then R will use the function from the most recently loaded
package. To prioritize functions from tidyverse, which are commonly used
for data manipulation and visualization, load tidyverse last.

```{r, echo=TRUE, eval=TRUE}

# Ensures the package "pacman" is installed
if (!require("pacman")) {
     install.packages("pacman") }

# install (if necessary) from CRAN and load packages to be used
pacman::p_load(
  rio,        # importing data  
  skimr,      # get overview of data
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  epikit,     # to create age categories
  gtsummary,  # summary statistics, tests and regressions 
  apyramid,   # plotting age pyramids 
  flextable,  # Presentation ready tables
  naniar,     # Evaluating missingness of data
  remotes,    # Used to install package to download data
  tidyverse   # data management and visualization
)

```
:::

## Step 2. Download and import the data

### 2.1: Download the data

Your office provides you with two files for your analysis, both
containing data for 2024 and updated as of 15th January 2025:

-   A disease notification-level dataset (*"multidisease_notifications.xlsx"*) with case
    information from 5 health centers
-   A laboratory test-level dataset (*"multidisease_tests.csv"*) submitted by three laboratories conducting testing for the 5 health centers.

For this case study, you can download the data via Applied Epi's very
useful data repository, which you can access using the
`{appliedepidata}` package. Follow these steps:

1)  Install the `{appliedepidata}` package from GitHub using the `install_github()` function in the `{remotes}` package (which you installed previously)

```{r , echo=TRUE, eval=FALSE}
# Use the install_github function from remotes to install appliedepidata
remotes::install_github("appliedepi/appliedepidata")
```

2)  Save the two datasets into a specific folder using the `save_data()`
    function from `{appliedepidata}`, by running the code below. The
    example below saves the data into a 'data' subfolder within the
    RStudio project. Note that if you do not specify a location within
    the 'path' argument of the function, a window will pop up asking you
    to manually select a folder.

```{r , echo=TRUE, eval=FALSE}
# Save down the two data files using the save_data() function from appliedepidata
appliedepidata::save_data("multidisease_test",
                        path = "data")

appliedepidata::save_data("multidisease_notifications",
                          path = "data")
```

### 2.2 Import the data

Great! Thanks country office and Applied Epi! Now it's time to import the data from that folder into RStudio, so you can analyse it.

#### **Task A: Import the two downloaded data files into your R Studio environment**

Ideally, you will use the same function for importing both datasets, despite one
being a .csv and the other an .xlsx file. Note going forward we will simply say "environment" when we mean the environment pane in R Studio.

::: {.callout-tip collapse="true"}
## Click to read a hint

Use the `import` function from the `{rio}` package, which can recognize
and import different file types. It replaces importing functions that
are specific to the file type, such as `read.csv()` from `{base}` for
.csv files and `read_excel()` from `{readxl}` to import .xlsx files.

If you feel you need to know more about importing functions, read the
[Import and export](https://epirhandbook.com/new_pages/importing.html)
chapter of the Epi R Handbook.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Below we use the import function to bring in both files. Note how we are
assigning the imported data to two objects, one called
*data_notif_raw*, and one called *data_lab_raw*. We add the 'raw'
suffix to distinguish this data from the cleaned versions we will make
later.

```{r , echo=TRUE, eval=FALSE}
# Import data

# Notification data
data_notif_raw <- import("data/multidisease_notifications.xlsx")

# Lab data
data_lab_raw <- import("data/multidisease_tests.csv")

```

```{r, eval=T, include = FALSE}
# This code is actually run; the prior chunk is just for show for simplicity

library(googlesheets4)
# 
# data_notif_raw <- read_sheet("https://docs.google.com/spreadsheets/d/1Dgh-VLiAdZ3TFYhslVr-sNCUdrPayKKxqp0E-F6-I_w/edit?gid=1417096190#gid=1417096190", sheet = "data_surv_simulated")
# 
# data_lab_raw <- read_sheet("https://docs.google.com/spreadsheets/d/1Dgh-VLiAdZ3TFYhslVr-sNCUdrPayKKxqp0E-F6-I_w/edit?gid=1417096190#gid=1417096190", sheet = "data_lab_simulated")

pacman::p_load("remotes")

if (!requireNamespace("appliedepidata", quietly = TRUE)) {
  remotes::install_github("appliedepi/appliedepidata")
}

appliedepidata::get_data(name = "multidisease_tests")
appliedepidata::get_data("multidisease_notifications")

data_notif_raw <- multidisease_notifications
data_lab_raw <- multidisease_tests

rm(multidisease_notifications, multidisease_tests)

```
:::

## Step 3. Inspect the data

The data’s in, and now it’s time to see what story it tells. Take an
initial look at your two raw data frames to check their contents and quality.

### Step 3.1 Inspect the surveillance data

#### **Task A: First, inspect the *dimensions* and general contents of the notification data**

**Use `skim()` from the `{skimr}` package, `names()`, `ncol()`, and
`nrow()` to inspect your data frame.**

`skim()` gives you a lot of information on data structure and content,
whereas `names()` will show you the different column names in your data.
The `ncol()` and `nrow()` functions to simply count the numbers of
columns and rows in the data. Do you know what to put inside the
parentheses?

Easiest of all though, is to look at the environment. Remember
the object in your environment for the notification data is called
`data_notif_raw`.

Click on the solution box underneath the questions if you need help.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}
pacman::p_load(webexercises)

opts <- c(
  "10",
  "11",
  answer = "12",
  "13"
)

cat("How many columns are there in the notification data?", longmcq(opts))

opts <- c(
  "Onset date",
  "Date reported by Health Facility/Community",
  "Date of outcome",
  answer = "Date of test",
  "Date of birth"
)

cat("Which of these columns are NOT in the data?", longmcq(opts))

pacman::p_load(webexercises)

opts <- c(
  answer = "Notification ID",
  "Test ID",
  "Health facility code",
  "Combination of Notification ID and Sex"
)

cat("What is the name of the column in the notification data that identifies each notification?", longmcq(opts))

opts <- c(
  answer = "987",
  "1314",
  "950",
  "778"
)

cat("How many rows are there in the notification data?", longmcq(opts))

opts <- c(
  answer = "Laboratory test results",
  "District of residence",
  "Birthday and sex",
  "Health facility in which the case was diagnosed",
  "Outcome"
)

cat("What type of information can you NOT see in the notification data?", longmcq(opts))
```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use `skim()` from the `{skimr}` package to look at a summary of the entire
data frame, and `View()` to look at the whole data frame directly:

```{r}
skim(data_notif_raw)
```

Or, you could use `names()` to print out just the column names. Through
either `skim()` or `names()` you will be able to see the types of
information including: the health facility of the case, birth-date, sex, a flag indicating pregnancy, district of residence, onset date, and date reported by the clinic, and outcome information. There is also a `Notification ID` which appears to be a unique identifier for a case,
but we would want to double check duplicates before we are sure. Note
that there are NO test results in this data, as these notifications are
from clinics diagnosing notifiable diseases based on clinical case
definitions.

```{r eval=T}
names(data_notif_raw)
```

Use `ncol()` and `nrow()` to print the number of columns and rows, like
this:

```{r}
ncol(data_notif_raw)
nrow(data_notif_raw)
```

This will print the numbers of columns and rows in your console.

```{r eval=T, echo=F}
ncol(data_notif_raw)
nrow(data_notif_raw)
```

Otherwise, when you look at the environment you can see that the
number of observations (which is the same as rows) and columns are
listed next to the name of the data frame.
:::

#### **Task B: Next, look at the classes of columns in your raw notification data frame**

**Use `skim()` from the `{skimr}` package or `class()` to inspect your
column classes.**

Do you remember how to specify the column of interest inside the
`class()` function? Alternatively, you can just look at the environment.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "0",
  "2",
  "4"
)

cat("How many columns in the notification data frame are recognised by R to be date columns?", longmcq(opts))

opts <- c(
  answer = "character",
  "numeric",
  "factor"
)

cat("What is the class of most columns in the raw notification data frame?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can use `class` like the example below. The \$ is an operator used
to select a specific column from the `data_notif_raw` data frame. Note
that the back-ticks are used around Date of Birth because the column name
contains spaces.

```{r}
class(data_notif_raw$`Date of Birth`)
```

To look at class via the environment, click on the blue arrow
next to the data frame name. The column names will appear, with the class
next to it (e.g. it says "chr" to show character class).

You can see the none of the columns that should be dates are recognized as dates. Instead, they are recognized as character values. 
:::

#### **Task C: Inspect categorical values and missingness**

**Use the `tabyl()` function to inspect the values within categorical
columns**, specifying the data frame object in the first argument, and the
column name in the second argument. For example, this code tabulates the
values for the Sex column. The output shows that male and female are
inconsistently spelled across the data. This column would need further
cleaning before analysis.

```{r eval=T}
tabyl(data_notif_raw, Sex)

```

To inspect missingness, you can use the `miss_var_summary()` function
from the {naniar} package:

```{r eval=T}
miss_var_summary(data_notif_raw)
```

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "No - they need cleaning",
  "They are standardised and are ready to be used for analysis"
)

cat("Are the values in the `Residential District` column standardised?", longmcq(opts))

opts <- c(
  "No - they need cleaning",
 answer = "They are standardised and are ready to be used for analysis"
)

cat("Are the values in the `Disease notified` column standardised?", longmcq(opts))

opts <- c(
  "Either no value, or just a space, or just a dot",
 answer = "No value in a cell, represented with NA",
 "The words Unknown and Uncertain"
)


cat("What does R recognise as a missing value?", longmcq(opts))


opts <- c(
  "Yes, the missingness is low so this column is useful",
 answer = "Minimally, as the missingness is too high"
)

cat("Based on the missingness of its values, is the `Onset date` column useful?", longmcq(opts))

cat("Are the values in the `Disease notified` column standardised?", longmcq(opts))

opts <- c(
  "A bot scrambles the data so it becomes less identifiable",
 answer = "Each clinic might use software that is configured slightly differently, or use free-text entries, so there are variations in spelling",
 "The surveillance system software used by the clinical settings has lots of bugs"
)

cat("Why might some columns in the notification data have different spellings and non-standardized categories?", longmcq(opts))

opts <- c(
 "The clinician does not ask the patient the question during their consultation",
 "The patient might not know or want to share the answer",
 "The clinician might not have time to prioritise filling in that field in the data, even if they know the information",
 "All of the above, and many more reasons"
)

cat("Why might some columns in the notification data have high missingness?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use the `tabyl()` function to tabulate the values within the Residential
district column. Again, the first argument is the name of the data frame
object, and the second argument is the name of the column.

```{r, eval=T}
tabyl(data_notif_raw, `Residential District`)
```

You can see that each of the three locations (Feveria Central, Lake
Minara, and Kasara) are spelled in different ways and with different
capitalization. This will need to be cleaned out if we want to analyse
the geographic distribution of the notifiable diseases.

Similarly, use the `tabyl()` function to tabulate the values within the
`Disease notified` column. You can see these are spelled out
appropriately and consistently, so you can already see the distribution
of rows by disease without further cleaning.

```{r, eval=T}
tabyl(data_notif_raw, `Disease notified`)
```

A different way of checking missingness is to tabulate the output of
the function `is.na()`. In the example below, the function `is.na()`
evaluates each cell within the column `Onset date`, returning TRUE for
missing ones and FALSE for present ones. Running `tabyl()` on this
TRUE/FALSE output then quickly gives you a clear count and percentage of
both missing and non-missing values in that column. Remember, values
like a space or the words "Unknown" or "Missing" will not be recognized
by R as missing. R will only recognize true blanks as missing,
represented by "NA".

For `Onset date`, you can see that 70% of cases are missing onset date,
suggesting that this column would not be particularly useful for
analyzing trends in disease over time.

```{r, eval=T}
tabyl(is.na(data_notif_raw$`Onset date`))
```

Missing or non-standardised data can arise for many reasons, including the design of the data collection tool (e.g. whether questions are mandatory or use free text vs. drop-downs), the processes and standards in place (such as which fields staff are instructed to prioritise), and contextual factors (like whether staff have sufficient time to gather the information) — among many others.
:::

### Step 3.2 Inspect the laboratory data

#### **Task A: Inspect the *dimensions* and general content of the lab data**

Like with the surveillance data, **use `skim()`, `ncol()`, and `nrow()`
functions or check the environment to inspect the lab data.**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}
pacman::p_load(webexercises)

opts <- c(
  "Lab data",
  answer = "Surveillance data",
  "They have the same number of columns"
)

cat("Which linelist has more columns - the surveillance data or the laboratory data?", longmcq(opts))

opts <- c(
  answer = "Lab data",
  "Surveillance data",
  "They have the same number of rows"
)

cat("Which linelist has more rows?", longmcq(opts))

opts <- c(
  answer = "There may be several tests or targets per sample",
  "There are so many trial test results in the data",
  "Not all the notifications have test results yet"
)

cat("Inspect the lab data with `View()`. Why might the lab data have more records?", longmcq(opts))

opts <- c(
  "Notification ID",
  "Sample ID",
  "Test type",
  answer = "Date of birth",
  "Test result"
)

cat("Which of these columns are NOT in the lab data?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Just like in section 3.1, you can use `skim()` from the `{skimr}` package
to look at the entire laboratory data frame with test results.
This will also show you the different column names in the data, showing
you that the lab data only contains information about the test and not
about the patient. It does however also contain a notification ID, just
like the notification data does.

```{r}
skim(data_lab_raw)
```

Use `ncol()` and `nrow()` to print the number of columns and rows, like
this:

```{r}
ncol(data_lab_raw)
nrow(data_lab_raw)
```

This will print the numbers of columns and rows in your console, showing you that the lab data has more rows than the notification data you inspected earlier. 

```{r eval=T, echo=F}
ncol(data_lab_raw)
nrow(data_lab_raw)
```

There are often more records in the lab data than in the clinical data.
If you inspect the data with `View(data_lab_raw)` and then click on the arrow at the top of the `notification_id` column to sort it alphabetically, you'll see that several rows share the same notification_id. This can happen when
multiple targets are tested from the same sample (same sample ID), or
when a case is retested (resulting in a different sample ID).

```{r}
View(data_lab_raw)
```

```{r, eval=T, echo=F}
flextable(head(data_lab_raw |> filter(str_detect(target,"Dengue")))) |> autofit()
```
:::

#### **Task B: Look at the classes, categorical values, and missingness** 

As above, **use the `class()`, `skim()`, or `tabyl()` functions, or
inspect the environment, to look at your columns in more detail.**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "0",
  answer =  "1",
  "2"
)

cat("How many columns in the laboratory data frame are recognised by R to be date columns?", longmcq(opts))


opts <- c(
  "1",
  "3",
  answer = "7 (all of them!)"
)

cat("How many columns in the laboratory data frame have complete data?", longmcq(opts))


opts <- c(
  "Malaria",
  answer = "Dengue",
  "Yellow Fever",
  "Cholera",
  "Typhoid Fever"
)

cat("Which test detects multiple targets (and therefore has multiple rows per sample)?", longmcq(opts))

opts <- c(
  answer = "5",
  "3",
  "4"
)

cat("How many possible test result values are there in the column `value`?", longmcq(opts))

opts <- c(
  answer = "P",
  "P01",
  "P0139",
  "N",
  "I"
)

cat("What is NOT a possible test result for the stool culture test which detects V. cholerae bacteria'?)", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

The laboratory data has one date column, recognized by R as an "IDate" class. This is a date class used by {rio}'s import() when reading csv files. Like base R's Date class, it allows sorting by date and analyzing trends over time.

```{r eval=T}
class(data_lab_raw$date_test)
```

Use of the `miss_var_summary()` function from the `{naniar}` package
demonstrates that all columns in the laboratory data are actually complete. This may be because the laboratory systems use automated processes, so are much less likely to have human error. 

(**Important point**: Note that in real life, the lab data would probably have some issues too!)

```{r eval=T}
miss_var_summary(data_lab_raw)
```

To see how many targets are detected by each test, you can cross-tabulate `test` and `target` columns with `tabyl()`. Write the column names into the function as two separate arguments. The output shows that each test clearly aligns with one or more targets, and only the dengue assay detects more than one target (IgG, IgM, and NS.1). 

**Tip:** Experiment with changing the order of the column names in the function to see the impact on the table.

```{r eval=T}
tabyl(data_lab_raw, target, test)
```

Finally, you can inspect the different test result values in the column `value`, again using `tabyl()`. You can see that there are six possible results, including N for negative, P for positive, and I for
indeterminate. Cholera specifically does not show P, but can show P01 and P0139, which in this case represent being positive for serogroups O1 or O139.

```{r eval=T}
tabyl(data_lab_raw, test, value)
```
:::

## Step 4. Clean and describe the notification data

You now know that the notification data (`data_notif_raw`) contains
information about suspected cases, alongside basic demographic
information (age, sex, pregnancy, district of residence), and
information about their onset date, date reported by the health facility,
and outcome. Some columns need cleaning before further analysis, due to
variations in spelling of categorical values and some date columns not
being recognized as dates.

You will now start writing longer chunks of code to clean data, using
various `{dplyr}` functions chained together with pipes (which look like
this: \|\>).

**NOTE ON PIPES**: Pipes allow you to perform several operations in one smooth sequence, by "chaining" different functions together. The output from one function becomes the input for the next. If you need more information on piping, please refer to the [Epi R Handbook](https://www.epirhandbook.com/en/new_pages/cleaning.html#cleaning-pipeline). Note that this exercise uses the base pipe (|>) rather than the magrittr pipe (%>%), as it is faster and does not require package installation. Use the magrittr pipe if you prefer it. 

### Step 4.1 Clean the data

#### **Task A: Clean your column names and select columns for analysis**

Due to quality and data storage issues, your team recommends that you create a clean linelist that only contains information on the unique identifier, location of the case, disease, and the date the notification was reported to the surveillance system. 

**Write R code to produce a new clean data frame called `data_notif`**,
applying the following cleaning tasks:

-   Rename columns to be more machine readable (remove spaces and
    capitalization) using `clean_names()` from the {janitor} package
-   Use the `rename()` function from `{dplyr}` so that the column with
    the date the case was reported is changed to a more concise `date_report`.\
-   Select relevant columns for analysis with the `select()` function
    from the `{dplyr}` package.

::: {.callout-tip collapse="true"}
## Click to read a hint

Start your code with the name of the new data frame, the assignment
arrow, and the name of the raw data object. This shows that the outcome
of the raw data processing will be assigned to a new object called
`data_notif`.

```{r}
data_notif <- data_notif_raw

```

Then build on this code by adding in additional functions, chained
together with a pipe. This lets you perform several operations in one
smooth sequence. First, you'll use `clean_names()` to standardize all
your column names. It automatically replaces spaces and special
characters with underscores and converts everything to lowercase, making
names easier to work with. Then, you can use `rename()` to give a column
a new name. Just remember, when you use rename(), the column will
already have its `clean_names()` version.

```{r}
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(NEW_NAME = OLD_NAME) |> 
  select(VAR_NAMES)

```
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is the code to clean column names and select the right columns for
analysis:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report)

```
:::

#### **Task B: Standardize categorical values**

You already know from your data inspection that the values for district
are not standardized.

**Add a `mutate()` function to clean the `residential_district`
column**, to:

-   Standardize the capitalization of the column
-   Replace the existing `residential_district` column with a clean
    column that only contains these district values: "Lake Minara",
    "Feveria Central", and "Kasara".

See the hint to see what functions you can use.

::: {.callout-tip collapse="true"}
## Click to read a hint

Try using `str_to_title()` from `{stringr}` package so that the first
letter of each word is upper case and all other letters are lower case.
You can also use `case_match()` to specify different specific typos.

Use the 'help' functionality of RStudio to see how to use these
functions. For example, type `?case_match` in your console to get the
help page. **NOTE** on `case_match()` - this is a very useful function for replacing or correcting values, and supersedes `recode()`.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Your cleaning code should now look like this:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = str_to_title(residential_district)) |> 
  mutate(residential_district = case_match(residential_district,
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara"))


```

You could also wrap the `str_to_title` function into the `case_match()`
for shorter code, as follows:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = case_match(str_to_title(residential_district),
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara"))


```
:::

#### **Task C: Manage dates**

The column for report date needs to be transformed so that it is
recognized as a date in R. This will allow you to analyse trends over
time, including over weeks and months.

**Review the values within the `date_report` column. Then, add a line to
your cleaning code to change `date_report` into a date class.**

Knowing the structure will allow you to use the correct function to
convert the column into a date class. We recommend you use one of the
functions from the `{lubridate}` package: either `ymd()` (for converting
dates written as year-month-date), `mdy()` (for dates written as
month-day-year), or `dmy()` (for dates written as day-month-year). These
functions will recognize any way of writing the date as long as it is
the correct order, for example "21st August 2025" and "21-08-2024" would
both be recognized by `dmy()`.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "day-month-year", 
  answer = "year-month-day",
  "month-day-year",
  "year-day-month"
)

cat("How are the dates currently formatted?", longmcq(opts))

opts <- c(
  answer = "mutate(date_report = ymd(date_report))", 
  "mutate(date_report = dmy(date_report))",
  "mutate(date_report = mdy(date_report))"
)

cat("Which `mutate()` function should you use to convert the date_report column into a date class?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can use the `head()` function to view the first six rows of data for
the `date_report` column. You can see that they are written with the
year first, then the month, then the date.

```{r eval=T}
head(data_notif$date_report)
```

You can use the `ymd()` function inside `mutate()` to convert the class
of the `date_report` function. You can double-check that the class is
correct by running a `class()` function afterwards.

Your cleaning code should now look like this:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = case_match(str_to_title(residential_district),
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara")) |> 
  mutate(date_report = ymd(date_report)) 

```

And you can double check the class with this:

```{r eval=T}
class(data_notif$date_report)
```
:::

#### **Task D: Check for duplicates**

Your colleagues tell you that each notification_id represents one
suspected case. You now want to **create a table to check if
`notification_id` is duplicated across rows in you data**.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(answer = "Yes", "No")

cat("Does one row in the notification data equate to one case?", longmcq(opts))

opts <- c("Yes", answer = "No")

cat("Do you need to deduplicate your data for epidemiological analysis of cases?", longmcq(opts))

```
:::
::::

::: {.callout-tip collapse="true"}
## Click to read a hint

There are many ways to do this, but try using `count()` function from
`{dplyr}`. It will create a table that counts the number of rows per
unique value of the column that you specify inside the function. Then,
use `tabyl()` to look at the distribution of these counts.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

First, pipe from the surveillance data into the `count()` function,
giving the `notification_id` column as the only argument. This creates a
table that counts the number of rows per unique value of `sample_id`,
shown in a new column `n`. You can see for example in this excerpt that
there is only one row per each of these 6 `notification_id`s.

```{r, eval=F}

data_notif |> 
  count(notification_id) 
```

```{r, eval=T, echo=F}

data_notif |> 
  count(notification_id) |> 
  head()
```

Then tabulate the new column `n` with the `tabyl()`, which shows that
there is only one row per unique notification_id. This means that one
row equates to one case, and no further deduplication is needed.

```{r, eval=T}

data_notif |> 
  count(notification_id) |> 
  tabyl(n)
```
:::

### Step 4.2 Simple descriptive analysis

You can now comfortably proceed with descriptive analyses of cases, as
your data is clean and you know that one row equals one case. Use the
`tabyl()` function for the following tasks.

#### **Task A: Count the number of suspected cases of each disease diagnosed in Feveria in 2024**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "Cholera", 
  answer = "Malaria",
  "Dengue",
  "Typhoid Fever",
  "Yellow Fever"
)

cat("Which disease was most commonly diagnosed by clinics in Feveria in 2024?", longmcq(opts))

opts <- c(
  "Cholera", 
  "Malaria",
  "Dengue",
  answer = "Typhoid Fever",
  "Yellow Fever"
)

cat("Which disease was least commonly diagnosed by clinics in Feveria Central in 2024?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)
 
Using `tabyl()`, we can see that there were 533 suspected cases of
malaria in Feveria in 2024, and only 35 suspected cases of
typhoid fever.

```{r eval=T}
tabyl(data_notif, disease_notified)
```
:::

#### **Task B: Count the number of suspected cases by disease and district of residence**

**Use `tabyl()` to cross-tabulate the disease and district of residence columns.**

Build on your table by adding various `adorn` functions from the `{janitor}` package, to see percentage distributions, e.g. `adorn_percentages()`, `adorn_pct_formatting()`, and `adorn_ns()`

Type the name of the function after a ? in your console (e.g. `?adorn_ns`) to see the relevant Help pages. You can also look at the [section about {janitor} in the Epi R Handbook](https://epirhandbook.com/en/new_pages/tables_descriptive.html#tbl_janitor) for more explanation of `adorn_xxx()` functions. 

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "Lake Minara",
  "Feveria Central",
  "Kasara"
)

cat("Which district reported the most vector-borne disease in 2024 (malaria, dengue, yellow fever)?", longmcq(opts))

opts <- c(
  "Lake Minara",
  answer = "Feveria Central",
  "Kasara"
)

cat("Which district reported the most diarrhoeal disease in 2024 (cholera, typhoid fever)?", longmcq(opts))


opts <- c(
  answer = "Unreliable water and sanitation infrastructure",
  "Overcrowding of mosquitoes",
  "We don't know"
)

cat("What factors contribute to increased diarrhoeal disease in this specific district (selected in previous question)?", longmcq(opts))

```
:::
::::

::: {.callout-tip collapse="true"}
## Click to read a hint

Here is some code to get you started. It cross-tabulates `disease_notified` and `residential_district` with `tabyl()`, then adding `adorn_percentages()` converts these numbers to proportions with many decimals. You then need to pipe into `adorn_pct_formatting()` to convert into actual percentage formatting, and then `adorn_ns()` to add numbers back in in parentheses. Note that `adorn_xxx()` functions need to be applied in a specific order!
```{r}
tabyl(data_notif, disease_notified, residential_district) |>
  adorn_percentages()
```

For factors contributing to more diarrhea - scroll up to earlier in the case study when the districts were first introduced!


:::


:::



::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Using `tabyl()`, we can see that most suspected cases of dengue, malaria, and
yellow fever were located in Lake Minara - the lake area with higher
density of mosquitoes and therefore vector-borne disease. Meanwhile the
majority of cholera and typhoid fever were in Feveria Central, the
over-populated urban area with water and sanitation infrastructure
issues that result in higher risk of flooding and drinking water
contamination during rainy weather.

```{r eval=T}
tabyl(data_notif, disease_notified, residential_district) |>
  adorn_percentages() |>
  adorn_pct_formatting() |>
  adorn_ns()
```
:::
