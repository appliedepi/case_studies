---
editor_options: 
  chunk_output_type: console
execute:
  warning: false
  error: false
format: 
  html: 
    css: webex.css
    include-after-body: webex.js
editor: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

# Linking and analysing notifiable disease surveillance data and laboratory data in R {#multi_disease_lab}

::: {.callout-note appearance="minimal" icon="false"}
**Tool**: R \| **Technical complexity**: Intermediate \|
**Methodological complexity**: Basic \| **Prior knowledge required:** [R
basics](https://epirhandbook.com/en/new_pages/basics.html) (Using
Rstudio; R packages, functions and arguments, using pipes) \| **Source:**


Applied Epi, with technical support provided by the Global Surveillance, Laboratory, and Data Systems Branch in collaboration with TEPHINET. The contents are those of the author(s) and do not necessarily represent the official views of CDC or the US Department of Health and Human Services. \| **Funding:** This project was 100% supported by cooperative agreement #XXXXXXXXXXXXXX funded by the U.S. Centers for Disease Control (CDC) and Prevention (CDC).
:::

For instructions on how to use our case studies, see our [How-to
Guide](instructions.html). We welcome feedback and suggestions via
[contact\@appliedepi.org](mailto:contact@appliedepi.org). You can also
discuss the case study or related concepts on the [Applied Epi
Community](https://community.appliedepi.org/).

\pagebreak

## Scenario

You are an epidemiologist working in the national surveillance office of Feveria, a very small tropical country. There are three districts within Feveria:

-   **Feveria Central**: an over-populated urban area, with sometimes unreliable
    water and sanitation infrastructure.
-   **Lake Minara**: a lake area with good infrastructure but many
    mosquitoes in the warmer months of the year.
-   **Kasara**: a more sub-urban area on the other side of Feveria Central.

**Map of districts in the country Feveria**

![](/images/multi_disease_lab/map.png){width="70%"}

It is January 2025, and your supervisor would like you to transfer the routine
processing of notifiable disease data from Excel into R, and conduct some analyses on the data. She wants to know at least:

- How many suspected cases of the different notifiable diseases were reported in 2024, and which was most common?
- What percentage of them ended up being confirmed?
- How many confirmed cases of different notifiable diseass were reported in 2024, and which was most common?
- How were confirmed cases geographically and temporally distributed in Feveria?

She asks that you write code to import, clean, link, and analyse the
following linelists:

-   **2024 Notifiable Disease Surveillance Data:** This is the data your
    surveillance unit gets on a daily basis from clinics throughout Feveria, reported via submission to an online system every weekday. It includes details about cases **suspected** of having dengue, malaria, cholera, typhoid fever, and yellow fever, all diagnosed based on the patients' symptoms. 
-   **2024 Laboratory Data:** This data comes from lab test results, from three major labs in Feveria. These results are for samples taken from those suspected notifiable disease cases mentioned above.

Let's go!

## Objectives

In this case study you will:

1.  Use key R functions to clean data, reshape datasets, link data
    sources, and create new columns using logical conditions to
    prepare data for analysis.
2.  Conduct data quality checks at multiple stages of the project and
    understand their importance for reliable analysis.
3.  Perform basic descriptive analyses to compare disease trends across
    different data sources, before and after linkage.
4.  Interpret differences in results across data sources and understand
    how these reflect the structure and design of the overall
    surveillance system.

## Step 1. Set up

### 1.1 Get started in RStudio

Start by setting up a reproducible and well-organized workflow. This
will make it easy to rerun your analysis whenever needed.

**Tasks:**

-   Set up an RStudio project
-   Set up clear sub-folders where your code, data, and outputs will go
-   Create an R script, or an R Markdown file if you prefer. Make sure
    the script purpose, date, and author are written as comments at the
    top.
-   Extra: Ensure your working language in RStudio is appropriate (e.g.
    English for this exercise)

::: {.callout-tip collapse="true"}
## Click to read a hint

-   Create a folder where all the work in this case study will go. For
    example, create 'multi_disease_lab' on your computer desktop. Create
    your RStudio project to be based in this folder.

-   We suggest creating the following sub-folders: `scripts` (for your
    code), `data` (for your data), and `outputs` (for your analytical
    outputs).
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Create a folder (e.g. 'multi_disease_lab' on your Desktop) for your
work. To create an Rstudio project in your new folder, click
`New Project…` in the top left of your R Studio, then
`Existing Directory`, then `Browse` to select your new folder. For more
information, look at the [R
projects](https://epirhandbook.com/new_pages/r_projects.html) section of
the Epi R Handbook.

Start a new R script by clicking `New File…` in the top left of your R
Studio, then `R Script`. Save it immediately in the appropriate place,
e.g. in a 'scripts' subfolder of your R Project.

At the top of your new R script, write some essential information like
your name, the purpose of the file, and the date.

Your R locale determines the language and regional settings used for
things like date formats and translations. If your locale is different
from the language you want for your report (e.g., a French locale vs. an
English report), you can change it to English by running
`Sys.setlocale("LC_ALL", "English")`. Include this in your script if
needed, or skip it if your locale is usually appropriate. This is
explained in more detail in the [How-to Guide](pages/instructions.qmd_).
:::

### 1.2 Install/load packages

Next in your R script, you need to install and load the necessary R
packages. This ensures that the functions you need are available for
your analysis.

You will need the following packages: `rio` (for importing data),`skimr` (for reviewing data), 
`janitor` (for cleaning data), `lubridate` (for cleaning dates),  `epikit` (for epi-related tasks), `gtsummary` (for summary statistics/tests and regression), `apyramid` (for age-sex pyramids), `tidyverse` (for general data manipulation/science tasks), `flextable` (for presentation-ready tables), and `naniar` (for evaluating missing data).

As you start, your trusted colleague nudges you and whispers "I've heard
that a great way to manage your packages is with the `pacman` package".

Over to you!

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use the function `p_load()` from `pacman` for this task. You provide the
function with a list of packages that you want to use. The function will undertake two
steps per package: 1) Check if the package is installed on your
computer, and install it if necessary, then 2) Load the package so it
can be used during this R session.

If you don't already have `pacman` installed, you will need to install
it the "traditional way" first, with `install.packages()`.

Note that the order of packages in your p_load function can be
important. If two packages have the same function names (e.g. `select()`
in the package `MASS` and `select()` in `tidyverse`, which do different
things), then R will use the function from the most recently loaded
package. To prioritize functions from tidyverse, which are commonly used
for data manipulation and visualization, load tidyverse last.

```{r, echo=TRUE, eval=TRUE}

# Ensures the package "pacman" is installed
if (!require("pacman")) {
     install.packages("pacman") }

# install (if necessary) from CRAN and load packages to be used
pacman::p_load(
  rio,        # importing data  
  skimr,      # get overview of data
  janitor,    # data cleaning and tables
  lubridate,  # working with dates
  epikit,     # to create age categories
  gtsummary,  # summary statistics, tests and regressions 
  apyramid,   # plotting age pyramids 
  flextable,  # Presentation ready tables
  naniar,     # Evaluating missingness of data
  tidyverse   # data management and visualization
)

```
:::

## Step 2: Download and import the data

### 2.1: Download the data

Your office provides you with two files for your analysis, both
containing data for 2024 and updated as of 15th January 2025:

-   A disease notification-level dataset (*"notification_data.xlsx"*) with case
    information from 5 health centers
-   A laboratory test-level dataset (*"lab_data.csv"*) submitted by three laboratories
    conducting testing for the 5 health centers.

For this case study, you can access the data via Applied Epi's very
useful data repository, which you can access using the
`{appliedepidata}` package. So first you need to download these two
files to your own computer, as follows:

1)  Install the `{appliedepidata}` package from GitHub using the
    `install_github()` function in the `{remotes}` package. Install
    `{remotes}` if you need to first.

```{r , echo=TRUE, eval=FALSE}
# Install remotes if you need to (so you can install a package from GitHub)
pacman::p_load("remotes")

# Use the install_github function from remotes to install appliedepidata
remotes::install_github("appliedepi/appliedepidata")
```

2)  Save the two datasets into a specific folder using the `save_data()`
    function from `{appliedepidata}`, by running the code below. The
    example below saves the data into a 'data' subfolder within the
    RStudio project. Note that if you do not specify a location within
    the 'path' argument of the function, a window will pop up asking you
    to manually select a folder.

```{r , echo=TRUE, eval=FALSE}
# Save down the two data files using the save_data() function from appliedepidata
appliedepidata::save_data("notification_data",
                        path = "data")

appliedepidata::save_data("lab_data",
                          path = "data")
```

### 2.2 Import the data

Great! Thanks country office and Applied Epi! Now it's time to import
the data from that folder into RStudio, so you can analyse it.

#### **Task A: Import the two downloaded data files into your R Studio environment**

Ideally, you will use the same function for importing both datasets, despite one
being a .csv and the other an .xlsx file. Note going forward we will simply say "environment" when we mean the environment pane in R Studio.

::: {.callout-tip collapse="true"}
## Click to read a hint

Use the `import` function from the `{rio}` package, which can recognize
and import different file types. It replaces importing functions that
are specific to the file type, such as `read.csv()` from `{base}` for
.csv files and `read_excel()` from `{readxl}` to import .xlsx files.

If you feel you need to know more about importing functions, read the
[Import and export](https://epirhandbook.com/new_pages/importing.html)
chapter of the Epi R Handbook.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Below we use the import function to bring in both files. Note how we are
assigning the imported data to two objects, one called
*data_notif_raw*, and one called *data_lab_raw*. We add the 'raw'
suffix to distinguish this data from the cleaned versions we will make
later.

```{r , echo=TRUE, eval=FALSE}
# Import data

# Notification data
data_notif_raw <- import("data/notification_data.xlsx")

# Lab data
data_lab_raw <- import("data/lab_data.csv")

```

```{r, eval=T, include = FALSE}
# This code is actually run; the prior chunk is just for show for simplicity

library(googlesheets4)

data_notif_raw <- read_sheet("https://docs.google.com/spreadsheets/d/1Dgh-VLiAdZ3TFYhslVr-sNCUdrPayKKxqp0E-F6-I_w/edit?gid=1417096190#gid=1417096190", sheet = "data_surv_simulated")

data_lab_raw <- read_sheet("https://docs.google.com/spreadsheets/d/1Dgh-VLiAdZ3TFYhslVr-sNCUdrPayKKxqp0E-F6-I_w/edit?gid=1417096190#gid=1417096190", sheet = "data_lab_simulated")

# pacman::p_load("remotes")
# 
# if (!requireNamespace("appliedepidata", quietly = TRUE)) {
#   remotes::install_github("appliedepi/appliedepidata")
# }
# 
# appliedepidata::get_data("surv_data")
# notification_data_raw <- notification_data
# 
# 
# appliedepidata::get_data("lab_data")
# lab_data_raw <- lab_data
# 
# rm(notification_data, lab_data)

```
:::

## Step 3: Inspect the data

The data’s in, and now it’s time to see what story it tells. Take an
initial look at your two raw data frames to check their contents and quality.

### Step 3.1: Inspect the surveillance data

#### **Task A: First, inspect the *dimensions* and general contents of the notification data**

**Use `skim()` from the `{skimr}` package, `names()`, `ncol()`, and
`nrow()` to inspect your data frame.**

`skim()` gives you a lot of information on data structure and content,
whereas `names()` will show you the different column names in your data.
The `ncol()` and `nrow()` functions to simply count the numbers of
columns and rows in the data. Do you know what to put inside the
parentheses?

Easiest of all though, is to look at the environment. Remember
the object in your environment for the notification data is called
`data_notif_raw`.

Click on the solution box underneath the questions if you need help.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}
pacman::p_load(webexercises)

opts <- c(
  "10",
  "11",
  answer = "12",
  "13"
)

cat("How many columns are there in the notification data?", longmcq(opts))

opts <- c(
  "Onset date",
  "Date reported by Health Facility/Community",
  "Date of outcome",
  answer = "Date of test",
  "Date of birth"
)

cat("Which of these columns are NOT in the data?", longmcq(opts))

pacman::p_load(webexercises)

opts <- c(
  answer = "Notification ID",
  "Test ID",
  "Health facility code",
  "Combination of Notification ID and Sex"
)

cat("What is the name of the column in the notification data that identifies each notification?", longmcq(opts))

opts <- c(
  answer = "987",
  "1314",
  "950",
  "778"
)

cat("How many rows are there in the notification data?", longmcq(opts))

opts <- c(
  answer = "Laboratory test results",
  "District of residence",
  "Birthday and sex",
  "Health facility in which the case was diagnosed",
  "Outcome"
)

cat("What type of information can you NOT see in the notification data?", longmcq(opts))
```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use `skim()` from the `{skimr}` package to look at a summary of the entire
data frame, and `View()` to look at the whole data frame directly:

```{r}
skim(data_notif_raw)
```

Or, you could use `names()` to print out just the column names. Through
either `skim()` or `names()` you will be able to see the types of
information including: the health facility of the case, birthdate, sex, a flag indicating pregnancy, district of residence, onset date, and date reported by the clinic, and outcome information. There is also a `Notification ID` which appears to be a unique identifier for a case,
but we would want to double check duplicates before we are sure. Note
that there are NO test results in this data, as these notifications are
from clinics diagnosing notifiable diseases based on clinical case
definitions.

```{r eval=T}
names(data_notif_raw)
```

Use `ncol()` and `nrow()` to print the number of columns and rows, like
this:

```{r}
ncol(data_notif_raw)
nrow(data_notif_raw)
```

This will print the numbers of columns and rows in your console.

```{r eval=T, echo=F}
ncol(data_notif_raw)
nrow(data_notif_raw)
```

Otherwise, when you look at the environment you can see that the
number of observations (which is the same as rows) and columns are
listed next to the name of the data frame.
:::

#### **Task B: Next, look at the classes of columns in your raw notification data frame**

**Use `skim()` from the `{skimr}` package or `class()` to inspect your
column classes.**

Do you remember how to specify the column of interest inside the
`class()` function? Alternatively, you can just look at the environment.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "0",
  "2",
  "4"
)

cat("How many columns in the notification data frame are recognised by R to be date columns?", longmcq(opts))

opts <- c(
  answer = "character",
  "numeric",
  "factor"
)

cat("What is the class of most columns in the raw notification data frame?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can use `class` like the example below. The \$ is an operator used
to select a specific column from the `data_notif_raw` data frame. Note
that the backticks are used around Date of Birth because the column name
contains spaces.

```{r}
class(data_notif_raw$`Date of Birth`)
```

To look at class via the environment, click on the blue arrow
next to the data frame name. The column names will appear, with the class
next to it (e.g. it says "chr" to show character class).
:::

#### **Task C: Inspect categorical values and missingness**

**Use the `tabyl()` function to inspect the values within categorical
columns**, specifying the data frame object in the first argument, and the
column name in the second argument. For example, this code tabulates the
values for the Sex column. The output shows that male and female are
inconsistently spelled across the data. This column would need further
cleaning before analysis.

```{r eval=T}
tabyl(data_notif_raw, Sex)

```

To inspect missingness, you can use the `miss_var_summary()` function
from the {naniar} package:

```{r eval=T}
miss_var_summary(data_notif_raw)
```

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "No - they need cleaning",
  "They are standardised and are ready to be used for analysis"
)

cat("Are the values in the `Residential District` column standardised?", longmcq(opts))

opts <- c(
  "No - they need cleaning",
 answer = "They are standardised and are ready to be used for analysis"
)

cat("Are the values in the `Disease notified` column standardised?", longmcq(opts))

opts <- c(
  "Either no value, or just a space, or just a dot",
 answer = "No value in a cell, represented with NA",
 "The words Unknown and Uncertain"
)

cat("What does R recognise as a missing value?", longmcq(opts))


opts <- c(
  "Yes, the missingness is low so this column is useful",
 answer = "Minimally, as the missingness is too high"
)

cat("Based on the missingness of its values, is the `Onset date` column useful?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use the `tabyl()` function to tabulate the values within the Residential
district column. Again, the first argument is the name of the data frame
object, and the second argument is the name of the column.

```{r, eval=T}
tabyl(data_notif_raw, `Residential District`)
```

You can see that each of the three locations (Feveria Central, Lake
Minara, and Kasara) are spelled in different ways and with different
capitalisation. This will need to be cleaned out if we want to analyse
the geographic distribution of the notifiable diseases.

Similarly, use the `tabyl()` function to tabulate the values within the
`Disease notified` column. You can see these are spelled out
appropriately and consistently, so you can already see the distribution
of rows by disease without further cleaning.

```{r, eval=T}
tabyl(data_notif_raw, `Disease notified`)
```

A different way of checking missingness is to tabulate the output of
the function `is.na()`. In the example below, the function `is.na()`
evaluates each cell within the column `Onset date`, returning TRUE for
missing ones and FALSE for present ones. Running `tabyl()` on this
TRUE/FALSE output then quickly gives you a clear count and percentage of
both missing and non-missing values in that column. Remember, values
like a space or the words "Unknown" or "Missing" will not be recognised
by R as missing. R will only recognise true blanks as missing,
represented by "NA".

For `Onset date`, you can see that 70% of cases are missing onset date,
suggesting that this column would not be particularly useful for
analysing trends in disease over time.

```{r, eval=T}
tabyl(is.na(data_notif_raw$`Onset date`))
```
:::

### Step 3.2: Inspect the laboratory data

#### **Task A: Inspect the *dimensions* and general content of the lab data**

Like with the surveillance data, **use `skim()`, `ncol()`, and `nrow()`
functions or check the environment to inspect the lab data.**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}
pacman::p_load(webexercises)

opts <- c(
  "Lab data",
  answer = "Surveillance data",
  "They have the same number of columns"
)

cat("Which linelist has more columns - the surveillance data or the laboratory data?", longmcq(opts))

opts <- c(
  answer = "Lab data",
  "Surveillance data",
  "They have the same number of rows"
)

cat("Which linelist has more rows?", longmcq(opts))

opts <- c(
  answer = "There may be several tests or targets per sample",
  "There are so many trial test results in the data",
  "Not all the notifications have test results yet"
)

cat("Inspect the lab data with `View()`. Why might the lab data have more records?", longmcq(opts))

opts <- c(
  "Notification ID",
  "Sample ID",
  "Test type",
  answer = "Date of birth",
  "Test result"
)

cat("Which of these columns are NOT in the lab data?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Just like in section 3.1, you can use `skim()` from the `{skimr}` package
to look at the entire laboratory data frame with test results.
This will also show you the different column names in the data, showing
you that the lab data only contains information about the test and not
about the patient. It does however also contain a notification ID, just
like the notifiable disease surveillance data does.

```{r}
skim(data_lab_raw)
```

Use `ncol()` and `nrow()` to print the number of columns and rows, like
this:

```{r}
ncol(data_lab_raw)
nrow(data_lab_raw)
```

This will print the numbers of columns and rows in your console.

```{r eval=T, echo=F}
ncol(data_lab_raw)
nrow(data_lab_raw)
```

There are often more records in the lab data than in the clinical data.
If you inspect the data with `View(data_lab_raw)` and then click on the arrow at the top of each column to sort, you'll see that
several rows share the same notification_id. This can happen when
multiple targets are tested from the same sample (same sample ID), or
when a case is retested (resulting in a different sample ID).

```{r}
View(data_lab_raw)
```

```{r, eval=T, echo=F}
flextable(head(data_lab_raw |> filter(str_detect(target,"Dengue")))) |> autofit()
```
:::

#### **Task B: Look at the classes, categorical values, and missingness** 

As above, **use the `class()`, `skim()`, or `tabyl()` functions, or
inspect the environment, to look at your columns in more detail.**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "0",
  answer =  "1",
  "2"
)

cat("How many columns in the laboratory data frame are recognised by R to be date columns?", longmcq(opts))


opts <- c(
  "1",
  "3",
  answer = "6 (all of them!)"
)

cat("How many columns in the laboratory data frame have complete data?", longmcq(opts))


opts <- c(
  "Malaria",
  answer = "Dengue",
  "Yellow Fever",
  "Cholera",
  "Typhoid Fever"
)

cat("Which test detects multiple targets (and therefore has multiple rows per sample)?", longmcq(opts))

opts <- c(
  answer = "5",
  "3",
  "4"
)

cat("How many possible test result values are there in the column `value`?", longmcq(opts))

opts <- c(
  answer = "P",
  "P01",
  "P0139",
  "N",
  "I"
)

cat("What is NOT a possible test result for the stool culture test which detects V. cholerae bacteria'?)", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

There is one date column in the laboratory data, and R does recognise it as a date. You can see this by running the function `class()`, which returns the value "character". This means R understands the order of
values in this column (e.g. from oldest to newest) and trends over time can be inspected with this column.

```{r eval=T}
class(data_lab_raw$date_test)
```

Use of the `miss_var_summary()` function from the `{naniar}` package
demonstrates that all columns in the laboratory data are actually complete. This may be because
the laboratory systems use automated processes, so are much less likely
to have human error. 

(**Important point**: Note that in real life, the lab data would probably have some issues too!)

```{r eval=T}
miss_var_summary(data_lab_raw)
```

To see how many targets are detected by each test, you can cross-tabulate `test` and `target` columns with `tabyl()`. Write the column names into the function as two separate
arguments. The output shows that each test clearly aligns with one or
more targets, and only the Dengue RDT test detects more than one target (IgG, IgM, and NS.1). 

**Tip:** Experiment with changing the
order of the column names in the function to see the impact on the table.

```{r eval=T}
tabyl(data_lab_raw, target, test)
```

Finally, you can inspect the different test result values in the column `value`, again using `tabyl()`. You can see that there are six possible results, including N for negative, P for positive, and I for
indeterminate. Cholera specifically does not show P, but can show P01 and P0139, which in this case represent being positive for serogroups O1 or O139.

```{r eval=T}
tabyl(data_lab_raw, test, value)
```
:::

## Step 4: Clean and describe the notifiable disease surveillance data

You now know that the notifiable disease surveillance data (`data_notif_raw`) contains
information about suspected cases, alongside basic demographic
information (age, sex, pregnancy, district of residence), and
information about their onset date, date reported by the health facility,
and outcome. Some columns need cleaning before further analysis, due to
variations in spelling of categorical values and some date columns not
being recognised as dates.

You will now start writing longer chunks of code to clean data, using
various `{dplyr}` functions chained together with pipes (which look like
this: \|\>).

**NOTE ON PIPES**: Pipes allow you to perform several operations in one smooth sequence, by "chaining" different functions together. The output from one function becomes the input for the next. If you need more information on piping, please refer to the [Epi R Handbook](https://www.epirhandbook.com/en/new_pages/cleaning.html#cleaning-pipeline). Note that this exercise uses the base pipe (|>) rather than the magrittr pipe (%>%), because XXXXXXXX. 

### Step 4.1: Clean the data

#### **Task A: Clean your column names and select columns for analysis**

Due to quality and data storage issues, your team recommends that you create a clean linelist that only contains information on the unique identifier, location of the case, disease, and the date the notification was reported to the surveillance system. 

**Write R code to produce a new clean data frame called `data_notif`**,
applying the following cleaning tasks:

-   Rename columns to be more machine readable (remove spaces and
    capitalisation) using `clean_names()` from the {janitor} package
-   Use the `rename()` function from `{dplyr}` so that the column with
    the date the case was reported is changed to a more conscise `date_report`.\
-   Select relevant columns for analysis with the `select()` function
    from the `{dplyr}` package.

::: {.callout-tip collapse="true"}
## Click to read a hint

Start your code with the name of the new data frame, the assignment
arrow, and the name of the raw data object. This shows that the outcome
of the raw data processing will be assigned to a new object called
`data_notif`.

```{r}
data_notif <- data_notif_raw

```

Then build on this code by adding in additional functions, chained
together with a pipe. This lets you perform several operations in one
smooth sequence. First, you'll use `clean_names()` to standardize all
your column names. It automatically replaces spaces and special
characters with underscores and converts everything to lowercase, making
names easier to work with. Then, you can use `rename()` to give a column
a new name. Just remember, when you use rename(), the column will
already have its `clean_names()` version.

```{r}
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(NEW_NAME = OLD_NAME) |> 
  select(VAR_NAMES)

```
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is the code to clean column names and select the right columns for
analysis:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report)

```
:::

#### **Task B: Standardise categorical values**

You already know from your data inspection that the values for district
are not standardised.

**Add a `mutate()` function to clean the `residential_district`
column**, to:

-   Standardise the capitalisation of the column
-   Replace the existing `residential_district` column with a clean
    column that only contains these district values: "Lake Minara",
    "Feveria Central", and "Kasara".

See the hint to see what functions you can use.

::: {.callout-tip collapse="true"}
## Click to read a hint

Try using `str_to_title()` from `{stringr}` package so that the first
letter of each word is upper case and all other letters are lower case.
You can also use `case_match()` to specify different specific typos.

Use the 'help' functionality of RStudio to see how to use these
functions. For example, type `?case_match` in your console to get the
help page. **NOTE** on `case_match()` - this is a very useful function for replacing or correcting values, and supercedes `recode()`.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Your cleaning code should now look like this:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = str_to_title(residential_district)) |> 
  mutate(residential_district = case_match(residential_district,
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara"))


```

You could also wrap the `str_to_title` function into the `case_match()`
for shorter code, as follows:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = case_match(str_to_title(residential_district),
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara"))


```
:::

#### **Task C: Manage dates**

The column for report date needs to be transformed so that it is
recognized as a date in R. This will allow you to analyse trends over
time, including over weeks and months.

**Review the values within the `date_report` column. Then, add a line to
your cleaning code to change `date_report` into a date class.**

Knowing the structure will allow you to use the correct function to
convert the column into a date class. We recommend you use one of the
functions from the `{lubridate}` package: either `ymd()` (for converting
dates written as year-month-date), `mdy()` (for dates written as
month-day-year), or `dmy()` (for dates written as day-month-year). These
functions will recognise any way of writing the date as long as it is
the correct order, for example "21st August 2025" and "21-08-2024" would
both be recognised by `dmy()`.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "day-month-year", 
  answer = "year-month-day",
  "month-day-year",
  "year-day-month"
)

cat("How are the dates currently formatted?", longmcq(opts))

opts <- c(
  answer = "mutate(date_report = ymd(date_report))", 
  "mutate(date_report = dmy(date_report))",
  "mutate(date_report = mdy(date_report))"
)

cat("Which `mutate()` function should you use to convert the date_report column into a date class?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can use the `head()` function to view the first six rows of data for
the `date_report` column. You can see that they are written with the
year first, then the month, then the date.

```{r eval=T}
head(data_notif$date_report)
```

You can use the `ymd()` function inside `mutate()` to convert the class
of the `date_report` function. You can double-check that the class is
correct by running a `class()` function afterwards.

Your cleaning code should now look like this:

```{r eval=T}
# Clean data
data_notif <- data_notif_raw |> 
  clean_names() |> 
  rename(date_report = date_reported_by_health_facility_community) |> 
  select(notification_id, residential_district, disease_notified, date_report) |> 
  mutate(residential_district = case_match(str_to_title(residential_district),
                                           c("F Central", "Feveria C", "Feveria Central") ~ "Feveria Central",
                                           c("Kasara", "Ksr") ~ "Kasara",
                                           c("L Minara", "Lake Minara", "Lakeside") ~ "Lake Minara")) |> 
  mutate(date_report = ymd(date_report)) 

```

And you can double check the class with this:

```{r eval=T}
class(data_notif$date_report)
```
:::

#### **Task D: Check for duplicates**

Your colleagues tell you that each notification_id represents one
suspected case. You now want to **create a table to check if
`notification_id` is duplicated across rows in you data**.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(answer = "Yes", "No")

cat("Does one row in the notifiable disease surveillance data equate to one case?", longmcq(opts))

opts <- c("Yes", answer = "No")

cat("Do you need to deduplicate your data for epidemiological analysis of cases?", longmcq(opts))

```
:::
::::

::: {.callout-tip collapse="true"}
## Click to read a hint

There are many ways to do this, but try using `count()` function from
`{dplyr}`. It will create a table that counts the number of rows per
unique value of the column that you specify inside the function. Then,
use `tabyl()` to look at the distribution of these counts.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

First, pipe from the surveillance data into the `count()` function,
giving the `notification_id` column as the only argument. This creates a
table that counts the number of rows per unique value of `sample_id`,
shown in a new column `n`. You can see for example in this excerpt that
there is only one row per each of these 6 `notification_id`s.

```{r, eval=F}

data_notif |> 
  count(notification_id) 
```

```{r, eval=T, echo=F}

data_notif |> 
  count(notification_id) |> 
  head()
```

Then tabulate the new column `n` with the `tabyl()`, which shows that
there is only one row per unique notification_id. This means that one
row equates to one case, and no further deduplication is needed.

```{r, eval=T}

data_notif |> 
  count(notification_id) |> 
  tabyl(n)
```
:::

### Step 4.2: Simple descriptive analysis

You can now comfortably proceed with descriptive analyses of cases, as
your data is clean and you know that one row equals one case. Use the
`tabyl()` function for the following tasks.

#### **Task A: Count the number of suspected cases of each disease diagnosed in Feveria in 2024**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "Cholera", 
  answer = "Malaria",
  "Dengue",
  "Typhoid Fever",
  "Yellow Fever"
)

cat("Which disease was most commonly diagnosed by clinics in Feveria in 2024?", longmcq(opts))

opts <- c(
  "Cholera", 
  "Malaria",
  "Dengue",
  answer = "Typhoid Fever",
  "Yellow Fever"
)

cat("Which disease was least commonly diagnosed by clinics in Feveria Central in 2024?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)
 
Using `tabyl()`, we can see that there were 533 suspected cases of
malaria in Feveria in 2024, and only 35 suspected cases of
typhoid fever.

```{r eval=T}
tabyl(data_notif, disease_notified)
```
:::

#### **Task B: Count the number of suspected cases by disease and district of residence**

**Use `tabyl()` to cross-tabulate the disease and district of residence columns.**

Build on your table by adding various `adorn` functions from the `{janitor}` package, to see percentage distributions, e.g. `adorn_percentages()`, `adorn_pct_formatting()`, and `adorn_ns()`

Type the name of the function after a ? in your console (e.g. `?adorn_ns`) to see the relevant Help pages. You can also look at the [section about {janitor} in the Epi R Handbook](https://epirhandbook.com/en/new_pages/tables_descriptive.html#tbl_janitor) for more explanation of `adorn_xxx()` functions. 

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "Lake Minara",
  "Feveria Central",
  "Kasara"
)

cat("Which district reported the most vector-borne disease in 2024 (malaria, dengue, yellow fever)?", longmcq(opts))

opts <- c(
  "Lake Minara",
  answer = "Feveria Central",
  "Kasara"
)

cat("Which district reported the most diarrhoeal disease in 2024 (cholera, typhoid fever)?", longmcq(opts))


opts <- c(
  answer = "Unreliable water and sanitation infrastructure",
  "Overcrowding of mosquitoes",
  "We don't know"
)

cat("What factors contribute to increased diarrhoeal disease in this specific district (selected in previous question)?", longmcq(opts))

```
:::
::::

::: {.callout-tip collapse="true"}
## Click to read a hint

Here is some code to get you started. It cross-tabulates `disease_notified` and `residential_district` with `tabyl()`, then adding `adorn_percentages()` converts these numbers to proportions with many decimals. You then need to pipe into `adorn_pct_formatting()` to convert into actual percentage formatting, and then `adorn_ns()` to add numbers back in in parentheses. Note that `adorn_xxx()` functions need to be applied in a specific order!
```{r}
tabyl(data_notif, disease_notified, residential_district) |>
  adorn_percentages()
```

For factors contributing to more diarrhoea - scroll up to earlier in the case study when the districts were first introduced!


:::


:::



::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Using `tabyl()`, we can see that most suspected cases of dengue, malaria, and
yellow fever were located in Lake Minara - the lake area with higher
density of mosquitoes and therefore vector-borne disease. Meanwhile the
majority of cholera and typhoid fever were in Feveria Central, the
over-populated urban area with water and sanitation infrastructure
issues that result in higher risk of flooding and drinking water
contamination during rainy weather.

```{r eval=T}
tabyl(data_notif, disease_notified, residential_district) |>
  adorn_percentages() |>
  adorn_pct_formatting() |>
  adorn_ns()
```
:::

## Step 5. Clean, consolidate, and describe the laboratory data

From your earlier work in step 3, you have found that the laboratory
data contains only testing data, and no patient information. The data is
already very clean, so we only need to standardise one column. We will also want to process the laboratory data frame to be one row per notification, so that it can be neatly linked to the notification data frame. 

### Step 5.1 Standardise the test results

#### **Task A: Convert any value with "P" to "Positive", "N" to "Negative", and "I" to "Indeterminate.** 

Create a new object `data_lab`. This will allow a more straight-forward
analysis and interpretation of results.

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Use `case_match()` to turn the different original values into
"Positive", "Negative", or "Indeterminate":

```{r eval=T}
data_lab <- data_lab_raw |> 
  mutate(value = case_match(value, 
                            c("P", "PO1", "PO139") ~ "Positive",
                            "N" ~ "Negative",
                            "I" ~ "Indeterminate"))
```

You can then double-check that the new values look correct by tabulating
and comparing the values in the original data frame and the clean one. Make sure that you used the letter 'O' and not the number '0'!

```{r, eval=T}
tabyl(data_lab_raw, value)
```

```{r, eval=T}
tabyl(data_lab, value)
```
:::

### Step 5.2 Consolidate to one row per test

#### **Task A: Review the number of samples with multiple rows**

We already know that some samples have multiple rows, and that this is
because the dengue test has three targets, with one row per target
result.

Now **find the number of samples with multiple rows**.

Do this as you did with the notifiable disease surveillance data, using
the `data_lab` object: first count the number of rows per sample, then
create a table to show the distribution of row numbers. Keep in mind
that each sample is identified by a sample ID.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "200", 
  answer = "215",
  "230"
)

cat("How many samples (unique `sample_id`s) are repeated across three rows?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

First, pipe from the lab data into the `count()` function, giving the
`sample_id` column as the only argument. This creates a table that
counts the number of rows per unique value of `sample_id`, shown in a
new column `n`. You can see for example that the `sample_id` "000e8eee"
has three rows, whereas the `sample_id` "001e1878" is only seen on one
row.

```{r, eval=F}

data_lab |> 
  count(sample_id) 
```

```{r, eval=T, echo=F}

data_lab |> 
  count(sample_id) |> 
  head()
```

Then tabulate the new column `n` with the `tabyl()`.

```{r, eval=T}

data_lab |> 
  count(sample_id) |> 
  tabyl(n)
```

You can even double-check that this only applies to the dengue tests by
adding in the `disease` column to the calculation. You can see that it
is only the dengue test that has 3 rows per sample.

```{r, eval=T}

data_lab |> 
  count(test, sample_id) |> 
  tabyl(test, n)
```
:::

#### **Task B: Consolidate to one row per sample_id, prioritising positive results** 

As you saw in section 3.2, your dengue RDT provides results for
three different targets: IgG, IgM, and NS.1. The results for each of these targets can be either
negative or positive. However, to simplify and consolidate your data,
you want to assign a single negative or positive label to each sample,
to indicate if the sample represents current infection.

```{r eval=T, echo=F}
data_lab |> 
filter(test=="Dengue NS1/IgG/IgM RDT") |> 
tabyl(target, value) |> flextable() |> autofit()
```

Your colleague Ben, who works in the lab, advises you on the cleaning as follows:
- A sample can be considered positive if NS.1 or IgM are positive (as both can represent acute infection)
- You can ignore IgG (because a positive result in the absence of positive NS.1 or IgM is indicative of immunity after a past resolved infection) 

Now you need to **consolidate the dengue test results to one row per
test, with one result value**. Use `filter()`, `arrange()`, and `slice()`, making sure any
sample positive for NS.1 or IgM is considered positive for dengue.
Create a new object called `data_lab_tests`

::: {.callout-tip collapse="true"}
## Click to read a hint

Try to apply the following to consolidate according to Ben's recommendation:

1)  Remove IgG Results: filter out rows where the target is "IgG" using
    `filter()` from `{dplyr}`.
2)  Prioritize positive IgM/NS1results: Group by `sample_id` and arrange
    rows with `arrange()` so any 'P' (positive) result appears first
3)  Filter to final status: Keep only the first row using `slice(1)` to
    get the positive or negative result for the sample.
:::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is the code to filter out the dengue IgG results, and then
consolidate the test result within each group of rows with the same `sample_id`,
prioritising positive results. You need to specify `desc` within
`arrange()`, as this means that the results will be in reverse
alphabetical order, meaning P will be at the top. Also, add the
`ungroup()` function at the end so that the new data is not grouped,
which could confuse further analyses.

```{r eval=T}
data_lab_tests <- data_lab |> 
  filter(target != "Dengue IgG") |> 
  group_by(sample_id) |> 
  arrange(desc(value)) |> 
  slice(1) |> 
  ungroup()
```

You can then double-check that the new object `data_lab_tests` has only
one row per test, using the combination of `count()` and `tabyl()` like
you did in Task A. This table shows you that all unique sample IDs are
only present in one row each:

```{r eval=T}
data_lab_tests |> 
  count(sample_id) |> 
  tabyl(n)
```
:::

#### **Task C: Deduplicate to one row per notification_id, prioritising positive results** 

Next, you check the number of tests per notification ID in your new
consolidated data. You can see that there are 26 rows with the same
notification id as another row, but only among cases tested with whole blood microscopy
for malaria.

```{r, eval=T}
data_lab_tests |> 
  count(test, notification_id) |> 
  tabyl(test, n)

```

You investigate further, looking at one example case with
`notification_id` "043228". This shows you that this one case was tested
twice, with two different samples, one week apart. The first result was
positive, and the second result was negative.

```{r, eval=T}
data_lab_tests |> 
  filter(notification_id == "043228")

```

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "All cases of different diseases get retested", 
  answer = "Some malaria cases get retested",
  "All malaria cases get retested"
)

cat("Which statement about the lab data is correct?", longmcq(opts))

opts <- c(
  answer = "Yes - we need one row representing the lab result per notification",
  "No - the data is sufficiently deduplicated"
)

cat("Will you need to deduplicate the lab data, to link with the notifiable disease surveillance data?", longmcq(opts))



```
:::
::::

If you answered that you need to deduplicate, you are correct!

**Deduplicate your data to have one row per notification ID**, prioritizing positive results, so that you can link to the notifiable disease surveillance data.

To do this, follow a similar process as you did in Task B, using the data frame produced by task B: 
- Group by `notification_id`
- Arrange by the test result value so that values starting with P are prioritised in the top row, followed by N (negative), and then I (indeterminate). 
- Then keep the first row within each group of `notification_id`s, using `slice()`. 
- When doing this, create a new object called `data_lab_cases`.

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is the code to deduplicate rows within each group of rows with the
same `notification_id`, prioritising positive results. Once again you
need to specify `desc` within `arrange()`. This works perfectly because
the desired priority order for results — positive, then negative, then
indeterminate — happens to align with reverse alphabetical order (P comes
before N, which comes before I, when sorted descending).

If your priority order was more complex or didn't match alphabetical
sorting (e.g., if "indeterminate" needed to come before "negative"),
you'd have to convert the result column into a factor and explicitly
define the desired order of its levels.

```{r eval=T}
data_lab_cases <- data_lab_tests |> 
  group_by(notification_id) |> 
  arrange(desc(value)) |> 
  slice(1)
```

You can then double-check that the new object `data_lab_cases` has only
one row per test, using the combination of `count()` and `tabyl()` like
you did in Task A. This table shows you that all unique sample IDs are
only present in one row each:

```{r eval=T}
data_lab_cases |> 
  count(notification_id) |> 
  tabyl(n)
```
:::

### Step 5.3 Simple descriptive analysis

Now we have two objects that we can use for analysis of laboratory data: `data_lab_tests` and `data_lab_cases`. 

#### **Task A: Count the number of disease-specific tests, positive results, and negative results in the 2024 laboratory data**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "data_lab_tests",
  "data_lab_cases",
  "neither"
)

cat("Which object should you use to analyse tests?", longmcq(opts))



opts <- c(
  "215", 
  answer = "503",
  "88",
  "190"
)

cat("How many tests were conducted to test for malaria (via whole blood microscopy)?", longmcq(opts))

opts <- c(
  "21%", 
  "11%",
  answer = "84%",
  "87%"
)

cat("What proportion of tests for cholera (via stool culture) were positive?", longmcq(opts))

opts <- c(
  answer = "IgM ELISA (for yellow fever detection)",
  "Stool Culture (for cholera detection)", 
  "Blood culture (for typhoid fever detection)"
)

cat("Which test had the highest proportion of indeterminate results?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Using `tabyl()`, we can see the number of positive, negative, and
indeterminate results per test. You can add a series of `adorn()`
functions to show percentages and totals.

```{r eval=T}
tabyl(data_lab_tests, test, value) |> 
  adorn_totals(where = "col") |> 
  adorn_percentages() |> 
  adorn_pct_formatting() |> 
  adorn_ns()
```
:::

#### **Task B: Count the number of suspected cases tested in the 2024 data**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "data_lab_raw", 
  answer = "data_lab_cases",
  "data_lab_tests",
  "data_lab"
)

cat("Which lab data frame should you use to count the number of suspected cases tested?", longmcq(opts))

opts <- c(
  answer = "858",
  "1314",
  "884"
)

cat("How many suspected cases were tested in the 2024 lab data?", longmcq(opts))

opts <- c(
  answer = "Notifiable disease surveillance data",
  "Lab data"
)

cat("Are there more suspected cases in the notifiable disease surveillance data or the lab data?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can simply look at the number of rows in the `data_lab_cases`
data frame to see the number of suspected cases who were tested. 

```{r eval=T}
nrow(data_lab_cases)
```

This is less than the number of suspected cases that were in the clean notifiable
disease surveillance data (`data_notif`) - which suggests that not all
suspected cases in 2024 were tested by the time this data was available.

```{r eval=T}
nrow(data_notif)
```


:::

## Step 6. Linkage and final processing

Now that both linelists are clean and have one row per suspected case,
you can link them to complete the analysis requested by your boss. 

### Step 6.1 Link the notifiable disease surveillance data and lab data

#### **Task A: Conduct the linkage** 

Create a new object called `data_linked`, using a `xxx_join()` function
from `{dplyr}`.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "left_join(data_notif, data_lab_cases...",
  "full_join(data_notif, data_lab_cases...",
  "right_join(data_notif, data_lab_cases..."
)

cat("Which function is the correct approach if you want to retain all rows from your notifiable disease surveillance data and bring in results from your lab data?", longmcq(opts))

opts <- c(
  "sample_id", 
  answer = "notification_id",
  "sample_id and date of report",
  "notification_id and date of report"
)

cat("What identifier should be used to link the two linelists?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You are lucky to work with a straight-forward example of linkage! Both
data frames have an identifier called `notification_id`, which is clean and
formatted in the same way. Usually you would need to really clean and
check this column.

The staff in the clinics have been **fantastic** at consistently
allocating notification IDs to each patient, including on the sample
forms sent to the lab, and then the lab staff have been equally
**brilliant** at recording the notification ID in their lab systems.

Link the data using the `left_join()` function, with notifiable disease
surveillance data as the main data frame on the left. This will keep all
the rows from this data frame, and will just bring in the test results
from the lab data specified on the "right" of the function.

```{r eval=T}

data_linked <- left_join(data_notif, data_lab_cases, by = "notification_id")
```
:::

#### **Task B: Check the linkage worked as expected**

Now check your data and review a few things:

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "987",
  "884",
  "858"
)

cat("How many rows are in your new `data_linked` data frame?", longmcq(opts))

opts <- c(
  "more rows than the original",
  answer = "same number of rows",
  "fewer rows"
)

cat("How does this compare to your original notifiable disease surveillance data?", longmcq(opts))


opts <- c(
  "many-to-one", 
  answer = "one-to-one",
  "many-to-many"
)

cat("What term best describes the linkage you just did?", longmcq(opts))

opts <- c(
  "30", 
  "19",
  answer = "0"
)

cat("How many lab results were NOT linked (hint: `use anti-join()`)?", longmcq(opts))

opts <- c(
  "What? Isn't all linkage this simple??",
  answer = "Very! Usually some records don't match"
)

cat("How fortunate are you that your linkage is so successful?", longmcq(opts))


opts <- c(
  "There are typos in the columns used for linkage, so they are not recognised as matching",
  "The lab data may contain additional cases from other clinics or regions",
  "The lab data may include test samples",
  "Notifications may have been accidentally missed in the surveillance data even though the sample was tested in the lab",
  answer = "All of the above"
)

cat("What are typical reasons that lab data doesn't match to the notifiable diseases data?", longmcq(opts))


opts <- c(
  "83",
  "100",
  answer = "129"
)

cat("How many suspected cases do not have a result?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Check the number of rows in each data frame with the `nrow()` function, or
by checking the object information in your environment. You can see that this
was simply a one-to-one merge, because each row had a unique
notification_id, so one row in the notifiable disease surveillance data
linked directly to one row in the lab data.

```{r eval=T}

nrow(data_notif)
```

```{r eval=T}
nrow(data_linked)
```

You answered previously that the number of suspected cases in the
`data_lab_cases` column was even less. You can also check with an
`anti_join()` if there were any lab results that were not linked into
the notifiable disease surveillance data. This time the `data_lab_cases`
object is on the left, as the function assess how many rows from the
left data frame were not found in the right data frame, matching by
`notification_id`. Here you do not need to generate a new data frame, you
can simply pipe into an `nrow()` to count the number of rows. The output
is 0, which shows there were no unlinked results - amazing!

```{r eval=T}
anti_join(data_lab_cases, data_notif, by = "notification_id") |> nrow()
```

Finally, to check the number of cases without a result, you can either
conduct an anti_join in putting `data_notif` first:

```{r eval=T}
anti_join(data_notif, data_lab_cases, by = "notification_id") |> nrow()
```

Or, you can simply tabulate the number of missing values in the `value` column
in `data_linked`. Both of these show that 129 are missing.

```{r eval=T}
tabyl(is.na(data_linked$value)) 
```
:::

### Step 6.2: Label confirmed, discarded, and suspected cases

#### **Task A: Create a final "case category" column**

**Use `mutate()` to create a new column `case_category`**, updating the
label of suspected cases according to their lab result. The categories
should be as follows:

-   If the result was positive: Confirmed
-   If the result was negative: Discarded
-   If the result was indeterminate or missing: Suspected

This means that all cases in the notifiable disease surveillance data
are initially suspected when reported, and then remain suspected if
there is no conclusive test result.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "case_when()",
  "if_else()",
  "case_match()"
)

cat("Which is the most appropriate function for creating this new column?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You should use `case_when()` to create the new column. This function is
ideal for applying multiple logical conditions to create multiple
values, whereas `case_match()` is better for replacing specific values,
and `if_else()` is better if there are only two possible values.

```{r eval=T}

data_linked <- data_linked |> 
  mutate(case_category = case_when(value=="Positive" ~ "Confirmed",
                                   value=="Negative" ~ "Discarded",
                                   value=="Indeterminate" | is.na(value) ~ "Suspected"))
```
:::

### Step 6.3: Review the distribution of laboratory results among cases

#### **Task A: Use `tabyl()` to cross-tabulate the case notifications with their test results**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "202",
  "347",
  "250"
)

cat("How many cases in the notifiable disease surveillance data did not have a positive or negative result?", longmcq(opts))

opts <- c(
  "60.1%",
  answer="79.5%",
  "92.2%"
)

cat("What proportion of cases in the notifiable disease surveillance data DID have a positive or negative result?", longmcq(opts))


opts <- c(
  answer = "Cholera",
  "Malaria",
  "Typhoid Fever",
  "Yellow Fever",
  "Dengue"
)

cat("Which disease had the highest proportion of confirmed cases?", longmcq(opts))



```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Once again you can use `tabyl()` to show the number of originally
suspected cases of cholera, dengue, malaria, typhoid fever, and yellow fever,
and their upgraded case category after linking with the lab data.

This time you can specify that `where = "both"` inside the
`adorn_totals()` function, so that you can see the total number of cases
per disease, displayed by disease category.

The total number of suspected cases, i.e. those with either no lab
result at all or with an indeterminate result, is 202. This means 785 cases, i.e. 79.5%, did have a definitive laboratory result.

```{r eval=T}
tabyl(data_linked, disease_notified, case_category) |> 
  adorn_totals(where = "both") |> 
  adorn_percentages() |> 
  adorn_pct_formatting() |> 
  adorn_ns()
```
:::

#### **Task B: Use `tabyl()` to assess the positive predictive value of the clinical definitions used to identify the notified cases**

The positive predictive value is calculated by dividing the number of
confirmed cases (i.e. those with a positive result) over the number of
suspected cases (i.e. those that were reported in the notifiable disease
surveillance system). Don't forget that we only want to consider cases that have a test result!

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "86%",
  answer = "41%",
  "23%"
)

cat("What was the positive predictive value of the clinical malaria diagnosis?", longmcq(opts))

opts <- c(
  answer = "87%",
  "41%",
  "23%"
)

cat("What was the positive predictive value of the clinical dengue diagnosis?", longmcq(opts))
```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

You can simply filter out the suspected cases from the cross-tabulation
you did above, to see the proportion of cases that become confirmed or
discarded among those with test results. You can see that almost all
clinically diagnosed cholera cases were eventually confirmed, whereas
only 23% of typhoid fever cases were confirmed.

```{r eval=T}

data_linked |> 
  filter(case_category != "Suspected") |> 
  tabyl(disease_notified, case_category) |> 
  adorn_totals(where = "both") |> 
  adorn_percentages() |> 
  adorn_pct_formatting() |> 
  adorn_ns()
```
:::

### Step 6.4: Create a linelist with just confirmed cases

**Task A: Create a new linelist called `data_linked_confirmed`**.

This is what you will use in official surveillance reporting.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer = "Confirmed cases provide more consistent and reliable data when positive predictive value is not high and lab testing is routine",
  "Because we want to hide the true number of cases"
)

cat("What is a valid reason to report only confirmed cases in our surveillance data?", longmcq(opts))

opts <- c(
  answer = "The positive predictive value of suspected cases is high so suspected cases are still reasonably accurate to report",
  "Because we want to hide the true number of cases"
)

cat("What is a valid reason to also report the suspected cases in our surveillance data?", longmcq(opts))

opts <- c(
  "389",
  answer = "438",
  "858"
)

cat("How many rows are in this new data frame?", longmcq(opts))




```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Your surveillance unit wants to focus on confirmed cases in reporting. This is because discarded cases are not relevant, and suspected cases eventually get their results, of which many are negative.

The decision to publish suspected cases may be different in
other contexts. For example, if the positive predictive value of
a suspected case is very high (i.e. most of them are true cases if
tested), and testing itself is not common, publishing suspected cases
can be informative.

Create the new linelist like this:

```{r, eval=T, include=T, results="asis"}
data_linked_confirmed <- data_linked |> 
  filter(case_category=="Confirmed")
```

And check the number of rows by looking at the information in your environment, or with `nrow()`:

```{r, eval=T, include=T, results="asis"}
nrow(data_linked_confirmed)
```
:::

## Step 7. Descriptive analysis of confirmed cases

Now that you have your linelist of confirmed notifiable diseases cases in 2024, you are ready to conduct your surveillance analysis of confirmed cases! Your primary tasks in your surveillance unit are to summarize the five notifiable diseases by geography and time. 

**Tip**: Typically surveillance analysis would also include analysis by person. You could expand on this case study by also analyzing by demographic variables.  

### Step 7.1 Describe cases by district

#### **Task A: Produce a table of confirmed cases by district using `tabyl()` and `adorn_xxx()` functions, including totals**

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}


opts <- c(
  answer = "Dengue",
  "Malaria",
  "Yellow Fever"
)

cat("Which notifiable disease was most commonly reported in 2024, when restricting to only confirmed cases?", longmcq(opts))

opts <- c(
  "The sensitivity and specificity of the clinical diagnosis may differ by disease",
  "The performance of the tests used in the lab may differ by disease",
  "There may be reporting biases"
  answer = "All of the above!"
)

cat("Why is the most commonly reported disease different when looking at confirmed and suspected cases?", longmcq(opts))



opts <- c(
  "Lake Minara",
  answer = "Feveria Central",
  "Kasara"
)

cat("Which district reported the most confirmed cholera cases in 2024?", longmcq(opts))

opts <- c(
  answer = "35",
  "42",
  "4"
)

cat("How many confirmed cases of cholera reported in 2024 were among residents of Feveria Central?", longmcq(opts))

opts <- c(
  "Lake Minara",
  answer = "Feveria Central",
  "Kasara"
)

cat("Which district reported the most confirmed malaria cases in 2024?", longmcq(opts))


opts <- c(
  "Yellow fever",
  answer = "Dengue",
  "Cholera"
)

cat("Overall, what notifiable disease is most reported in Feveria?", longmcq(opts))

opts <- c(
  answer = "No - a different disease may be under-reported and/or not notifiable",
  "Yes - if it's most reported then it must be most common"
)

cat("Does this data confirm that dengue is the most common infectious disease in Feveria?", longmcq(opts))



```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Again, because this is a cross-tabulation, we suggest you use `tabyl()` and the relevant `adorn_xxx()` functions.

We can see that dengue was the most commonly reported disease when restricting to confirmed cases, with 186 confirmed cases. Note that this is different from the suspected cases, where malaria was most commonly reported (with 533 suspected cases)! This was hinted at previously, when you saw that the positive predictive value for suspected dengue cases was higher than for suspected malaria cases. This can be for different reasons, for instance the clinical diagnosis method used for malaria may be less specific (so many of the suspected cases are actually other diseases), or the test used for dengue may be more sensitive.

Like with the suspected cases, we can see that most confirmed cases of dengue, malaria, and yellow fever were located in Lake Minara - the lake area with higher density of mosquitoes and therefore vector-borne disease. The majority of confirmed cholera and typhoid fever cases were in Feveria Central, where there are water and sanitation issue.

The data suggests that vector-borne disease (dengue and malaria) are a particular concern in this tropical country. However, we don't know for sure which is the most common disease and what the underlying patterns are - only five diseases are notifiable, and typically the reported cases only represent a fraction of true cases in the community. 


```{r eval=T}

data_linked_confirmed |> 
  tabyl(disease_notified, residential_district) |> 
  adorn_totals(where = "both") |> 
  adorn_percentages() |> 
  adorn_pct_formatting() |> 
  adorn_ns() 
```
:::

### Step 7.2 Describe cases over time

You are going to work towards producing this epicurve, over the various
tasks below.

```{r eval=T, echo=F, fig.width=10, fig.height=7}
data_linked_confirmed |> 
  ggplot()+
  geom_histogram((aes(x = date_report, fill = residential_district)), binwidth=7) +
  facet_wrap(.~disease_notified, ncol=2) +
  theme_minimal() + 
  labs(fill = "District of residence",
       x = "Date reported by clinic",
       y = "Count",
       subtitle = "Number of confirmed cholera, dengue, malaria, typhoid fever, and yellow fever cases by week in Feveria, 2024") +
  scale_fill_manual(values = c("navy", "lightblue", "seagreen")) +
  scale_x_date(date_breaks = "1 month", 
               date_labels = "%d %b") +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle=90)) 
```

#### **Task A: Start by using `ggplot()` and `geom_histogram()` to produce one overall epicurve for Feveria, showing case counts by week with different colors stacked by disease**

Make sure you specify the argument `binwidth=7` so that each bar in the histogram represents the number of cases within a 7 day period. 

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "January 2024",
  answer = "April 2024",
  "October 2024"
)

cat("When was the first case of typhoid fever in Feveria in 2024?", longmcq(opts))


opts <- c(
  "10",
  "20",
  "30",
  answer = "It's very hard to tell from this stacked graph!"
)

cat("According to this graph, what was the highest number of dengue cases reported in a single week in 2024?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is some simple code to produce the epicurve. Note that you are not controlling the colors just yet, or specifying what day of the week each 7-day period starts on. 
```{r eval=T}
data_linked_confirmed |> 
  ggplot()+
  geom_histogram((aes(x = date_report, fill = disease_notified)), binwidth=7)
```

Refer to the [dates chapter in the Epi R Handbook](https://epirhandbook.com/en/new_pages/dates.html) if you want more specific date formatting, for instance so that each bar represents a Monday-Sunday week, or the x axis labels the week number (weeks 1 - 52). 

Importantly - it is not straight forward to see the trends per disease when stacked this way! To see such temporal trends, you should produce plots per disease. 

:::

#### **Task B: Use `ggplot()` to produce a basic epicurve showing case counts by week, faceted (and not stacked) by disease**

Use `facet_wrap()` to easily create several mini-plots, one per disease. To understand this further, you can look at the [facet section of the ggplot2 chapter in the Epi R Handbook](https://epirhandbook.com/en/new_pages/ggplot_basics.html#ggplot_basics_facet) 

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "11",
  answer = "15",
  "29",
  "I still can't tell!"
)

cat("According to this faceted graph, what was the highest number of dengue cases reported in a single week in 2024?", longmcq(opts))


opts <- c(
  "All three districts",
  "Feveria Central",
  "Kasara",
  "Lake Minara",
  answer = "This graph does not show this information"
)

cat("Among the dengue cases reported that week, what districts did they live in?", longmcq(opts))

```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Now you can see an epicurve per disease! And you can see that during one week in July, 15 cases of dengue were reported. However, this graph does not show any geographical information yet. 

```{r eval=T}
data_linked_confirmed |> 
  ggplot()+
  geom_histogram((aes(x = date_report)), binwidth=7) + 
  facet_wrap(.~disease_notified)
```
:::

#### **Task C: Now add color to your faceted graph so that the bars are stacked by district**


:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer= "All three districts",
  "Feveria Central",
  "Kasara",
  "Lake Minara"
)

cat("Among the 15 dengue cases reported in one week in July 2024, what districts did they live in?", longmcq(opts))


opts <- c(
  "Kasara",
  answer = "Feveria Central",
  "Lake Minara",
  "I still can't tell!"
)

cat("In what district was the first typhoid fever case reported in 2024?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Now you can see an epicurve per disease, with the coloring
reflecting the district the case is a resident of.

You can see that during the week where there were 15

```{r eval=T}
data_linked_confirmed |> 
  ggplot()+
  geom_histogram((aes(x = date_report, fill = residential_district)), binwidth=7) + 
  facet_wrap(.~disease_notified)
```
:::

#### **Task D: Add further formatting to your faceted plot so it is publication-ready**

You can specify:

-   The theme/appearance of the overall graph (e.g. background color,
    appearance of grid lines)
-   The title and labels
-   The colors of the bars (with `scale_fill_manual()`)
-   The formatting and spacing of dates along the x-axis (with
    `scale_x_date`)
-   Many other things!

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  answer= "No - the data suggests smaller occasional outbreaks",
  "Yes they are both endemic"
)

cat("Do cholera and typhoid fever appear endemic?", longmcq(opts))


opts <- c(
  "Yes - around November/December time",
  answer = "Yes - around July/August (summer) time",
  "No, it is consistently high"
)

cat("Is there a particular time of the year when malaria peaked in 2024?", longmcq(opts))


```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Here is the fully formatted code. Note some other changes include
specifying that we only want two columns of mini-plots within
`facet_wrap()`, and that the date label along the x axis should only
show day and month (not year, since all cases are in 2024 anyway).

```{r eval=T}
data_linked_confirmed |> 
  ggplot()+
  geom_histogram((aes(x = date_report, fill = residential_district)), binwidth=7) +
  facet_wrap(.~disease_notified, ncol=2) +
  theme_minimal() + 
  labs(fill = "District of residence",
       x = "Date reported by clinic",
       y = "Count",
       subtitle = "Number of confirmed cholera, dengue, malaria, typhoid fever, and yellow fever cases by week in Feveria, 2024") +
  scale_fill_manual(values = c("navy", "lightblue", "seagreen")) +
  scale_x_date(date_breaks = "1 month", 
               date_labels = "%d %b") +
  theme(legend.position="bottom",
        axis.text.x = element_text(angle=90)) 
```

We can also see from the epicurve that cholera and typhoid appear to be occuring as isolated outbreaks, rather than showing endemicity. Malaria and dengue however were present in Feveria throughout the year, peaking in the summer months. 

:::

#### **Task E: Produce table summarising dates**

This time, use `group_by()` and `summarize()` to produce a table by
region showing the minimum and maximum dates. Then use `filter()` to look at one region at a time.

:::: {.callout-note icon="false"}
# Questions

::: webex-check
```{r, eval=T, include=T, results="asis", echo=FALSE}

opts <- c(
  "18th January 2024",
  answer = "17th January 2024",
  "12th February 2024"
)

cat("When was the first dengue case reported in **Feveria** in 2024?", longmcq(opts))

opts <- c(
  answer = "22nd August 2024",
  "18th November 2024",
  "25th December 2024"
)

cat("When was the last dengue case reported in **Feveria Central**?", longmcq(opts))
```
:::
::::

::: {.callout-caution icon="false" collapse="true"}
## Click to see the solution (try it yourself first!)

Group the data by disease and then summarize the first and most recent date to look at the overall timeline of each disease in Feveria.

```{r eval=T}
data_linked_confirmed |> 
  group_by(disease_notified) |> 
  summarize(first_reported = min(date_report), 
            recent_reported = max(date_report)) |>
  ungroup()
```

Add a `filter()` to the code to look at first and most recent report dates for the district you're interested in.

```{r eval=T}
data_linked_confirmed |> 
  filter(residential_district == "Feveria Central") |> 
  group_by(disease_notified) |> 
  summarize(first_reported = min(date_report), 
            recent_reported = max(date_report)) |>
  ungroup()
```
:::

## Conclusion

Wow! You have made fantastic progress cleaning, linking, and preparing notifiable disease surveillance and laboratory data for analysis. The data is now much cleaner and more organized, and you have already begun to uncover meaningful insights. In response to your boss' original four questions, you can say:

- **How many suspected cases of the different notifiable diseases were reported in 2024, and which was most common?**: Malaria was the most common notifiable disease in Feveria in 2024, reported through the notifiable disease surveillance system: There were 533 suspected cases of malaria reported, 273 suspected cases of dengue, 100 yellow fever, 46 cholera, and 35 typhoid.
- **What percentage of them ended up being confirmed?** Almost 80% of notifiable cases reported in 2024 had a laboratory test result by the time the linked dataset was created, with some variation by disease. In total, 56% of notified cases were eventually confirmed, but this ranged from only 23% for typhoid fever (7 confirmed of 31 suspected cases with test results), to 95% for cholera (38 confirmed of 40 suspected cases with rest results). Additionally, the positive predictive value for suspected dengue was higher than for suspected malaria (87% vs 41%).
- **How many confirmed cases of different notifiable diseass were reported in 2024, and which was most common?** Confirmed cases followed a slightly different trend to suspected cases: the most commonly reported infection was dengue with 186 cases, followed by malaria (174), then cholera (38), yellow fever (33), and typhoid fever (7). 
- **How are confirmed cases geographically and temporally distributed in Feveria?** Feveria experienced dengue and malaria transmission throughout the year, peaking in the summer, and concentrated in the Laka Minara district. Feveria also experienced small and infrequent outbreaks of diarrhoeal disease, e.g. cholera and typhoid fever, particularly in the urban Feveria Central where there can be issues with water and sanitation. 

Throughout this process, you practiced key R functions to clean, reshape, and link data frames, plus created new columns using logical conditions. You also created graphs and tables to investigate and describe your data. Finally, you performed thorough data quality checks to ensure reliable results, and you compared disease trends across sources before and after linkage, gaining a deeper understanding of how the surveillance system is structured.

There is so much more potential ahead. You can explore disease patterns by age or sex, calculate disease rates with population data, and even analyze reporting delays by examining the different dates in your datasets.

You have built a strong foundation and are well equipped to take your analysis to the next level. Keep going — exciting discoveries await!


To learn more, check out the other case studies or dive into the [Epi R Handbook](https://www.epirhandbook.com).

## Case study information

::: {.callout-note appearance="minimal" icon="false"}
**Original authors**: Paula Blomquist and Alanah Jansen \| **Data source**: Fictional data provided by Applied Epi, with input from CDC DGHP GSLDSB 
:::

| Date      | Changes made | Version | Author                            |
|-----------|:-------------|--------:|-----------------------------------|
| July 2025 | First draft  |       1 | Paula Blomquist and Alanah Jansen |

## Terms of Use

**Disclaimer**: The information presented in this exercise and the associated data files have been developed to help learners achieve the intended learning objectives. 

**License**: This case study is under a [CC BY-NC-SA 4.0 license](https://github.com/appliedepi/case_studies/tree/master/licenses/multi_disease_lab_LICENSE.md). For more information about sharing and adapting this case study, see the [associated deed](https://creativecommons.org/licenses/by-nc-sa/4.0/).

**Funding**: This case study was supported by Grant or Cooperative Agreement number NU2HGH00004 funded by the Centers for Disease Control and Prevention. Its contents are solely the responsibility of the authors and do not necessarily represent the official views of the Centers for Disease Control and Prevention, the Department of Health and Human Services, The Task for for Global Health, Inc. or TEPHINET. 