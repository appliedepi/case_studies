[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AppliedEpi cases studies",
    "section": "",
    "text": "Case-studies Open Repository\nObjective : In this repository can help you to develop your epidemiological through case studies covering common epidemiological situations and methods.\nWritten by epidemiologists, for epidemiologists\nApplied Epi is a nonprofit organisation and grassroots movement of frontline epis from around the world. We write in our spare time to offer this resource to the community. Your encouragement and feedback is most welcome:\nSee below the list of case studies available in the repository"
  },
  {
    "objectID": "index.html#authors",
    "href": "index.html#authors",
    "title": "AppliedEpi cases studies",
    "section": "Authors",
    "text": "Authors"
  },
  {
    "objectID": "pages/fulton.html",
    "href": "pages/fulton.html",
    "title": "Fulton (EN)",
    "section": "",
    "text": "Import data and cleaning\nIn this section we will see how to create an automatic and dynamic report to present a descriptive analysis of the data previously imported and cleaned.\nIn this section we will show how to include in the report an analysis of the risk factors for Covid-19 mortality.\nNow that you have finished this case study, reflect on the different steps taken and how they can be relevant for your day to day work. You may do so by asking yourself the following questions:\nData quality and limitations\nAnalysis\nR code"
  },
  {
    "objectID": "pages/fulton.html#overview",
    "href": "pages/fulton.html#overview",
    "title": "Fulton (EN)",
    "section": "Overview",
    "text": "Overview\n\n\n\nCase study characteristics\n\n\n\n\n\nName\nFulton County\n\n\nLanguage\nEnglish\n\n\nTool\nR\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nCOVID-19\n\n\nKeywords\nCOVID-19; SARS-COV-2; Outbreak\n\n\nTechnical complexity\nIntermediate\n\n\nMethodological complexity\nBasic\n\n\n\nAuthorship\nOriginal authors: Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer\nData source: Anonymised and jittered data provided by Fulton County for training purposes\nAdapted by: Alberto Mateo Urdiales to the case study template"
  },
  {
    "objectID": "pages/fulton.html#instructions",
    "href": "pages/fulton.html#instructions",
    "title": "Fulton (EN)",
    "section": "Instructions",
    "text": "Instructions\n\nGetting Help\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\nHints and Solutions\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\nPosting a question in the Community Forum\n… description here about posting in Community…\n\n\nTerms of Use\nXXXXXXXXXXXXXXXXXXXXX\n\n\n\nFeedback & suggestions\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\nVersion and revisions\nThe first version was written by Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer in August 2021.\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nMar 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGuidance\n\nBackground and Objectives of this case study\nThis is a an example R-markdown script which demonstrates how to create an automated outbreak situation report for COVID-19 in Fulton county, USA. The data used comes from an anonymised and fake (scrambled) linelist of COVID-19 cases in Fulton county from the beginning of the pandemic (early 2020) until July 2021.\nThe overall objective is to create an automatic and dynamic report that shows the COVID-19 epidemiological situation in Fulton County.\nIn this case study you will learn:\n\nHow to import, clean and analyse your data.\n\nCarry out descrptive analysis by time, place and person.\n\nUse the above to create an automatic and dynamic report in word using Rmarkdown.\n\n\nFor the purpose of the case study we separate this by descriptive analysis and visualisation (normally this would be mixed together of course). The visualisation section is organised in to place, time and person. This is to simplify flow for didactic delivery.\nAnalysis is loosely based off the monthly epidemiology reports for Fulton county\n\n\n\nPrevious level of expertise assumed\nUsers should have some prior experience with R, including:\n\nR basics: Several packages are required for different aspects of analysis with R. You will need to install these before starting. We install and load packages using the {pacman} package. Its p_load() command will install packages if necessary and load them for use in the current session. This might prove difficult if you have limited administrative rights for your computer. Making sure your IT-department gives you the correct access can save a lot of headache. See this handbook pages on the basics of installing packages and running R from network drives (company computers) for more detail. https://epirhandbook.com/r-basics.html#installation https://epirhandbook.com/r-on-network-drives.html#r-on-network-drives\nR projects: See Chapter 6 R Projects from the EpiRhandbook\nImport and export of data: See Chapter7 Import and export\n\n\n\nPreparation for the case study\n\nDownload folder fulton_en and extract contents in the local laptop\nOpen the Rstudio project inside the folder called fulton_en.Rproj\nInside the folder you can find the Rmd and the word output (weekly report). You can also find a word template that will be used as the template for the report. The Rmd and the output are there to help you if you struggle, but you should try to recreate these yourself following this case study.\nSubfolder data contains fulton COVID-19 data needed for the analysis\nSubfolder solution_materials has a copy of the Rmd document with the solution and a copy Word document with the output requested\nOpen a new Rmarkdown file in RStudio and save it in the root folder fulton_en. If you have any doubts about how to create an Rmarkdown follow the EpiRhandbook instructors here\nThis Rmarkdown file will be the file used throughout the case study and, rendering it will produce the weekly report in word format"
  },
  {
    "objectID": "pages/fulton.html#step-1-rmarkdown-set-up",
    "href": "pages/fulton.html#step-1-rmarkdown-set-up",
    "title": "Fulton (EN)",
    "section": "Step 1: Rmarkdown set up",
    "text": "Step 1: Rmarkdown set up\nRemember that this case study is created in Rmarkdown and that code goes within “chunks”, which is different from a standard R script. The first steps will be to define the language in which you want the report, the default chunk options and to install/load the necessary packages.\n\nStep 1.1: Define R language\nDepending on where you are and how to carried out R installation, your language “locale” might be different from the language of the report that you want to produce. For example, a french person might have a french “locale”. If that is the case, when creating a graph by day of the week, Monday will be displayed as “lundi”. If that french person wants to create an English report, as for this case study, the language “locale” should be changed.\nTask: Ensure your “locale” is in English and change it into English if it is not.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\n\nStep 1.2: Default chunk options\nChange the default chunk options of your Rmarkdown script to:\n\nhide all code chunks in the report\ndo not show messages or warnings\nshow errors if they appear, but to not stop the rendering\nset up the default figure width to 7 and the figure height to 6\nto show the figure titles on top of the plots by default\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# hide all code chunks in the output, but show errors \nknitr::opts_chunk$set(echo = FALSE,  # hide all code chunks in output\n                      error = TRUE,  # show errors if they appear, but don't stop (produce the word doc)\n                      warning = FALSE, # do not show warnings in the output word doc \n                      message = FALSE, # do not show  messages in the output word doc\n                      fig.width = 7,         # Figure width\n                      fig.height = 6,        # Figure height\n                      fig.topcaption = TRUE  # show figure titles on top of plot\n                     )\n\n\n\n\n\nStep 1.3: Install/load packages\nInstall the following packages that will be needed to carry out the analysis: officedown, officer, rio, here, skimr, janitor, lubridate, epikit, tidyverse, flextable, sf, scales, gtsummary, labelled, ggspatial, patchwork, apyramid and incidence2.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  officedown, # format MS word document output\n  officer,    # add table of contents to output\n  rio,        # importing data  \n  here,       # relative file pathways \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # age_categories() function\n  flextable,  # converting tables to pretty images\n  sf,         # manage spatial data using a Simple Feature format\n  scales,     # define colour schemes for flextables \n  gtsummary,  # summary statistics, tests and regressions \n  labelled,   # create variable labels to be displayed in table outputs\n  ggspatial,  # basemaps and scalebars \n  patchwork,  # combining multiple ggplots \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n\n)"
  },
  {
    "objectID": "pages/fulton.html#step-2-data-import-and-exploration",
    "href": "pages/fulton.html#step-2-data-import-and-exploration",
    "title": "Fulton (EN)",
    "section": "Step 2: Data import and exploration",
    "text": "Step 2: Data import and exploration\n\nStep 2.1: Data import\n\nImport the COVID-19 linelist called covid_example_data.xlsx that can be found in the following path: data/covid_example_data/.\nImport also the csv files named fulton_population.csv found in data/covid_example_data needed to retrieve the population in Fulton County.\n\n\n\nClick to see a solution code (try it yourself first!)\n\n\n\nlinelist_raw &lt;- rio::import(\n  file = here::here(\"data\", \"covid_example_data\", \"covid_example_data.xlsx\"),\n  which = \"in\"\n)\n\n# import population data by zipcode to calculate incidence\npop &lt;- import(\n     here(\"data\", \"covid_example_data\", \"fulton_population.csv\")\n)\n\n\n\n\n\nStep 2.2: Data exploration\nExplore the linelist to understand better the data.\n\nQuestion 2.1: How many rows are present in linelist_raw?\n\n 48 31 82101 5\n\nQuestion 2.2: How many columns are of class numeric?\n\n 8 4 19 31\n\n\n\n\nClick to see a solution code (try it yourself first!)\n\n\n\n# view your whole dataset interactively (in an excel style format)\nView(linelist_raw)\n\n# get mean, median and max values of numeric variables; counts for categorical variables and NAs with summary\nsummary(linelist_raw)\n\n# get information about each variable in a dataset \nskim(linelist_raw)\n\n# view unique values contained in variables - useful for categorical variables\nunique(linelist_raw$case_gender)"
  },
  {
    "objectID": "pages/fulton.html#step-3-data-cleaning",
    "href": "pages/fulton.html#step-3-data-cleaning",
    "title": "Fulton (EN)",
    "section": "Step 3: Data cleaning",
    "text": "Step 3: Data cleaning\n\nStep 3.1: Create date objects\nCreate an object called surveillance_date defined as 7 days prior to the reporting date (30 June 2021). Then, create another object rounding it to the closest Wednesday. Create two daily sequences of dates, one as the 14 days prior to the surveillance_date and another as 14-28 days prior to the same date. We will use these throughout the case study\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# create a date object for the surveillance\n# Minus 7 days from the date of report (see YAML) to account for lag in reporting lab results\nsurveillance_date &lt;- as.Date(\"2021-06-30\") - 7\n\n# create an epiweek object from the date \n# floor_date() rounds down to the closest week here\nsurveillance_week &lt;- floor_date(surveillance_date,\n                          # round by weeks\n                          unit = \"week\", \n                          # define week to start on Wednesday\n                          week_start = 3)\n\n# define recent (past 14 days) and previous (28 to 14 days prior)\nrecent_period   &lt;- seq(surveillance_week  - 13, surveillance_week, by = 1)\nprevious_period &lt;- seq(surveillance_week  - 27, surveillance_week - 14, by = 1)\n\n# define a text label of date range for the recent period (for table headers)\nrecent_period_labels &lt;- str_glue(\n  format(min(recent_period), format = \"%m/%d\"), \n  \"-\", \n  format(max(recent_period), format = \"%m/%d\")\n)\n\n# define text label of date range for previous period (for table headers) \nprevious_period_labels &lt;- str_glue(\n  format(min(previous_period), format = \"%m/%d\"), \n  \"-\", \n  format(max(previous_period), format = \"%m/%d\")\n)\n\n\n# define a label for past 28 days (for table captions)\nfull_period_labels &lt;- str_glue(\n  format(min(previous_period), format = \"%B %d\"), \n  \"-\", \n  format(surveillance_week, format = \"%B %d, %Y\")\n)\n\n\n\n\n\nStep 3.2: Clean column names\nClean the column names ensuring that names do not contain special characters. Rename the following columns from the raw data:\n\nDate of report (reprt_creationdt_FALSE) to date_report\nDate of birth (case_dob_FALSE) to date_dob\nDate of symptom onset (sym_startdt_FALSE) to date_onset\nDate of positive testing (pos_sampledt_FALSE) to date_positive\nDate of recovery (sym_resolveddt_FALSE) to date_recovery\nDate of hospitalisation (hosp_admidt_FALSE) to date_hospitalized\nDate of discharge (hosp_dischdt_FALSE) to date_discharge\nDate of death (died_dt_FALSE) to date_died\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist_raw %&gt;% \n     clean_names() %&gt;% \n     # NEW name = OLD name\n  rename( \n    date_report         = reprt_creationdt_false,      \n    date_dob            = case_dob_false,              \n    date_onset          = sym_startdt_false,\n    date_recovery       = sym_resolveddt_false, \n    date_hospitalized   = hosp_admidt_false,\n    date_discharge      = hosp_dischdt_false,\n    date_died           = died_dt_false,\n    date_positive       = pos_sampledt_false\n    )\n\n\n\n\n\nStep 3.3: Remove duplicated rows\nRemove rows that have duplicated information on: patient id, gender and date of birth. Keep duplicates in a separate dataframe.\n\n\n Click to read a hint\n\n\nTo store duplicates in a new dataframe you can use the function get_dupes() from the {janitor} package\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# get a data frame of all the duplicates. This is mostly to inspect manually, but can be used for analysing those dropped\nduplicates &lt;- linelist %&gt;% \n     get_dupes(pid, case_gender, date_dob)\n\n# find duplicates based on unique ID, gender and date of birth. Only keep the first occurrence \nlinelist &lt;- linelist %&gt;% \n  distinct(pid, case_gender, date_dob, .keep_all = TRUE)\n\n\n\n\nQuestion 3.2: How many duplicated rows were present in the raw data?\n\n 28 31 38 124\n\n\n\n\nStep 3.4: Change column class and remove data inconsistencies\nUsing the across() function from {dplyr} make the following:\n\nEnsure that dates are considered dates by R\nClean date columns dealing with values that are not compatible with the period under analysis (early 2020 to July 2021)\nMake the column age of numeric class\nSet us NA those with negative ages and missing Date of birth\nMake the zip code column a factor class column\n\n\n\n Click to read a hint\n\n\nThe across() allows to apply the same modification to multiple columns in an easy way. So, these two options are equivalent:\n\n# Without across()\n\nlinelist &lt;- linelist %&gt;% \n  mutate(date_report = as.Date(date_report)) %&gt;% \n  mutate(date_dob = as.Date(date_dob)) %&gt;% \n  mutate(date_onset = as.Date(date_onset)) %&gt;% \n  mutate(date_hospitalized = as.Date(date_hospitalized)) %&gt;% \n  mutate(date_discharge = as.Date(date_discharge)) %&gt;% \n  mutate(date_died = as.Date(date_died)) %&gt;% \n  mutate(date_positive = as.Date(date_positive))\n\n\n# With across()\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = contains(\"date\"), .fns = ~as.Date(.x)))\n\nYou can read more about across() in the EpiRhandbook section\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(\n    .cols = contains(\"date\"),\n    .fns = ~as.Date(.x)\n  )) %&gt;%\n  \n  # mark as missing onset dates prior to 2020\n  mutate(across(\n    .cols = c(date_report, date_onset, date_hospitalized, date_discharge, date_died),\n    .fns  = ~replace(.x, .x &lt; as.Date(\"2020-01-01\"), NA)\n    )) %&gt;% \n\n  # mark as missing dates after the surveillance_date (for this report) from all date columns\n  mutate(across(\n    .cols = contains(\"date\"),\n    .fns  =  ~replace(.x, .x &gt; surveillance_date, NA)\n    )) %&gt;%\n     \n  # transform age into numeric class\n  mutate(\n    # ensure that age is a numeric variable\n    case_age = as.numeric(case_age),\n    # set those with negative ages and missing DOB to missing \n    # otherwise just leave the age value as is\n          # nb. NA_real_ just ensures the variable class is not changed\n    case_age = if_else(case_age &lt; 0 & is.na(date_dob), NA_real_, case_age)\n  ) %&gt;% \n     \n  # create a factor from a default numeric class\n  mutate(case_zip = as_factor(case_zip)) \n\n\n\n\nQuestion 3.3: Which one of the following could NOT be used to transform the column sym_startdt_FALSE from the raw data frame into a date object?\n\n base::as.Date() lubridate::as_date() lubridate::ymd() lubridate::dmy()\n\n\n\n\nStep 3.5: Create a column for weeks\nCreate a column named “epiweek” using the function floor_date() from the {lubridate} package rounding the report date to the nearest week, taking “Wednesday” as the start of the week.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n  # create an \"epiweek\" column from the report date. Use floor_date() to round down to the closest week\n  mutate(epiweek = floor_date(date_report,\n                          # round by weeks\n                          unit = \"week\", \n                          # define week to start on Wednesday\n                          week_start = 3)\n  )\n\n\n\n\n\nStep 3.6: Create time difference columns\nIn this step we ask you to create columns with various time differences that will be used later on in the case study. Please, try to create:\n\nA column with the number (numeric) of days from date of symptom onset to the date of hospitalization\nIn this new column, set as missing those cases where the difference is longer than 30 days (interval is too long for the hospitalization to be due to the infection), and those less than 0 (cannot be hospitalized before the symptom onset)\nUsing the function coalesce() from {dplyr} create a new column for the date of outcome among hospitalized cases, using date of death or date of discharge, depending on whether cases died or not\nCreate a new column with the length of hospitalization in days, calculated as the time difference between date of hospitalization and the recently created date of outcome.\nIn this newly created column mark as missing cases in which the difference between the date of hospitalization and the date of death/discharge was longer than 60 days or lower than 0 days\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;%\n     \n  # delay from onset to hospitalization\n  mutate(\n    # calculate time differences\n    days_onset_hosp = as.numeric(date_hospitalized - date_onset),\n    # set those under 0 or over 30 to missing\n    days_onset_hosp = replace(days_onset_hosp, days_onset_hosp &lt; 0, NA),\n    days_onset_hosp = replace(days_onset_hosp, days_onset_hosp &gt; 30, NA)\n  ) %&gt;%\n     \n  # length of hospitalization\n  mutate(\n    # create outcome date based on whether died or was discharged\n    date_outcome = coalesce(date_died, date_discharge),\n    # calculate time difference\n    days_hosp = as.numeric(date_outcome - date_hospitalized),\n    # set those under 0 or over 60 to missing\n    days_hosp = replace(days_hosp, days_hosp &lt; 0, NA),\n    days_hosp = replace(days_hosp, days_hosp &gt; 60, NA)\n  )\n\n\n\n\n\nStep 3.7: Create age groups\nCreate a column with 10 year age groups up until 70 (and 70+ afterwards) using the age_group() function from the package {epikit}. You can also use any other alternative\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;%\n     # create age group variable\n     mutate(\n       age_group = age_categories(case_age,\n        # define break points\n        c(0, 10, 20, 30, 40, 50, 60, 70),\n        # whether last break should be highest category\n        ceiling = FALSE\n     ))\n\n\n\n\n\nStep 3.8: Recode character/categorical columns\nRecode the following columns:\n\nIn the column named died_covid recode the category “Under Review” to “Unknown”\nIn the column named confirmed_case recode the category “Pending” to “Unknown”\nForce categorical columns to use consistent cases\nAcross character/factor columns recode the category “Unk” to “Unknown”\nAcross the different character/factor columns recode NA to “Unknown”\nIn the column named sym_resolved recode categories into “Yes”, “No” or “Unknown”\nTransform the gender column into a factor with these levels: “Female”, “Male” and “Unknown”\nTransform all columns that have the categories: “Yes”, “No” and “Unknown” into factors with the order of the levels as “Yes”, “No” and “Unknown”\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n     \n     # recode one value and leave the rest as they are \n     mutate(\n       died_covid = if_else(died_covid == \"Under Review\",\n                            \"Unknown\", died_covid), \n       confirmed_case = if_else(confirmed_case == \"Pending\", \n                                \"Unknown\", confirmed_case), \n     \n        # force categorical variables to use consistent cases (this can be done for others) \n        sym_myalgia = str_to_title(sym_myalgia),\n      ) %&gt;% \n     \n     #replace one value and leave the rest, across multiple variables\n      mutate(across(\n       .cols = c(contact_household, contains(\"sym_\")),\n       .fns  = ~if_else(.x == \"Unk\", \"Unknown\", .x)\n     )) %&gt;% \n     \n        # replace missing with \"Unknown\" where relevant \n     mutate(across(\n       .cols = c(case_gender, case_race, case_eth, case_zip,\n                 contact_id, contact_household, \n                 hospitalized, died, died_covid, confirmed_case,\n                 contains(\"sym_\"), age_group),\n       .fns  = ~fct_na_value_to_level(.x, level = \"Unknown\")\n     )) %&gt;% \n     \n          # recode with searching for string patterns \n     mutate(sym_resolved = case_when(\n          str_detect(sym_resolved, \"Yes\")     ~ \"Yes\", \n          str_detect(sym_resolved, \"No\")      ~ \"No\", \n          str_detect(sym_resolved, \"Unknown\") ~ \"Unknown\", \n          TRUE                                ~ \"Unknown\"\n     )) %&gt;% \n     \n      # set levels of a factor (define order)\n     mutate(case_gender      = fct_relevel(case_gender, \"Female\", \"Male\", \"Unknown\")) %&gt;% \n     \n          # set levels of all factors that are yes/no/unknown \n     mutate(across(\n          .cols = c(contact_household, hospitalized, died, died_covid,\n                    confirmed_case, contains(\"sym_\")), \n          .fns = ~fct_relevel(.x, \"Yes\", \"No\", \"Unknown\")\n     )) \n\n\n\n\n\nStep 3.9: Merge ethnicity and race\nThe linelist contains a column for ethnicity (case_eth) and a column for race (case_race). Create a new column merging information from these two existing columns. The new column should:\n\nContain a category “Hispanic, all races” when case_eth is “HISPANIC/LATINO”. For those cases where this condition is not met:\n\nShould have a category for those whose race is “Asian”, another for those whose race is “Black” and another for those whose race is “White”.\nCreate an “Other” category for the rest of races and an “Unknown” category for those with missing race\nEnsure all categories have consistent cases\n\nTransform the newly formed column into a factor with the “Unknown” category as the last level\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n          # create a composite category from race and ethnicitiy  \n     mutate(eth_race = case_when(\n          eth  == \"HISPANIC/LATINO\"                           ~ \"Hispanic, all races\", \n          race == \"ASIAN\"                                     ~ \"Asian, NH\", \n          race == \"BLACK\"                                     ~ \"Black, NH\",\n          race == \"WHITE\"                                     ~ \"White, NH\",\n      # find all instances of NATIVE (covers AMERICAN INDIAN/ALASKA NATIVE **AND** NATIVE HAWAIIAN/PACIFIC ISLANDER)\n          str_detect(race, \"NATIVE\")                          ~ \"Other, NH\",\n          race == \"OTHER\"                                     ~ \"Other, NH\", \n          TRUE                                                ~ \"Unknown\"\n     )) %&gt;% \n     mutate(eth_race = factor(eth_race, levels=c(\n          \"Black, NH\", \"White, NH\", \"Hispanic, all races\",\n          \"Asian, NH\", \"Other, NH\", \"Unknown\"\n     )))\n\n\n\n\nQuestion 3.4: A column that has ordinal data, what class should it have?\n\n logical character factor integer\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n\n\nStep 3.10: Filter data frame\nFilter the data to keep only confirmed cases whose date of report is not above the date of the report (June 30, 2021). Consider also keeping records with missing date of report.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n##############       FILTER     ##############        \n\n# store those which do not meet our filter criteria \ndropped &lt;- linelist %&gt;% \n     filter(confirmed_case != \"Yes\" |\n              date_report &gt; surveillance_date & \n                !is.na(date_report))\n\n\n# drop the cases that dont meet the criteria \nlinelist &lt;- linelist %&gt;% \n     filter(confirmed_case == \"Yes\" & \n              date_report &lt;= surveillance_date & \n                 !is.na(date_report))"
  },
  {
    "objectID": "pages/fulton.html#step-4-start-the-report-with-a-summary-of-the-findings",
    "href": "pages/fulton.html#step-4-start-the-report-with-a-summary-of-the-findings",
    "title": "Fulton (EN)",
    "section": "Step 4: Start the report with a summary of the findings",
    "text": "Step 4: Start the report with a summary of the findings\n\nWrite in rmarkdown three bullet points summarising the data we imported, showing the number of cases by the date of analysis, the number of hospitalisations and the number of deaths.\nWrite it in a dynamic way, so that the dates and numbers are updated automatically if you get a new updated dataset\n\n\n\nClick to see a solution (try it yourself first!)\n\n This is an example of how the code should look like in your rmarkdown file:"
  },
  {
    "objectID": "pages/fulton.html#step-5.-analysis-by-time",
    "href": "pages/fulton.html#step-5.-analysis-by-time",
    "title": "Fulton (EN)",
    "section": "Step 5. Analysis by time",
    "text": "Step 5. Analysis by time\n\nStep 5.1: Table weekly number of cases\nCreate a table with the number of cases per reporting week to see how the epidemic evolved by time in Fulton County\n\nQuestion 5.1: During which week do we observe the peak in cases by date of reporting?\n\n The week starting on March 02, 2021 The week starting on December 16, 2020 The week starting on January 13, 2021 The week starting on December 30, 2020\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# save a quick descriptive table of number of cases reported by week\nepiweek_table &lt;- linelist %&gt;% \n  # get counts and percentages \n  tabyl(epiweek) %&gt;% \n  # add the overall counts as a row\n  adorn_totals() %&gt;%  \n  # change from proportions to percentages (do not add a % sign)\n  adorn_pct_formatting(affix_sign = FALSE) \n\n# transform it into flextable for better visualisation\nepiweek_flextable &lt;- epiweek_table %&gt;% \n     qflextable()\n\n\n\n\n\nStep 5.2: Epicurve\nCreate an epicurve by reporting week, with the colour of the bins based on whether the cases were hospitalised or not\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n     # we first define the dataset to be used, the x axis which will be reporting week and the colour (fill) of the bins which will depend on hospitalisation outcome\nggplot(\n     data = linelist,\n     mapping = aes(\n          x = epiweek,\n          fill = hospitalized\n     )) + \n     \n     geom_histogram() + \n     \n     # we define that we want breaks by month and formated with scales::label_date_short()\n     scale_x_date(\n          date_breaks = \"month\",\n          labels = label_date_short()\n     ) +\n     \n     # we change the name of the different elements of the graph\n     labs(\n          x = \"\",\n          y = \"Weekly number of cases\",\n          fill = \"Hospitalised\",\n          caption = paste0(\"Data as of \", format(surveillance_date, \"%d %b %Y\"))\n          \n     ) + \n     \n     # we apply one of the predefined themes\n     theme_bw()"
  },
  {
    "objectID": "pages/fulton.html#step-6.-analysis-by-person",
    "href": "pages/fulton.html#step-6.-analysis-by-person",
    "title": "Fulton (EN)",
    "section": "Step 6. Analysis by person",
    "text": "Step 6. Analysis by person\n\nStep 6.1: Table with demographic information\nCreate a table summarising, with counts and percentages, the total cumulative number of cases and deaths, as well the cases and deaths notified in the last 28 days by demographic characteristics: sex, age and race.\n\nQuestion 6.1: In which age group do we observe the largest proportion of cumulative cases?\n\n 0-9 30-39 20-29 70+\n\nQuestion 6.2: In which race do we observe the largest proportion of deaths in the last 28 days?\n\n Black White Asian Hispanic\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# get counts tables for measures of interest \n############################################\n\n# we generate 3 summary tables and bind them together\n# summary demographic table for gender\ndem_gender &lt;- linelist %&gt;% \n  tabyl(gender) %&gt;% \n  select(Characteristic = gender, n, percent)\n\n# summary demographic table for age\ndem_age &lt;- linelist %&gt;% \n  tabyl(age_group) %&gt;% \n  select(Characteristic = age_group, n, percent)\n\n# summary demographic table for ethnicity and race\ndem_eth_race &lt;- linelist %&gt;% \n  tabyl(eth_race) %&gt;% \n  select(Characteristic = eth_race, n, percent)\n\n# bind all tables together\ntotal_cases &lt;- bind_rows(list(dem_gender, dem_age, dem_eth_race))\n\n# counts of new cases (last 28 days) \nrecent_cases &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter the linelist for dates on or after 28 days ago\n  .f = ~filter(linelist, \n          date_report &gt;= (surveillance_date - 28)) %&gt;% \n        # get counts based on filtered data\n        tabyl(.x) %&gt;% \n        # nb we dont keep the characteristic column because it would be duplicated\n        select(n_cases_recent = n,\n               perc_cases_recent = percent)\n  ) %&gt;%\n  bind_rows()\n\n# counts of total deaths \ntotal_deaths &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter for those who died \n  .f = ~filter(linelist, \n          died_covid == \"Yes\") %&gt;% \n        # get counts based on filtered data \n        tabyl(.x, show_na = TRUE) %&gt;%\n        select(n_deaths_total = n, perc_deaths_total = percent)\n  ) %&gt;% \n  bind_rows()\n\n# counts of new deaths (last 28 days)\nrecent_deaths &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter to those who died in the last 28 days\n  .f = ~filter(linelist, \n          died_covid == \"Yes\" & \n          date_died &gt;= (surveillance_date - 28)) %&gt;% \n        # get counts based on filtered data\n        tabyl(.x) %&gt;% \n        select(n_deaths_recent = n, perc_deaths_recent = percent) %&gt;% \n        # add in a variable column (used for colouring later) \n        mutate(variable = .x)\n  ) %&gt;% \n  bind_rows()\n\n\n# total counts for all of the above measures (not by demographic)\noverall &lt;- linelist %&gt;% \n  summarise(\n    # add in row label \n    Characteristic = \"Total\",\n    # counts of total cases \n    n_cases_total = n(),\n    # leave all percentages empty (would just be 100)\n    perc_cases_total  = NA, \n    # counts of new cases (last 28 days) \n    n_cases_recent = sum(date_report &gt;= (surveillance_date - 28)), \n    perc_cases_recent  = NA, \n    # counts of total deaths \n    n_deaths_total = sum(died_covid == \"Yes\"), \n    perc_deaths_total = NA, \n    # counts of new deaths (last 28 days)\n    n_deaths_recent = sum(died_covid == \"Yes\" & \n                          date_died &gt;= (surveillance_date - 28)),\n    perc_deaths_recent = NA, \n    # add in a variable column (used for colouring later) \n    variable = \"Overall\"\n  )\n\n\n# merge tables together \n#######################\n\n# combine all the demographic tables - side by side\ndemographics_counts &lt;- bind_cols(total_cases, recent_cases, total_deaths, recent_deaths) %&gt;% \n  # mutate each of the proportion columns to be percentages\n  mutate(across(\n    .cols = contains(\"perc\"),\n    .fns = ~round(.x * 100, digits = 1)\n    )) \n# add in the totals row at the top of the merged demographics table\ndemographics_counts &lt;- bind_rows(overall, demographics_counts)\n\n\n# define colour scheme \n######################\n\n# get the column numbers that are percentages (based on the name) \npercentage_cols &lt;- names(demographics_counts) %&gt;% \n  str_detect(\"perc\") %&gt;% \n  which()\n\n# define colour cut-offs for gender column \ngender_colours &lt;- scales::col_bin(\n  # choose colours \n  palette = c(\"#91CF60\", \"#FC8D59\"), \n  # choose min and max (range)\n  domain  = c(0, 100),\n  # choose how to split (in this case above and below 50)\n  bins    = 2\n)\n\n# define colour cut-offs for age column \nage_colours &lt;- scales::col_bin(\n  # choose colours\n  palette = c(\"#91CF60\",\"#FFFFBF\", \"#FC8D59\"),\n  # choose min and max (range)\n  domain  = c(0, 100), \n  # choose cut-off categories \n  bins    = c(0, 5, 20, 100)\n)\n\n# define colour cut-offs for ethnicity column \neth_colours &lt;- scales::col_bin(\n  palette = c(\"#91CF60\",\"#FFFFBF\", \"#FC8D59\"),\n  domain  = c(0, 100), \n  bins    = c(0, 10, 40, 100)\n)\n\n\n# create styled table  \n######################\n\ndemographics_counts %&gt;%\n  # initiate flextable to produce styled output table\n  flextable(\n    # retain variable column for formatting but do not display it\n    col_keys = names(demographics_counts)[-10]\n  ) %&gt;%\n  # redefine column names based on original names\n  set_header_labels(\n    \"n_cases_total\"       = \"Total Confirmed Cases\",\n    \"perc_cases_total\" = \"% of Total Cases\",\n    \"n_cases_recent\"       = \"Confirmed Cases past 28 days\",\n    \"perc_cases_recent\" = \"% of Confirmed Cases past 28 days\",\n    \"n_deaths_total\"       = \"Total Confirmed Deaths\",\n    \"perc_deaths_total\" = \"% of Total Deaths\",\n    \"n_deaths_recent\"       = \"Confirmed Deaths past 28 days\",\n    \"perc_deaths_recent\" = \"% of Confirmed Deaths past 28 days\"\n  ) %&gt;%\n  # move the header text to the centre\n  align(align = \"center\", part = \"header\") %&gt;%\n  # make header text bold\n  bold(part = \"header\") %&gt;%\n  # make the totals row bold (i.e. first row)\n  bold(i = 1, part = \"body\") %&gt;%\n  # fill in the cells\n  # choose the rows with gender counts\n  bg(i = ~variable == \"gender\",\n     # choose the columns with percentages in them\n     j = percentage_cols,\n     # fill in based on previous defined cut-offs\n     bg = gender_colours) %&gt;%\n  bg(i = ~variable == \"age_group\",\n     j = percentage_cols, bg = age_colours) %&gt;%\n  bg(i = ~variable == \"eth_race\",\n     j = percentage_cols, bg = eth_colours) %&gt;%\n  # add horizontal lines after the cells with totals and unknowns\n    # (short-cut to find row ending of each demographic variable)\n  hline(i = ~Characteristic %in% c(\"Total\", \"Unknown\")) %&gt;%\n  # add in footnotes for rows counting unknowns (reference in first column)\n  footnote(i = ~Characteristic == \"Unknown\", j = 1, part = \"body\", ref_symbols = c(\"a\"),\n           value = as_paragraph(\"Unknown includes cases not yet interviewed\")) %&gt;%\n  # add in footnote for deaths counts (ref in the header)\n  footnote(i = 1, j = c(6, 8), part = \"header\", ref_symbols = c(\"b\"),\n           value = as_paragraph(\"Deaths refer to all persons who had a positive PCR test result\n                                for Covid-19 and there is evidence that COVID-19 was the cause of\n                                death or a significant contributor to their death.\")) %&gt;%\n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\") %&gt;% \n  # decrease the fontsize in the header and body for aesthetic purposes in the document\n  fontsize(part = \"all\", size = 8)\n\n\n\n\n\nStep 6.2: Age pyramid\nCreate an age pyramid with the percentage of cases by age group and sex.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# prepare dataset\n\n# start a new dataframe (as dont want to overwrite the original)\nlinelist_2g &lt;- linelist %&gt;% \n  # update the gender and age_group columns\n  mutate(across(.cols = c(gender, age_group), \n                .fns = ~{\n                  # replace \"Unknown\" with NA\n                  .x = na_if(.x, \"Unknown\") \n                  # drop \"Unknown\" from the factor levels \n                  .x = fct_drop(.x)\n                }))\n\n# plot age pyramid \nage_pyramid(\n  data = linelist_2g,\n  age_group = \"age_group\",\n  split_by = \"gender\",\n  # Show as percentages of total cases\n  proportional = TRUE,\n  # remove guide line for mid-point\n  show_midpoint = FALSE) +\n  # set theme to basic \n  theme_minimal() +\n  # add labels \n  labs(\n    title = \"\",\n    subtitle = ,\n    x = \"Age group\",\n    y = \"Percent of total\",\n    fill = \"Gender\",\n    # use str_glue to set dynamic captions \n    # {missing} is defined in the second argument below\n    caption = str_glue(\n      \"{missing} cases missing either age or gender are not shown. \\n Fictional COVID-19 data\",\n      missing = linelist_2g %&gt;%\n        filter(is.na(gender) | is.na(age_group)) %&gt;%\n        nrow()\n      )\n    )\n\n\n\n\n\nStep 6.3: Scatter plot\nCreate a scatter plot showing the relation between age and duration of hospital stay. Colour the points based on whether cases died or not.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#################### C) SCATTER PLOT ####################\n# open a plot with the linelist data\nggplot(data = linelist) +\n  # add points \n  geom_point(\n    mapping = aes(\n      # plot age on the x and days hospitalised on the y axis \n      x = age,\n      y = days_hosp,\n      # color points by outcome\n      color = died),  \n    # all points 3x size\n    size = 3, \n    # opacity of 30% (i.e. relatively see-through)\n    alpha = 0.3) +      \n  # make the x and y axes start at the origin \n  scale_y_continuous(expand = c(0, 0)) + \n  scale_x_continuous(expand = c(0, 0)) + \n  # add in labels \n  labs(\n    x = \"Age (years)\",\n    y = \"Duration (days)\",\n    caption = \"Fulton COVID-19 data\",\n    color = \"Deceased\"\n    ) + \n     theme_bw()\n\n\n\n\n\nStep 6.4: Bar plot\nCreate a bar stacked bar plot showing the absolute number of cases by race and vital status\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# open a plot with the linelist data\nggplot(linelist) +\n  # add bars \n  geom_bar(\n    mapping = aes(\n      # plot the number of cases by ethnicity (ordered in reverse frequency)\n      x = fct_rev(fct_infreq(eth_race)),\n      # stack bars and colour by died (ordered in reverse frequency)\n      fill = fct_rev(fct_infreq(died))\n    )\n  ) +\n  # flip the x and y axes \n  coord_flip() +\n  # make the x axes start at the origin (nb axes flipped)\n  scale_y_continuous(expand = c(0, 0), \n                     # define where to label xaxis (nb axes flipped )\n                     breaks = seq(from = 0,\n                                  to = 35000,\n                                  by = 5000)) + \n  # add in labels \n  labs(\n    # set the axes titles (nb axes flipped)\n    x = \"Race and Ethnicity\",\n    y = \"Cases (n)\",\n    caption = \"Fictional COVID-19 data\",\n    fill = \"Deceased\"\n    ) + \n  # apply a defined theme\n     theme_bw()"
  },
  {
    "objectID": "pages/fulton.html#step-7.-analysis-by-place",
    "href": "pages/fulton.html#step-7.-analysis-by-place",
    "title": "Fulton (EN)",
    "section": "Step 7. Analysis by place",
    "text": "Step 7. Analysis by place\nCreate a table by zip code in which you show the incidence in the most recent 14 days period, the incidence in the previous 14 days period and the percentage change in incidence between these periods.\n\nQuestion 7.1: What is the change in incidence observed between periods in the zip code number 30337?\n\n +20% +36% -62.5% -25%\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n################### TABLE BY ZIP CODE\n\nzip_counts &lt;- linelist %&gt;% \n  group_by(zip) %&gt;% \n  # count cases in the appropriate period \n  summarise(\n    recent   = sum(date_report %in% recent_period),\n    previous = sum(date_report %in% previous_period)\n  ) %&gt;% \n  adorn_totals() %&gt;% \n  # a percentage change column and round the digits\n  mutate(\n    perc_change = round((recent - previous) / previous * 100, digits = 1)\n    )\n\n# extract population counts for each zip from the shapefile\nzip_pop &lt;- shapefile %&gt;% \n  # change to tibble (otherwise geo-data gets pulled with)\n  as_tibble() %&gt;% \n  # only keep zip code and population counts\n  select(ZipCode, Population) %&gt;% \n  # add a row with overall counts\n  adorn_totals()\n  \n# merge case counts and population counts\n# zip (or ZipCode in the shapefile) variable is the unique identifier\nzip_counts &lt;- left_join(zip_counts, \n                        zip_pop, \n                        by = c(\"zip\" = \"ZipCode\")\n                        ) %&gt;% \n  # calculate the incidence \n  mutate(across(\n      # for each period (recent and previous)\n      .cols = c(recent, previous), \n      # divide each variable by population (and round the outcome)\n      .fns = ~round(.x / Population * 10000, digits = 1), \n      # for each period create a new variable with _inc on the end\n      .names = \"{.col}_inc\"), \n    \n    # replace NAs in incidence with 0\n    across(\n      .cols = contains(\"inc\"),\n      .fns = ~replace_na(.x, 0)),\n    \n    perc_change = case_when(\n      # fix the outliers: set missing to 0 and infinity (divided by 0) to 100\n      is.na(perc_change)       ~ 0,\n      is.infinite(perc_change) ~ 100, \n      TRUE                     ~ perc_change\n    ))\n\n\n# choose colours to fill in cells  \nrow_colour &lt;- case_when(\n  # those less than zero will be green (decreasing cases)\n  zip_counts$perc_change &lt; 0 ~ \"#91CF60\", \n  # over zero red (increasing)\n  zip_counts$perc_change &gt; 0 ~ \"#FC8D59\", \n  # missing or zero orange\n  TRUE                       ~ \"#FFFFBF\")\n\n\nzip_counts %&gt;% \n  # keep the columns of interest and define order\n  select(zip, recent, recent_inc, previous, previous_inc, perc_change) %&gt;% \n  # initiate {flextable} to produce styled output table\n  flextable() %&gt;% \n  # fill in cells - choose the column and then pass our colour-scheme defined above\n  bg(j = \"perc_change\", \n     bg = row_colour\n     ) %&gt;% \n  # add in a header for labeling counts and incidence by period \n    # note the empty columns (\"\") to fit to the original table headers\n  add_header_row(\n    values = c(\"\", \n               str_c(\"Recent 14-day reporting period\\n\", recent_period_labels), \n               \"\", \n               str_c(\"Previous 14-day reporting period\\n\", previous_period_labels), \n               \"\", \n               \"Change between reporting periods\"\n               )) %&gt;% \n  # redefine column names based on original names\n    # note the different syntax to dplyr::select, here it is old_name = new_name\n  set_header_labels(\n    zip          = \"Zip Code\", \n    recent       = \"n\", \n    recent_inc   = \"Incidence\", \n    previous     = \"n\", \n    previous_inc = \"Incidence\", \n    perc_change  = \"%\"\n  ) %&gt;% \n  # combine the headers cells for the appropriate periods \n  # (i defines rows, j defines columns)\n  merge_at(i = 1, j = 2:3, part = \"header\") %&gt;% \n  merge_at(i = 1, j = 4:5, part = \"header\") %&gt;% \n  # move the header text to the centre\n  align(align = \"center\", part = \"header\") %&gt;% \n  # make header text bold \n  bold(part = \"header\") %&gt;% \n  # make the row with totals in it bold (i.e. the last row in the dataframe)\n  bold(i = nrow(zip_counts), part = \"body\") %&gt;% \n  # add in footnotes for variables (referencing the header cells)\n  footnote(j = c(3, 5), part = \"header\", ref_symbols = c(\"a\"),\n           value = as_paragraph(\"Incidence calculated as cases per 10,000 population by zip code\")) %&gt;% \n  footnote(j = 6, part = \"header\", ref_symbols = c(\"b\"),\n           value = as_paragraph(\"These reflect the percentage increase or decrease of new diagnoses \n                                between the 14 days preceding the past 7 days and the 14 days\n                                preceding that.\")) %&gt;% \n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\")"
  },
  {
    "objectID": "pages/fulton.html#step-8.-analysis-of-risk-factors-for-mortality",
    "href": "pages/fulton.html#step-8.-analysis-of-risk-factors-for-mortality",
    "title": "Fulton (EN)",
    "section": "Step 8. Analysis of risk factors for mortality",
    "text": "Step 8. Analysis of risk factors for mortality\n\nCreate a table in which you assess, with the appropriate statistical tests, whether the demographic characteristics of those dying from Covid-19 are significantly different from cases who did not die from it.\nFor each of the variables used in the table that you just created, carry out univariate regression using each demographic variable as the independent variable and the outcome (dead, not dead) as the dependent variables. Create a table with the estimates -alongside 95% CI - of the estimates.\n\n\nQuestion 8.1: According to the results of the univariate analysis, how was having a sore throat associated with mortality from Covid-19\n\n It was a risk factor for mortality It was a protective factor for mortality It was not associated with mortality Impossible to know\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# define a list of variables for looping over later\nsymptom_vars &lt;- linelist %&gt;% \n     # choose all columns that contain \"sym_\" in the name but exclude \"sym_resolved\"\n     select(c(contains(\"sym_\"), -sym_resolved)) %&gt;% \n     # pull the names out \n     names()\n\n# define variables of interest (save typing them out later) \ndescriptive_vars &lt;- c(\"gender\", \n                      \"age_group\",\n                      \"eth_race\",\n                      symptom_vars,\n                      \"hospitalized\",\n                      \"days_hosp\")\n\n# filter dataset  \nrf_data &lt;- linelist %&gt;% \n  # only keep variables of interest\n  select(died_covid, age, all_of(descriptive_vars)) %&gt;% \n  # set unknown back to NA for all factor variables\n  mutate(across(\n    .cols = where(is.factor),\n    .fns = ~fct_recode(.x, NULL = \"Unknown\"))) %&gt;% \n  # flip factor levels (so that the reference values are correct)\n  mutate(eth_race = fct_infreq(eth_race)) %&gt;% \n  mutate(gender = fct_relevel(gender, \"Female\", \"Male\")) %&gt;% \n  mutate(across(all_of(c(\"died_covid\", symptom_vars, \"hospitalized\")), \n                ~fct_relevel(.x, \"No\", \"Yes\")\n                )) %&gt;% \n  # only keep rows with complete data for all variables of interest\n  # note that this will drop rows where **ANY** of the listed variables are NA\n  drop_na(any_of(c(\"died_covid\", \"age\", descriptive_vars)))\n\n\n# define variable labels to show in output tables \nrf_data &lt;- rf_data %&gt;%\n  set_variable_labels(\n    died_covid = \"Died\",\n    age = \"Age (years)\",\n    gender = \"Gender\",\n    age_group = \"Age group (years)\",\n    eth_race = \"Ethnicity\",\n    sym_fever = \"Fever\",\n    sym_subjfever = \"Subjective fever\",\n    sym_myalgia = \"Myalgia\",\n    sym_losstastesmell = \"Loss taste/smell\",\n    sym_sorethroat = \"Sore throat\",\n    sym_cough = \"Cough\",\n    sym_headache = \"Headache\",\n    hospitalized = \"Hospitalized\",\n    days_hosp = \"Days in hospital\"\n  )\n\n\n\nrf_data %&gt;%\n  # keep variables of interest\n  select(died_covid, gender, eth_race, age, days_hosp) %&gt;%\n  # produce summary table and specify grouping variable\n  tbl_summary(\n    by = died_covid\n  ) %&gt;%\n  # specify what test to perform\n  add_p(\n    list(\n      all_continuous() ~ \"kruskal.test\",\n      eth_race ~ \"kruskal.test\",\n      all_dichotomous() ~ \"chisq.test\"\n    )\n  ) %&gt;%\n  # edit what the column headers say (using {gtsummary})\n  # nb. {n} automatically shows the number in that group and \\n is a linebreak\n  modify_header(update = list(\n    stat_1 ~ \"**Dead**\\n (N={n})\",\n    stat_2 ~ \"**Alive**\\n (N={n})\"\n  )) %&gt;%\n  # edit what it says in the footnote (using {gtsummary})\n  modify_footnote(update = list(\n    all_stat_cols() ~ \"n (%) for categorical;\\n median (IQR) for continuous\",\n    p.value ~ \"Pearson's Chi-squared test for dichotomous;\\n Kruskal-Wallis rank sum test for continuous and categorical\"\n  )) %&gt;%\n  # change to flextable format\n  as_flex_table() %&gt;%\n  # make header text bold (using {flextable})\n  bold(part = \"header\")\n\n###################### B) UNIVARIATE REGRESSION ANALYSIS ####################################\n\n\n# produce table with regression estimates\nregress_tab &lt;- rf_data %&gt;%\n  # drop variables not interested in \n  select(-age_group) %&gt;%\n  # produce univariate table\n  tbl_uvregression(\n    # define outcome variable\n    y = died_covid, \n    # define regression want to run (generalised linear model)\n    method = glm, \n    # define what type of glm want to run (logistic)\n    method.args = list(family = binomial), \n    # exponentiate to produce odds ratios (rather than log odds)\n    exponentiate = TRUE, \n    # do not show the overall counts (this is done in cross_tab below)\n    hide_n = TRUE,\n    ## uncomment this line if you want to not show reference rows\n    # show_single_row = c(symptom_vars, gender, hospitalized),\n    ## note: NULL at the end allows you to have a comma before a commented out row\n    NULL\n  )\n\n# produce table with counts by outcome (using the data fed to the regression above)\ncross_tab &lt;- regress_tab$inputs$data %&gt;%\n  tbl_summary(\n    # group by outcome \n    by = died_covid,\n    ## uncomment this line if you only want to show the \"Male\" row for gender\n    ## this would be run if you also uncommented the single_row in regression above\n    # value = list(gender ~\"Male\"),\n    ## show all levels (otherwise only shows the \"Yes\" level)\n    type = list(all_dichotomous() ~ \"categorical\"),\n    ## note: NULL at the end allows you to have a comma before a commented out row\n    NULL\n  )\n\n# combine tables \ntbl_merge(list(cross_tab, regress_tab)) %&gt;%\n  # edit what it says in the grouping headers \n  modify_spanning_header(update = list(\n    c(\"stat_1_1\",\"stat_2_1\") ~ \"Died\",\n    c(\"estimate_2\", \"ci_2\", \"p.value_2\") ~ \"Univariate regression\")\n    ) %&gt;% \n  # edit what it says in the footnote (using {gtsummary})\n  modify_footnote(update = list(\n    all_stat_cols() ~ \"n (%) for categorical;\\n median (IQR) for continuous\")\n    ) %&gt;% \n  # change to flextable format\n  as_flex_table() %&gt;%\n  # make header text bold (using {flextable})\n  bold(part = \"header\") %&gt;% \n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\")"
  },
  {
    "objectID": "pages/r_practical.html",
    "href": "pages/r_practical.html",
    "title": "R basics practical (ENG)",
    "section": "",
    "text": "Import and explore data"
  },
  {
    "objectID": "pages/r_practical.html#overview",
    "href": "pages/r_practical.html#overview",
    "title": "R basics practical (ENG)",
    "section": "Overview",
    "text": "Overview\n\n\n\nCase study characteristics\n\n\n\n\n\nName\nR practical\n\n\nLanguage\nEnglish\n\n\nTool\nR\n\n\nLocation\nEU\n\n\nScale\nInternational\n\n\nDiseases\nMpox\n\n\nKeywords\nMpox; Cleaning; Descriptives\n\n\nTechnical complexity\nBasic\n\n\nMethodological complexity\nBasic\n\n\n\nAuthorship\nOriginal authors: Xanthi Andrianou, Gianfranco Spiteri (ECDC EI Group)\nData source: Fictional data provided by ECDC EI Group for training purposes\nAdapted by: Alberto Mateo Urdiales to the case study template\n\n\nInstructions\n\nGetting Help\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHints and Solutions\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\nPosting a question in the Community Forum\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\nTerms of Use\n\n\n\n\nFeedback & suggestions\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\nVersion and revisions\nThe first version was written by Xanthi Andrianou in October 2021.\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nJune 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales"
  },
  {
    "objectID": "pages/r_practical.html#guidance",
    "href": "pages/r_practical.html#guidance",
    "title": "R basics practical (ENG)",
    "section": "Guidance",
    "text": "Guidance\n\nBackground\nThe practical is based on a scenario that requires the analysis of surveillance data coming from two sources: case-based information reported by countries and aggregated case data collected from open sources. The datasets have been created to resemble the data collected during monitoring of mpox in Europe in 2022.\n\n\nScenario and objectives\nMpox has been reported in 5 countries for the first time in 2022. We have collected aggregated case numbers from open sources, and we have also reported case-based data. The objectives of the practical are the following: 1. Explore different types of files and how they can be imported in R 2. Perform basic data cleaning, e.g., changing the variable type, recode variables, aggregate and filter 3. Perform a basic descriptive analysis using tables and graphs\n\n\nPrevious level of expertise assumed\nTo follow this case study you should be already familiar with the layout of Rstudio and on basic R concepts, such as packages, functions and arguments.\nIf you feel you need to familiarise a bit further with these aspects, read the EpiRhandbook chapter on R basics\n\n\nPreparation for the case study\n\nDownload folder named r_practical and extract contents in the local laptop\nCreate an Rstudio project in the folder r_practical. If you are unsure on how to do that, read the EpiRhandbook on R projects\nInside “r_practical”: Subfolder “data” contains the raw data you will use in this case study. You should see six different files, three called E_pox_aggregated_data and three E_pox_case_based_data. Each has a specific file type.\nInside “r_practical”: Subfolder “scripts” should be used to save any scripts related to the analysis. Inside scripts there is another subfolder called “backup” where you can find a solution R script for each step in case you are stuck at any point or if you want to compare your own script with the solution one.\nInside “r_practical”: Subfolder “outputs” can be used to store any outputs (tables, graphs, documents) that are the result of the analysis."
  },
  {
    "objectID": "pages/r_practical.html#step-1-getting-ready-for-importing-the-data",
    "href": "pages/r_practical.html#step-1-getting-ready-for-importing-the-data",
    "title": "R basics practical (ENG)",
    "section": "Step 1: Getting ready for importing the data",
    "text": "Step 1: Getting ready for importing the data\n\nStep 1.1: Create a new R script\nOnce you have created an Rproject inside the “r_practical” folder (as specified in the second point of the section Preparation for the case study). Create a new script with the name mpox_rpractical and save it in the subfolder “scripts”. If you are familiar with Rmarkdown, you may decide to use this type of file instead of a standard R script.\n\n\nStep 1.2: Define R language\nDepending on where you are and how you carried out R installation, your language “locale” might be different from the language of the graphs that you want to produce. For example, a french person might have a french “locale”. If that is the case, when creating a graph by day of the week, Monday will be displayed as “lundi”. If that french person wants to create an English report, as for this case study, the language “locale” should be changed.\nTask: Ensure your “locale” is in English and change it into English if it is not. If you don’t know how to do this try finding it online (searching for online help is an important skill for R users!). Otherwise, see the solution below\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\n\nStep 1.3: Install/load packages\nAs you probably know, the first part of our script (besides including -commented- some information about the aim, author, date last updated and contact details) is to install and load packages. Fortunatelly there is a package that does this task very effectively: {pacman}. The function p_load() from this package will install any packages listed not already installed and will load them. If a listed package had already been installed, it will just load it. You can find more about installing/loading packages in the Packages section of the EpiRhandbook.\nUsing this approach, try to install and load the following packages: rio, janitor, lubridate, skimr, epikit, gtsummary, apyramid and tidyverse.\n\n\n Click to read a hint\n\n\nYou may end up using a long list of packages. Unfortunately different packages have functions with the same name. For example, the package {dplyr} (already installed with {tidyverse}) has a function called select() which we frequently use to subset columns of a data frame. But other packages such as {MASS} do also have a function called select(). This could create headaches if you want to subset columns using dplyr’s select() but R thinks you’re calling MASS’s select() (we call this masking - dplyr’s select() is masked by MASS’s select()). Given that you are more likely to use functions from {tidyverse}, ensure that this is the last package in your p_load() list so that functions from {tidyverse} (including {dplyr} functions) will always “prevail”.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # to create age categories\n  gtsummary,  # summary statistics, tests and regressions \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n)"
  },
  {
    "objectID": "pages/r_practical.html#step-2-import-and-explore-the-data",
    "href": "pages/r_practical.html#step-2-import-and-explore-the-data",
    "title": "R basics practical (ENG)",
    "section": "Step 2: Import and explore the data",
    "text": "Step 2: Import and explore the data\n\nStep 2.1: Import the different data frames\nThere are several ways in which you can import the different data frames. Inside “data/raw” you have three different types of files: csv, json and excel. One way would be to use a specific R function to import each file. For example, read.csv() from {base} can be used to import .csv files, fromJSON() function from {jsonlite} to import .json files and read_excel() from {readxl} to import .xlsx files. Fortunately, there is a more efficient approach which is to use the import() function from {rio} to open any file. This function will recognise the type of file and choose the appropriate function to import it. If you feel you need to know more about importing functions, read the Import and export chapter of the EpiRhandbook.\nTask: Import the three case-based data frames and the three aggregated data frames using import() from {rio}.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Importing ------------------------------------------------------\n# Case-based data\ncb_data_raw_csv &lt;- import(\"data/E_pox_case_based_data.csv\")\ncb_data_raw_json &lt;- import(\"data/E_pox_case_based_data.json\")\ncb_data_raw_xlsx &lt;- import(\"data/E_pox_case_based_data.xlsx\")\n\n# Aggregated data\nagg_data_raw_csv &lt;- import(\"data/E_pox_aggregated_data.csv\")\nagg_data_raw_json &lt;- import(\"data/E_pox_aggregated_data.json\")\nagg_data_raw_xlsx &lt;- import(\"data/E_pox_aggregated_data.xlsx\")\n\n\n\n\n\nStep 2.2: Explore the different data frames\nTake a look at the different data frames and try to find out:\n\nThe number of columns and observations\nThe class of their columns and whether it matches its nature (e.g., are “dates” considered “dates” by R?)\nLook at the different categories of the columns about gender, clinical symptoms, outcome, hiv status and sexual orientation existing in the case-based data. Do you need to recode any of them?\nHow is unknown or missing data being categorised in these columns? Should you standardise this category?\nIf case-based and aggregated data from file types (.csv, .json and .xlsx) are exactly the same, remove the .json and .xlsx data frames from your environment.\n\n\n\n Click to read a hint\n\n\nAn efficient way to explore data is to use the function skim() from the {skimr} package, as it gives you all the information needed with only one command. Of course, there are several alternatives. To know the different categories in a column, you can use the function tabyl() from {janitor}, which will give you counts and percentages of every category in the data column.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Explore the different case-based data frames\n\nskim(cb_data_raw_csv)\nskim(cb_data_raw_json)\nskim(cb_data_raw_xlsx)\n\n# Explore the different categories of gender and clinical columns in one of the cb data frames\ntabyl(cb_data_raw_csv, Gender)\n\ntabyl(cb_data_raw_csv, ClinicalSymptoms)\n\ntabyl(cb_data_raw_csv, Outcome)\n\ntabyl(cb_data_raw_csv, HIVStatus)\n\ntabyl(cb_data_raw_csv, SexualOrientation)\n\n# Explore the different aggregated data frames\n\nskim(agg_data_raw_csv)\nskim(agg_data_raw_json)\nskim(agg_data_raw_xlsx)\n\n# Remove json and xlsx files as they are exactly the same as the csv ones. Within rm() we ask for the objects containing the pattern \"json\" or \"xlsx\" to be removed from the environment\nrm(list = ls(pattern = \"json|xlsx\"))\n\n\n\n\nHow many columns does the aggregated data frame have?\n\n 2000 13 3 101\n\n\n\nWhat is the class of the column DateOfNotification in the case-based data?\n\n Date Character Numeric Factor\n\n\n\nFor how many cases is the HIV status Unknown or missing?\n\n 1168 722 900 446"
  },
  {
    "objectID": "pages/r_practical.html#step-3-cleaning-the-data",
    "href": "pages/r_practical.html#step-3-cleaning-the-data",
    "title": "R basics practical (ENG)",
    "section": "Step 3: Cleaning the data",
    "text": "Step 3: Cleaning the data\n\nStep 3.1: Clean the case-based data\nWhen exploring the case-based data, you may have noticed that there are a few things that we need to take care of before we can start doing further analysis. Firstly, names contain a mixture of upper and lower case letters. Whilst this isn’t in itself a problem, R is case-sensitive, so having all names in lower case may make our life easier. Also, date columns are not consider “Dates” by R, but instead they are being consider as “Character”, which means they are being considered as nominal data. This would give us problems when plotting by Dates. Another issue is that some columns have categories that may not be intuitive for all. For example, Gender is categorised with “F”, “M”, “O” and “UNK”. The column Outcome as “A” and “UNK”. We should give them more appropriate categories. Finally, it is important that missing data is considered as “missing” in R. That means that R treats it as “NA”. In the column clinical symptoms, for example, missing data is an empty cell, not “NA”. R is considering this as another nominal category instead of missing, and will consider it this way in any analysis or output you produce.\nTasks:\n\nCreate a clean version of your case-based data making all cleaning changes in a single piping command\nChange all column names to lower case.\nConvert all date columns to class “Date”.\nUse the column “DateOfNotification” to create a column called “week_date” which has the week of notification, starting on Mondays.\nTransform all empty cells into “NA”\nRecode “Gender” categories into : “Female”, “Male”, “Other” and “Unknown”\nRecode “Outcome” categories into: “Alive” and “Unknown”\nRecode HIV status into: “Positive”, “Negative” and “Unknown/Missing”\nRecode Sexual orientation into: “Bisexual”, “Heterosexual”, “MSM/homo or bisexual male” and “Unknown/missing”.\nCreate a column called “age_group” with ten year age groups and the oldest group being 70+\nCheck that all changes have been made correctly\n\n\n\n Click to read a hint\n\n\nTo convert all names to lower case, rather than renaming each column you may use the function clean_names() from the {janitor} package, which will do it automatically for all columns. Use lubridate functions to transform date columns into “Date” class, you can do this one by one, or you could do all at the same time using the across() function from {dplyr}. If you feel you need to know more about transforming dates read the chapter Working with Dates from the EpiRhandbook.If you are not sure how to use the across() function, you can also read the section on Transform multiple columns.\nOne simple way to create the “week_date” column would be to use the function floor_date() from {lubridate}. Take a look at the documentation to understand how it works and how to make Monday the starting day of the week.\nThere are different functions that we can use to recode. We propose three: The function recode() from {dplyr}, the function ifelse() from {base} and the function case_when() from {dplyr}. If you want to know more about these functions, look that the section on Re-code values from the EpiRhandbook.\nTo create the age groups, explore the function called age_categories() from the {epikit} package.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Create a new object called cb_data which is the clean version of the raw data, applying the cleaning functions\n\n\ncb_data &lt;- cb_data_raw_csv %&gt;% \n  \n  clean_names() %&gt;% # standardises names and puts all into lower case \n  \n  #(Note: after this point all column names have changed)\n  \n  mutate(date_of_notification = ymd(date_of_notification)) %&gt;%  #transform ONE column into date\n\n  mutate(across(starts_with(\"date\"), \n                .fns = ~ ymd(.x))) %&gt;%  #transforms ALL columns starting with \"date\" into dates\n  \n  mutate(week_date = floor_date(date_of_notification, # create week column with Monday start\n                              unit = \"week\",\n                              week_start = \"Monday\")) %&gt;% \n  \n  mutate(across(where(is.character), \n                .fns = ~ ifelse(.x == \"\", NA, .x)))  %&gt;% #transforms empty cells into NA across all character columns\n  \n  mutate(gender = recode(gender,\n                         \"F\" = \"Female\",\n                         \"M\" = \"Male\",\n                         \"O\" = \"Other\",\n                         \"UNK\" = \"Unknown\")) %&gt;%\n  \n    \n  mutate(across(where(is.character), \n                .fns = ~ ifelse(.x == \"UNK\", \"Unknown\", .x)))  %&gt;% #transforms UNK to Unknown across all character columns\n  \n  mutate(outcome = ifelse(outcome == \"A\", \"Alive\", outcome)) %&gt;%   #we can recode as well with ifelse if we want to change only one or two categories\n  \n  mutate(hiv_status = case_when(hiv_status == \"NEG\" ~ \"Negative\",    #for more complex recoding better case_when\n                                hiv_status == \"POS\" ~ \"Positive\",\n                                TRUE                ~ \"Unknown/missing\")) %&gt;% \n  \n  mutate(sexual_orientation = case_when(sexual_orientation == \"BISEXUAL\" ~ \"Bisexual\",\n                                        sexual_orientation == \"HETERO\" ~ \"Heterosexual\",\n                                        sexual_orientation == \"MSM\" ~ \"MSM/homo or bisexual male\",\n                                        TRUE                        ~  \"Unknown/missing\")) %&gt;% \n  \n  mutate(age_group = age_categories(age, \n                                    lower = 0,      #set up the lower age\n                                    upper = 70,     #set up the upper age\n                                    by = 10))       #set up the age breaks\n\n\n\n\n# Check that all changes have been made correctly\n\nskim(cb_data)\n\ntabyl(cb_data, gender)\n\ntabyl(cb_data, clinical_symptoms)\n\ntabyl(cb_data, outcome)\n\ntabyl(cb_data, hiv_status)\n\ntabyl(cb_data, sexual_orientation)\n\ntabyl(cb_data, week_date)\n\ntabyl(cb_data, age_group)\n\n\n\n\nHow many male cases we have in the data frame?\n\n 36 1960 65 1523\n\n\n\nWhich week has the largest number of cases?\n\n 2022-04-11 2022-07-25 2022-02-28 2022-05-09\n\n\n\nHow many cases with missing age are present?\n\n 1 3 None 396\n\n\n\n\nStep 3.2: Clean the aggregated data\nIn a similar way, clean the aggregated data by:\n\nStandardising names to lower case\nEnsure that date of reporting is of class “Date”\nCreate a column called “week_date” with the week of reporting starting on Monday\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Check class of date of reporting column\n\nclass(agg_data_raw_csv$DateRep) #It is a date, so we do not need to change its class\n\n# Create a new object called agg_data which is the clean version of the raw data, applying the cleaning functions\n\nagg_data &lt;- agg_data_raw_csv %&gt;% \n  \n  clean_names() %&gt;% # standardises names and puts all into lower case \n  \n  #(Note: after this point all column names have changed)\n  \n  mutate(week_date = floor_date(date_rep, # create week column with Monday start\n                              unit = \"week\",\n                              week_start = \"Monday\"))"
  },
  {
    "objectID": "pages/r_practical.html#step-4-basic-descriptives",
    "href": "pages/r_practical.html#step-4-basic-descriptives",
    "title": "R basics practical (ENG)",
    "section": "Step 4: Basic descriptives",
    "text": "Step 4: Basic descriptives\n\nStep 4.1: Table my place (country)\nTask: Using the case-based data, create a table with the number of cases by country\n\n\n Click to read a hint\n\n\nAn easy way to produce tables is using the function tbl_summary() from {gtsummary} package\n\n\n\nWhat’s the country with the largest percentage of cases?\n\n C D B A\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Create an object with the table\ncb_country_table &lt;- cb_data %&gt;%\n\n  select(country) %&gt;% #select the column that we want to use in the table\n  \n  gtsummary::tbl_summary() # create the table\n\n# Ask R to print the table\ncb_country_table\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    country\n\n        CountryA\n816 (41%)\n        CountryB\n391 (20%)\n        CountryC\n474 (24%)\n        CountryD\n217 (11%)\n        CountryE\n102 (5.1%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\nStep 4.2: Epicurve by week of notification (overall)\nTask: Using the case-based data, create an epicurve by week of notification\n\n\n Click to read a hint\n\n\nTo do the epicurve, you can use ggplot() and geom_histogram(), which will automatically aggregate your data. If you are unsure on how ggplot() works, read the EpiRhandbook chapter on Epidemic curves\nAn alternative approach is to first aggregate the number of cases by week of notification. You can do this using the functions group_by() and summarise() from {dplyr}. If you are unsure on how to do this, review the Grouping data chapter of the EpiRhandbook.\nOnce you have an object with aggregated cases by week of notification, create the epicurve using ggplot() \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nepicurve_epox &lt;- ggplot(data = cb_data,          #data to be used\n                        aes(x = week_date)) +    #with geom_histogram() you only need to assign the x axis\n  \n  geom_histogram(binwidth = 7,                   #binwidth 7 ensures that the width represents 7 days\n                 fill=\"darkgreen\",               #colour inside the bins\n                 color=\"white\",                  #outline colour of the bins\n                 alpha=0.8) +                    #transparency of the bins\n  \n  scale_x_date(breaks = \"2 weeks\") +             #set the x axis labels to two week intervals\n\n  \n  labs(title=\"Mpox cases reported in 2022\") +  #add a title\n  \n  theme_bw() +                                  #assign a predefined theme\n  \n  theme(axis.text = element_text(size=9),       #define the font size of the axis text\n        axis.title = element_blank(),           #remove the titles of the x and y axis \n        axis.text.x = element_text(angle=90))   #rotate the x axis text\n           \n  \nepicurve_epox\n\n\n\n\n\n\n\n\nStep 4.3: Epicurve by week of notification (by country)\nTask: Using the case-based data, create an epicurve by week of notification in which the colour of the bins represents the number of cases by country\n\n\n Click to read a hint\n\n\nThe code will be very similar to the previous one, but remember that if want a dynamic colour inside the bins, you need to assign the fill to the column you want to use (country) and place it inside the aesthetics \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nepicurve_epox_country &lt;- ggplot(data = cb_data,  #data to be used\n                        aes(x = week_date,       \n                            fill = country)) +   #now the fill needs to be inside aes()  \n  \n  geom_histogram(binwidth = 7,                   #binwidth 7 ensures that the width represents 7 days\n                 color=\"white\",                  #outline colour of the bins\n                 alpha=0.8) +                    #transparency of the bins\n  \n  scale_fill_viridis_d() +                       #we change the predefined colours\n\n  scale_x_date(breaks = \"2 weeks\") +             #set the x axis labels to two week intervals\n\n  \n  labs(title=\"Mpox cases reported by country in 2022\") +  #add a title\n  \n  theme_bw() +                                  #assign a predefined theme\n  \n  theme(legend.position = \"bottom\",             #legend position to the bottom\n        axis.text = element_text(size=9),       #define the font size of the axis text\n        axis.title = element_blank(),           #remove the titles of the x and y axis \n        axis.text.x = element_text(angle=90),   #rotate the x axis text\n        legend.title = element_blank())         #remove title of legend\n           \n  \nepicurve_epox_country\n\n\n\n\n\n\n\n\nStep 4.4: Demographic characteristics\nNow that we have created some outputs by time and place, we should focus on the “person” element. The two most important demographic characteristics are usually age and gender. In the case we are seeing, we may also want to explore the sexual orientation of cases.\nTask:\n\nExplore the number of cases by age group and gender.\nCreate a table with number and percentages of cases by sexual orientation\n\n\n\n Click to read a hint\n\n\nThe easiest way to explore both columns (age_group and gender) would be to use the tabyl() function from {janitor}. Then, to create the age pyramid explore the function age_pyramid() from the {apyramid} package. You can find more about this function in the EpiRhandbook chapter Demographic pyramids and Likert-scales To create the table by sexual orientation, consider using the function tbl_summary() from {gtsummary}\n\n\n\nWhich demographic group is more affected by Mpox?\n\n Females 60-69 Males 40-49 Females 10-19 Males 30-39\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Explore gender and age group columns\ntabyl(cb_data, gender)\n\n  gender    n percent\n  Female   36  0.0180\n    Male 1960  0.9800\n   Other    1  0.0005\n Unknown    3  0.0015\n\ntabyl(cb_data, age_group)\n\n age_group   n percent valid_percent\n       0-9   1  0.0005  0.0005007511\n     10-19  33  0.0165  0.0165247872\n     20-29 396  0.1980  0.1982974462\n     30-39 766  0.3830  0.3835753630\n     40-49 524  0.2620  0.2623935904\n     50-59 204  0.1020  0.1021532298\n     60-69  64  0.0320  0.0320480721\n       70+   9  0.0045  0.0045067601\n      &lt;NA&gt;   3  0.0015            NA\n\n# Table with sexual orientation \n\ntab_sor &lt;- cb_data %&gt;% \n  \n  select(sexual_orientation) %&gt;% \n  \n  tbl_summary(label = list(sexual_orientation ~ \"Sexual Orientation\")) \n\ntab_sor\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    Sexual Orientation\n\n        Bisexual\n7 (0.4%)\n        Heterosexual\n46 (2.3%)\n        MSM/homo or bisexual male\n833 (42%)\n        Unknown/missing\n1,114 (56%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\nStep 4.5: Clinical characteristics\nNow, let’s summarise the main clinical information that we have in our case-based data frame.\nTasks:\n\nCreate a bar plot with the proportion of each type clinical symptoms\nCreate a table with the number and percentage of cases by outcome\nCreate a table with sexual orientation by HIV status. You may add a statistical test to ascertain if there are significant differences between groups.\n\n\n\n Click to read a hint\n\n\nTo create bar plots we can use geom_bar() or geom_col() depending on the nature of our data. If we aggregate first, we can use geom_col(), otherwise we should use geom_bar(). There is a function of the {gtsummary} package called add_p() which enables you to easy calculate a statistical test across groups. If you want to know more read the section on gtsummary package of the EpiRhandbook.\n\n\n\nAre there significant differences in HIV status across cases according to their sexual orientation ?\n\n Yes No\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Bar plot with clinical symptoms\n\nbar_clinical &lt;- cb_data %&gt;% \n  \n  drop_na(clinical_symptoms) %&gt;%   # we remove those with missing clinical symptoms\n  \n  group_by(clinical_symptoms) %&gt;% \n  \n  summarise(n_cases = n(), .groups = \"drop\") %&gt;%\n  \n  mutate(prop=(n_cases/sum(n_cases))*100) %&gt;%  # we create a column with proportions\n  \n  ggplot(aes(y = reorder(clinical_symptoms, prop), x = prop)) +  # the reorder function ensures that categories are ordered by proportion in the graph\n  \n  geom_col(fill = \"darkgreen\") + \n  \n  labs(\n    title= \"Frequency of clinical symptoms in Mpox cases\",\n    y = \"\",\n    x = \"Number of cases\"\n  ) +\n  \n  theme_bw() +\n  \n  theme(axis.text = element_text(size=9))       #define the font size of the axis\n\nbar_clinical  \n\n\n\n# Table with number and percentage of cases by outcome\n\ntab_outcome &lt;- cb_data %&gt;% \n  \n  select(outcome) %&gt;% \n  \n  tbl_summary(label = list(outcome = \"Reported outcome\")) # with the argument \"label\" we can change how the column name is displayed\n\ntab_outcome\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    Reported outcome\n\n        Alive\n1,405 (70%)\n        Unknown\n595 (30%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n# Table with sexual orientation by HIV outcome\n\ntab_hiv_sor &lt;- cb_data %&gt;% \n  \n  select(hiv_status, sexual_orientation) %&gt;% \n  \n  filter(hiv_status != \"Unknown/missing\") %&gt;% # we remove the Unknown\n  \n  tbl_summary(by = hiv_status, label = list(sexual_orientation ~ \"Sexual Orientation\")) %&gt;% \n  \n  add_p()                                     # this function will estimate a p value with the appropriate statistical test based on the class of the columns and the number of observations\n\ntab_hiv_sor\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      Negative, N = 5251\n      Positive, N = 3071\n      p-value2\n    \n  \n  \n    Sexual Orientation\n\n\n0.4\n        Bisexual\n3 (0.6%)\n1 (0.3%)\n\n        Heterosexual\n14 (2.7%)\n3 (1.0%)\n\n        MSM/homo or bisexual male\n219 (42%)\n131 (43%)\n\n        Unknown/missing\n289 (55%)\n172 (56%)\n\n  \n  \n  \n    \n      1 n (%)\n    \n    \n      2 Fisher’s exact test"
  },
  {
    "objectID": "pages/r_practical.html#step-5-optional-analysis",
    "href": "pages/r_practical.html#step-5-optional-analysis",
    "title": "R basics practical (ENG)",
    "section": "Step 5: Optional analysis",
    "text": "Step 5: Optional analysis\nIf you have time, let’s now undertake some further analysis. First we will look at data quality. We will check two things:\n\nIn the case-based data, what’s the delay between symptom onset, diagnosis and notification?\nAre numbers from case-based data consistent with the aggregated data?\n\nThen, we will test your skills in data visualisation creating a heat plot.\n\nStep 5.1: Delay between date of onset, diagnosis and notification\nTasks\n\nCalculate median time from symptom onset to diagnosis and from diagnosis to notification, both overall and by country\nAssess visually the number of cases by calendar period and type of date (onset, diagnosis and notification)\n\n\n\n Click to read a hint\n\n\nTo plot together the different dates you may need to transform your data from “wide” to “long” form. What we call “pivoting” in R. The objective is to have a column with the different date categories (onset, diagnosis and notification) and another column with their date value. If you are unsure on how to do this, have a look at the PivoTing data chapter of the EpiRhandbook. Then, try to plot with the daily values, but if that’s not easy to interpret you may want to aggregate cases by week.\n\n\n\nIs there a difference in the delay from diagnosis to notification by country?\n\n Yes No\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Estimate delay between onset and diagnosis, and between diagnosis and notification\n\ndelay_db &lt;- cb_data %&gt;% \n  \n  mutate(delay_diag = as.numeric(date_of_diagnosis - date_of_onset)) %&gt;%   #we create variables with difference between dates, we transform them in numeric to be able to then calculate measures of central tendency\n  \n  mutate(delay_not = as.numeric(date_of_notification - date_of_diagnosis))\n\nsummary(delay_db$delay_diag) #the summary will give us measures of central tendency and dispersion\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -2.000   4.000   7.000   7.758  10.000  66.000     897 \n\nsummary(delay_db$delay_not)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-46.0000  -2.0000   0.0000  -0.6078   1.0000  23.0000      715 \n\ndelay_country &lt;- delay_db %&gt;% #here, we group by country and summarise the median to compare across countries\n  \n  group_by(country) %&gt;% \n  \n  summarise(median_delay_diag = median(delay_diag, na.rm = T),\n            median_delay_not = median(delay_not, na.rm = T))\n\ndelay_country\n\n# A tibble: 5 x 3\n  country  median_delay_diag median_delay_not\n  &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 CountryA                 7                0\n2 CountryB                 7                0\n3 CountryC                 6                0\n4 CountryD                 7                0\n5 CountryE                 6                0\n\n# Line graph with the different dates \n\ndates_longer &lt;- cb_data %&gt;% # use the variables of the dates and make a longer dataset. In the pivot_longer() command we select the columns which we want to expand in long format and transform the dataset\n   \n  pivot_longer(\n    \n    cols=starts_with(\"date_\"),         # all columns starting with \"date_\" will be taken \n\n    names_to = \"indicator\",            #the names of the columns will be placed in a single column called \"indicator\"\n\n    values_to = \"date\")                # the values (which are dates in this case) will be placed in a column called \"date\"\n  \n\ndates_longer_week &lt;- dates_longer  %&gt;% \n\n  mutate(week_date = floor_date(date, unit = \"week\", week_start = \"Monday\")) %&gt;%  # we create a week column\n    \n  group_by(indicator, week_date) %&gt;% \n    \n  summarise(n=n(), .groups=\"drop\") %&gt;%   # we group and summarise to have the number of cases by date type and week\n    \n  drop_na(week_date)                     # we drop the cases with no data on dates\n\n\n\n\nplot_date_delay &lt;-   ggplot(data = dates_longer_week,\n                            aes(x = week_date, \n                                y = n, \n                                color=indicator)) +\n  \n  geom_line(linewidth = 1.5) +\n  \n  scale_x_date(breaks = \"2 weeks\")+\n  \n  theme_bw() +\n  \n  theme(legend.position = \"bottom\", \n        axis.text = element_text(size=9),\n        axis.title = element_blank(),\n        axis.text.x = element_text(angle=90),\n        legend.title = element_blank()) +\n  labs(title=\"Mpox cases reported in 2022, by date of onset, diagnosis and notification.\")\n\nplot_date_delay\n\n\n\n\n\n\n\n\nStep 5.2: Compare case-based and aggregated data\nTask: Create a plot comparing the number of cases reported to through the case-based flow and through the aggregated flow in each country.\nNOTE: Take into consideration that the column on cases in the aggregated data frame reports the cumulative number of cases.\n\nWhich country is not reporting aggregated data?\n\n A B C D E\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Create a data frame with the overall number of cases reported through the aggregated flux\n\nagg_data_country &lt;- agg_data %&gt;% \n  \n  group_by(country) %&gt;% \n  \n  filter(date_rep == max(date_rep)) %&gt;% # as we have cumulative data, we keep only the last week (after grouping by country)\n  \n  select(-date_rep, -week_date) %&gt;%     # remove unnecessary columns\n\n  mutate(source = \"aggregated\")         # we create this column to distinguish the numbers from the case-based flux\n\n\n# Create a data frame with the overall number of cases reported through the case-based flux\n\ncb_data_country &lt;- cb_data %&gt;%\n  \n  group_by(country) %&gt;% \n  \n  summarise(cases = n(), .groups = \"drop\") %&gt;% \n  \n  mutate(source = \"case_based\")       # we create this column to distinguish the numbers from the\n  \n\n# We append both data frames. Remember this is different from merging\n\ntotal_data &lt;- bind_rows(cb_data_country, agg_data_country)\n\n\n# We create a graph to compare the cases reported in both sources\n\ngraph_comp &lt;- ggplot(data = total_data,\n                     aes(x = source, \n                         y = cases, \n                         fill = source)) +\n  \n  geom_col(position = \"dodge\") +            #position dodge puts bars one next to each other, instead of \"stacked\"\n  \n  facet_wrap(~ country, scales = \"free_y\") +  # this command gives us one graph per country. The argument scales is used to allow each y axis scales to adjust to the data\n\n  scale_fill_viridis_d(\n    labels = c(\"Aggregated\", \"Case-based\")  # this function changes the colours, but with the argument \"labels\" we can change the text of each fill.\n     ) +\n  \n  \n  labs(\n    title = \"Number of cases of Mpox reported in 2022 according to source of data\",\n    fill = \"Source\",\n    x = \"\",\n    y = \"Total number of cases\"\n  ) + \n  \n  theme_bw() +\n  \n  theme(axis.text.x = element_blank(),      # we remove the text of the x axis because it is already present in the legend\n        axis.ticks.x = element_blank())     # we also remove the ticks for aesthetic purposes\n\ngraph_comp\n\n\n\n\n\n\n\n\nStep 5.3: Heat plot with number of cases by country and week of notification\nHeat plots are a type of plots that are gaining popularity in epidemiology. In our case, they can be useful to understand how the epidemic evolved in different countries.\nTask: Using the case-based data, create a heat plot with the number of cases by country and week of notification.\n\n\n Click to read a hint\n\n\nIn this case you will need to aggregate your data by country and week of notification. You can do this using the functions group_by() and summarise() from {dplyr}. If you are unsure on how to do this, review the Grouping data chapter of the EpiRhandbook. Then, use the geom geom_tile() to create a heat plot. If you’re unsure on how to do this, read the EpiRhanbook section on [Heat Plots]https://epirhandbook.com/en/new_pages/heatmaps.html)\n\n\n\nWhich country had the latest date of notification?\n\n A D E B\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nhp_epox &lt;- cb_data %&gt;% #we first group the data by country and week of notification\n  \n  group_by(country, week_date) %&gt;% \n  \n  summarise(n_cases = n(), .groups = \"drop\") %&gt;% \n\n  #now we can use the pipe to directly plot the resulting data from the grouping\n  \n  ggplot(aes(x = week_date,\n           y = country,           #we want the countries to be in the y axis\n           fill = n_cases)) +     #the colour of the tiles should depend on the number of cases\n  \n  geom_tile(colour = \"black\") +   #this is the outline colour of each tile\n  \n  scale_fill_gradient(            #here we define the colours we want to use in the gradient\n    low = \"lightgreen\",\n    high = \"red\") +\n  \n  scale_x_date(breaks = \"2 weeks\") +             #set the x axis labels to two week intervals\n  \n  labs(\n    title= \"Mpox cases by country and week of notification\",\n    fill = \"Number of cases\"                               \n  ) +\n  \n  theme_bw() +\n  \n  theme(legend.position = \"bottom\",             #legend position to the bottom\n        axis.text = element_text(size=9),       #define the font size of the axis\n        axis.title = element_blank(),           #remove the titles of the x and y \n        axis.text.x = element_text(angle=90))   #rotate the x axis text\n    \nhp_epox"
  },
  {
    "objectID": "pages/tbe.html",
    "href": "pages/tbe.html",
    "title": "TBE - Linear regression (ENG)",
    "section": "",
    "text": "Introduction to the case study\nTick-borne encephalitis (TBE) is a vaccine-preventable, tick-borne viral infection. The typical disease course is biphasic, consisting of a first phase with general symptoms such as headache and fever, a symptom-free interval, and a second phase with neurological manifestations. Severity ranges from mild symptoms over meningitis to severe meningoencephalitis or -myelitis. Overall, 70–95% of infections remain sub-clinical and &lt;1% die. Age, monophasic course, central nervous system (CNS) comorbidities, and general comorbidities were found to be independently associated with severe TBE in some analysis. Results were inconsistent regarding diabetes, male sex and other factors. Further possible predictors include blood type, ticks’ blood meal duration (proxy: large tick at removal), immunosuppression, autoimmune diseases, and chronic inflammatory comorbidities. In Germany, laboratory diagnosis of acute TBE became statutorily notifiable in 2001. From 2018-2020, intensified surveillance was conducted. Primary aims were to assess clinical manifestations, health care utilisation, informal care, treatment practices, quality of life, and sick leave, as well as to identify factors associated with TBE severity\nNote: This introduction has been adapted from Nygren et al. Tick-borne encephalitis: acute clinical manifestations and severity in 581 cases from Germany, 2018-2020. Journal of Infection. 2023 Apr 1;86(4):369-75\nYou are one of the epidemiologists in charge of the intensified surveillance system. Your boss asks you to test a hypothesis that high blood pressure may also be causal for severe TBE. As proxy for severe TBE, you will use the length of hospitalisation (number of days spent in hospital)."
  },
  {
    "objectID": "pages/tbe.html#overview",
    "href": "pages/tbe.html#overview",
    "title": "TBE - Linear regression (ENG)",
    "section": "Overview",
    "text": "Overview\n\n\n\nCase study characteristics\n\n\n\n\n\nName:\nTBE - linear regression\n\n\nLanguage:\nEnglish\n\n\nTool:\nR; DAGitty\n\n\nLocation:\nGermany\n\n\nScale:\nNational\n\n\nDiseases:\nTBE\n\n\nKeywords:\nTBE; Linear Regression; R\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nIntermediate\n\n\n\nAuthorship\nOriginal authors: Teresa Nygren (RKI), Alicia Barrasa Blanco (UK FETP), Jan Walter (RKI) and Achim Dörre (RKI)\nData source: Data is fictional and was inspired by Nygren et al. Tick-borne encephalitis: acute clinical manifestations and severity in 581 cases from Germany, 2018-2020. Journal of Infection. 2023 Apr 1;86(4):369-75\nAdapted by: Liese Van Gompel (MediPIET), Joana Gomes Dias (ECDC), Chiara Entradi (ECDC) and Alberto Mateo Urdiales (ISS)"
  },
  {
    "objectID": "pages/tbe.html#instructions",
    "href": "pages/tbe.html#instructions",
    "title": "TBE - Linear regression (ENG)",
    "section": "Instructions",
    "text": "Instructions\n\nGetting Help\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHints and Solutions\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\nPosting a question in the Community Forum\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\nTerms of Use\nDisclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/\n\n\n\nFeedback & suggestions\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\nVersion and revisions\nWrite date of first version\nWrite any revisions made to the case study\n\n\n\n\n\n\n\n\nDate\nChanges made\nAuthor\n\n\n\n\n2023\nRevision R code\nLiese Van Gompel (MediPIET)\n\n\n2024\nRevision R code\nJoana Gomes Dias and Chiara Entradi (ECDC)\n\n\n2024\nRevision of content, structure, R code and adaptation of format to Applied Epi’s template of case studies\nAlberto Mateo Urdiales (ISS)"
  },
  {
    "objectID": "pages/tbe.html#guidance",
    "href": "pages/tbe.html#guidance",
    "title": "TBE - Linear regression (ENG)",
    "section": "Guidance",
    "text": "Guidance\n\nObjectives of this case study\nAt the end of the case study, participants should be able to:\n\nUse directed acyclic graphs (DAG) to identify variables suitable to control for confounding;\nTo perform linear regression in R;\nTo write down the associated models and interpret them.\n\n\n\nPrevious level of expertise assumed\nParticipants are expected to be familiar with directed acyclic graphs (DAGs) and the use of DAGitty (a browser-based environment for creating DAGS) for the first part; and with data management as well as descriptive and stratified analysis in R for the second part.\n\n\nPreparation for the case study\nInclude the steps needed to start replicating the analysis of the case study\nFor example:\n\nDownload folder named tbe_en and extract contents in the local laptop\nCreate an Rstudio project in the folder tbe_en. If you are unsure on how to do that, read the EpiRhandbook Chapter on R projects\nInside the folder “tbe_en”: Subfolder “data” contains a raw data file named tbe.RDS. This is the only data file you will use in this case study.\nSubfolder scripts should be used to save any scripts related to the analysis. Inside “backup” you will find a solution script with the code of the case study named tbe_lr_backup.R. You will also find an image which corresponds to the proposed DAG solution.\nSubfolder “outputs” could be used to store all outputs (tables, graphs, documents) that are the result of the analysis\nYou will also find inside “tbe_en” a word document called starter_guide_DAGitty.docx in case you need help using this website"
  },
  {
    "objectID": "pages/tbe.html#goal-1-draw-a-directed-acyclic-graph-dag",
    "href": "pages/tbe.html#goal-1-draw-a-directed-acyclic-graph-dag",
    "title": "TBE - Linear regression (ENG)",
    "section": "Goal 1: Draw a Directed Acyclic Graph (DAG)",
    "text": "Goal 1: Draw a Directed Acyclic Graph (DAG)\nSince you are interested in a causal question, please draw a DAG. If you want to use a computer, you may try http://www.dagitty.net/.\n\nWhich variables would you need to adjust for?\n\nIf you are new to DAGitty you can find a few helpful information in the document called starter_guide_DAGitty.docx present in the “tbe_en” folder you have downloaded in your laptop (See Preparation for the case study).\nOnce you have drawn your DAG, click on the solution below to see the DAG we propose.\n\n\nClick to see the solution\n\n\nWhen planning the study, the epidemiologist considered this DAG:\n\n\n\n\nAccording to this DAG, if you want to explore the association between hypertension and severe TBE, you should adjust for:\n\nTBE diagnosis\nTBE vaccination\nage\nlarge tick (=large viral load)\nmonophasic course\nother comorbidities\nsex.\n\nTBE Diagnosis is controlled by design (only cases are included).\nProbably your DAG will look differently. This is absolutely fine, since there is not only one possible DAG. But you should be able to justify your DAG based on the existing evidence."
  },
  {
    "objectID": "pages/tbe.html#goal-2-perform-linear-regression-in-r",
    "href": "pages/tbe.html#goal-2-perform-linear-regression-in-r",
    "title": "TBE - Linear regression (ENG)",
    "section": "Goal 2: Perform linear regression in R",
    "text": "Goal 2: Perform linear regression in R\nNow we will work on the data frame provided which includes data for 523 patients who have been hospitalized with TBE in the years 2018 to 2020 in Germany and for whom data were collected.\nThe following variables are provided:\nTable 1: Data dictionary for the dataframe “tbe.RDS”:\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nage\nage in years\ncontinuous\n\n\nhyper\nhypertension\n1= yes, 0=no\n\n\nvac\nvaccinated against TBE\n1= yes, 0=no\n\n\nmono\nmonophasic disease course\n1= yes, 0=no\n\n\nother\nother comorbidities\n1= yes, 0=no\n\n\ntick\nlarge tick at removal\n1= yes, 0=no\n\n\nsex\nsex\n1= female, 0= male\n\n\nhospd\nlength of hospitalisation in days\ncontinuous\n\n\n\n\nStep 1: Set up\n\nStep 1.1: Create a new R script\nOnce you have created an Rproject inside the “tbe_en” folder (as specified in the second point of the section Preparation for the case study). Create a new script with the name tbe_lr and save it in the subfolder “scripts”. If you are familiar with Rmarkdown, you may decide to use this type of file instead of a standard R script.\n\n\nStep 1.2: Define R language\nDepending on where you are and how you carried out R installation, your language “locale” might be different from the language of the graphs that you want to produce. For example, a french person might have a french “locale”. If that is the case, when creating a graph by day of the week, Monday will be displayed as “lundi”. If that french person wants to create an English report, as for this case study, the language “locale” should be changed.\nTask: Ensure your “locale” is in English and change it into English if it is not. If you don’t know how to do this try finding it online (searching for online help is an important skill for R users!). Otherwise, see the solution below\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\n\nStep 1.3: Install/load packages\nInstall and load the following packages: rio, skimr, janitor, gtsummary, broom, rstatix, ggfortify and tidyverse.\nYou can find more about installing/loading packages in the Packages section of the EpiRhandbook.\n\n\n Click to read a hint\n\n\nYou may end up using a long list of packages. Unfortunately different packages have functions with the same name. For example, the package {dplyr} (already installed with {tidyverse}) has a function called select() which we frequently use to subset columns of a data frame. But other packages such as {MASS} do also have a function called select(). This could create headaches if you want to subset columns using dplyr’s select() but R thinks you’re calling MASS’s select() (we call this masking - dplyr::select() is masked by MASS::select()). Given that you are more likely to use functions from {tidyverse}, ensure that this is the last package in your p_load() list so that functions from {tidyverse} (including {dplyr} functions) will always “prevail”.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  gtsummary,  # summary statistics, tests and regressions \n  broom,      # to generate tidy tibbles of regression analysis\n  rstatix,    # for statistics, including statistical tests\n  ggfortify,  # data visualisation for statistical analysis results\n  tidyverse  # data management and visualization\n)\n\n\n\n\n\n\nStep 2: Import and explore data\n\nStep 2.1: Import the data and brief exploration\nImport the data frame called “tbe.RDS” inside the “data” subfolder. If you are working within a project, finding the path to the dataframe should be relatively straightfoward. An “.RDS” file is an R object file. You can import this dataframe using the readRDS() function from {base}. However, we recommend that you use the import() function from {rio} because, as you may remember, this function will recognise the file type and import it whether the file is from R, Stata, excel or many others. If you have any doubts about importing review the Import and export chapter of the EpiRhandbook.\nThen, explore the data trying to answer the following questions:\n\nQUESTION: How many columns does the dataframe have?\n\n 523 2 8 6\n\n\n\nQUESTION: How many rows have missing the column ‘Other comorbidities’?\n\n 958 22 18 39\n\n\n\nQUESTION: How many cases have hypertension?\n\n 119 191 332 285\n\n\n\nQUESTION: How many character columns does the dataframe have?\n\n 0 6 2 8\n\n\n\nQUESTION: What is the difference between a column of class ‘character’ and a column of class ‘factor’?\n\n There is no difference between these classes, those are synonims Character columns contain text, whereas factors contain numbers Factors are used when we have more than 5 categories of data Both classes contain text, but factors are used when there are a limited number of unique character strings and they often represent categorical data\n\n\n\n\n Click to read a hint\n\n\nAn efficient way to explore data is to use the function skim() from the {skimr} package, as it gives you all the information needed with only one command. Of course, there are several alternatives.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Import the data\n\ntbe &lt;- import(\"data/tbe.RDS\")\n\n\n# Explore the dataframe\nskim(tbe)\n\n\nData summary\n\n\nName\ntbe\n\n\nNumber of rows\n523\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nhyper\n0\n1.00\nFALSE\n2\nno: 404, yes: 119\n\n\nvac\n0\n1.00\nFALSE\n2\nno: 503, yes: 20\n\n\nmono\n0\n1.00\nFALSE\n2\nno: 285, yes: 238\n\n\nother\n22\n0.96\nFALSE\n2\nyes: 317, no: 184\n\n\ntick\n0\n1.00\nFALSE\n2\nno: 345, yes: 178\n\n\nsex\n0\n1.00\nFALSE\n2\nmal: 332, fem: 191\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n18\n0.97\n47.30\n18.97\n1\n34\n47.0\n61\n90\n▂▅▇▆▂\n\n\nhospd\n39\n0.93\n42.23\n13.59\n3\n33\n41.5\n52\n86\n▁▆▇▃▁\n\n\n\n\n\n\n\nNormally, at this point we would start cleaning our data. Fortunately for you, the tbe data has already been cleaned, so you can jump directly to the fun part. However, feel free to rename/recode or change any aspect of the dataframe to accomodate it to your personal preferences.\n\n\nStep 2.2: Inspect factor columns\nAs we saw before, we have 6 factor columns representing categorical variables in our dataframe. Although we looked at them with the skim() function, explore them further with the tabyl() function from the {janitor} package.\n\n\n Click to read a hint\n\n\nTo save time, try to always use functions that allow you to apply the same function to many different objects (e.g., multiple columns) simultaneously. You can achieve this using several approaches, such as loops, lapply or purrr. Here we give the solution with purrr, so if you want to explore further purrr have a look at the dedicated section in the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n### One by one\ntabyl(tbe, hyper)\n\n\n\n\n\nhyper\nn\npercent\n\n\n\n\nno\n404\n0.7724665\n\n\nyes\n119\n0.2275335\n\n\n\n\n\ntabyl(tbe, vac)\n\n\n\n\n\nvac\nn\npercent\n\n\n\n\nno\n503\n0.9617591\n\n\nyes\n20\n0.0382409\n\n\n\n\n\n#### All at once\ntbe %&gt;% \n  \n  select(where(is.factor)) %&gt;% #we first select only the columns that are of class 'factor'\n  \n  map(.f = tabyl)              #inside map() from {purrr} we specify the function we want to apply to the entire dataframe\n\n$hyper\n .x[[i]]   n   percent\n      no 404 0.7724665\n     yes 119 0.2275335\n\n$vac\n .x[[i]]   n    percent\n      no 503 0.96175908\n     yes  20 0.03824092\n\n$mono\n .x[[i]]   n   percent\n      no 285 0.5449331\n     yes 238 0.4550669\n\n$other\n .x[[i]]   n    percent valid_percent\n      no 184 0.35181644     0.3672655\n     yes 317 0.60611855     0.6327345\n    &lt;NA&gt;  22 0.04206501            NA\n\n$tick\n .x[[i]]   n   percent\n      no 345 0.6596558\n     yes 178 0.3403442\n\n$sex\n .x[[i]]   n   percent\n    male 332 0.6347992\n  female 191 0.3652008\n\n\n\n\n\n\nStep 2.3: Histogram with length of hospitalisation\nOur outcome variable will be length of hospitalisation in days (column hospd). It might be worth it looking at this column in more detail, as its characteristics may influence how will carry out the analysis later on. One important aspect is to check its distribution and ascertain, at least visually, if it follows a normal distribution.\nTask Create an histogram with the distribution of hospd. Try adding the normal curve to this histogram.\n\nQUESTION: Does lenght of hospitalisation look like normally distributed?\n\n Yes No\n\n\n\n\n Click to read a hint\n\n\nThere are many ways to create an histogram in R, but try using the package {ggplot2}. You can have a look at the ggplot basics chapter of the EpiRhandbook if you struggle.\nAdding the normal curve to the histogram may prove quite challenging. Do not worry if you don’t manage. One hint is that, in the histogram, you will need to display the density and not the frequency count. Try asking a search engine or any AI platform for help. Most of us use these tools on a daily basis to ask for help in R.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%                                  #we call the data first and we pass it into ggplot with the pipe operator\n  \n  ggplot(mapping = aes(x = hospd)) +     #when drawing an histogram we only need to specify the x-axis   \n  \n  geom_histogram(aes(y = ..density..)) + #here we are telling ggplot2 to display the density and not the freq count\n  \n  #the function below will add the normal curve. \n  stat_function(fun = dnorm,  #In the fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(tbe$hospd, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(tbe$hospd, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) # Identify the colour and line width of the normal curve\n\n\n\n\nNow, that was tough! But we’re here to push you out of your comfort zone. Let’s go into more detail about what we have done.\nBy now, you should feel comfortable creating a basic histogram using ggplot, so let’s focus on the new things. We have added another aesthetic to the geom_histogram() in which we specify that we want the density plotted and not the frequency count. Why is that? Displaying the density is more appropriate when we want to focus on the shape of the data, as we can see the underlying probability distribution more clearly.\nBut, what is actually the density? The density represents the relative frequency, what we do is scale the y-axis so that the area under the histogram equals 1, normalising the histogram to represent probabilities (density) rather than raw counts. In fact, look at how the y-axis changes when you represent the density and when you represent the counts.\nFinally, why are we putting two dots before and after density in the aes()? The double dots before and after ..density.. are a special syntax used within ggplot2. They indicate an internal variable that ggplot2 calculates during the plotting process. So, ggplot2 normally calculates the density for histograms, but it does not display it unless you specify it (with this syntax).\n\n\n\n\nStep 2.4: Create a cross-table and calculate a statistical test\nLet’s say that we now want to explore whether sex is associated with hypertension. To find this out, create a cross-table displaying these two variables and calculate the appropriate statistical test to know if there is a statistical association between them.\n\n\n Click to read a hint\n\n\nThere are several ways in which you can do this. You could, for example, create the cross-table with tabyl() and then separately calculate the statistical test. The easiest way would be to use the tbl_summary() function from the {gtsummary} package, which allows you to do both, the cross tabulation and the statistical tests, in the same command. You should be familiar with this package by now, but if you need a little refresher have a look at the dedicated chapter of the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;% \n  select(hyper, sex) %&gt;%   #we select the columns we are interested in\n  tbl_summary(by = hyper) %&gt;% #we specify that we want by hypertension status\n  add_p()                     # adding this command will calculate the most appropriate statistical test\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      no, N = 4041\n      yes, N = 1191\n      p-value2\n    \n  \n  \n    sex\n\n\n0.5\n        male\n253 (63%)\n79 (66%)\n\n        female\n151 (37%)\n40 (34%)\n\n  \n  \n  \n    \n      1 n (%)\n    \n    \n      2 Pearson’s Chi-squared test\n    \n  \n\n\n\n\n\n\nAs you can see, there isn’t a significant association between sex and hypertension.\n\n\n\nStep 3: Check if there is a linear association between length of hospitalisation and age\nAge is a potential confounder for a more severe course of TBE involving a longer stay in hospital, for which we would like to adjust. Since age is a continuous variable, we could include it in the regression model in various ways (e.g. as a continuous variable, in categories, by transforming it, etc.). In order to decide this, we need to analyze the association of age with the length of hospitalisation.\n\nStep 3.1: Inspect a potential linear association\nPlease first have a look at the relationship between age and length of hospitalisation using a scatterplot.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n    \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\nWhat do you think? Is there a linear association? How can you be sure? Add a linear model trend line to help you with the interpretation.\n\n\n Click to read a hint\n\n\nFor the trend line, you can add a geom_smooth() geometry. Look up the documentation for geom_smooth (you can type ?geom_smooth() in the console and press “Enter”) and search for the methods option.For a linear trend line you can assign the methods argument to “lm” (linear model). \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\nNow you have visual evidence of a linear association of age with the duration of hospitalisation. Therefore, it seems reasonable to include age as a continuous variable in the analysis.\n\n\nStep 3.2: Check if the association between age and length of hospitalisation varies by sex\nNow, check visually whether the association between age and hospd differs by sex\n\n\n Click to read a hint\n\n\nYou may choose to create separate graphs adding a facet_grid() to your ggplot() (try looking up the syntax yourself). You may also decide to use colour coding to differentiate between factor levels of sex. For the latter, where do you think you should specify the colour, inside or outside the aes()? Read this section of the EpiRhandbook if you have doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# As separate graphs\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  facet_grid(~sex)  +                   # adding this function will generate a separate graph for each category of sex\n\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n# Same graphs with different colours\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd,\n                       colour = sex )) + #we add the colour in the aes so that it varies according to the categories of sex   \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\n\nWhat do you observe? The lines for female and male patients have different slopes, indicating that the association between age and hospitalisation days is modified by sex.\nWhy does this matter? Since there are different effects of age on the length of the hospitalisation by sex, you may want to control for this.\n\n\n\n\nStep 4: Check if there is a difference in length of hospitalisation by hypertension status\nLet’s now focus on our exposure of interest (hypertension). We can check whether TBE cases with high blood pressure have a significant longer stay in hospital compared with those without high blood pressure either by carrying out a simple statistical test or through univariate regression. We will do both.\n\nStep 4.1: Simple statistical test\nOne way would be to carry out a simple statistical test which enables us to ascertain if there is a significant difference between groups. As you know, the choice of statistical test will be determined by the number of groups that we have and the nature of the variables. If you don’t remember well how to choose the appropriate statistical test have a look at this BMJ article\n\nQUESTION: What simple statistical tests do you think would be most appropriate in this case?\n\n Chi-square or Fisher's exact test ANOVA or Kruskal Wallis Student t test or Mann Whitney/Wilcoxon rank-sum test McNemar's test or Spears Rank\n\n\nIn this case we have independent data, two groups (hypertension yes/no) and a quantitative dependent outcome (length of hospitalisation), so we will choose either a t-test or Wilcoxon rank-sum test (also known as Mann-Whitney). We’d do a t-test if length of hospitalisation is normally distributed and Wilcoxon if it isn’t. We already checked in Step 2.3 that the distribution of hospd looked normal from the histogram, let’s now have a look if the distribution looks normal for both categories of hypertension: hypertension-yes and hypertension-no.\nTask Check visually whether length of hospitalisation is normally distributed for both categories of hypertension.\n\n\n Click to read a hint\n\n\nA simple way to check this is by generating a histogram for hospd using ggplot(). Stratification by hyper is easily done by adding a facet_grid() to your ggplot, as we have already seen with the scatterplot. Looking at the frecuency distribution of “hospd” by “hypertension” can already give us an idea of whether the data is normally distributed or not.However, if we want to to add a normal curve, we’d need to plot the density and not the frequency, as you may rembember from the histogram we built above in Sep 2.3. \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Plot the frecuency distribution by hypertension status\ntbe %&gt;%\n  \n  ggplot(mapping = aes(x = hospd)) +\n  \n    geom_histogram() + \n    \n    facet_grid(~hyper) # add facet_grid() to get a graph for each hyper status\n\n\n\n# Plot the density and add a  normal curve\ntbe %&gt;%                                  \n  \n  ggplot(mapping = aes(x = hospd, fill = hyper)) +  #remember that for bars, fill is is the interior colour     \n  \n    geom_histogram(aes(y = ..density..)) + #here we are telling ggplot2 to display the density and not the freq count\n  \n    facet_grid(~hyper) + # add facet_grid() to get a graph for each hyper status\n\n    #the function below will add the normal curve. \n    stat_function(fun = dnorm,  #The fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(tbe$hospd, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(tbe$hospd, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) + # Identify the colour and line width of the normal curve\n    labs(\n      x = \"Length of hospitalisation in days\",\n      y = \"Density\"\n      )\n\n\n\n\n\n\nFrom the graphs looks like the data is normally distributed, but there is a test we can do to determine this: Shapiro-Wilk test.\nTask: Use the Shapiro test to determine whether length of hospitalisation comes from a normally-distributed population.\n\n\n Click to read a hint\n\n\nYou can choose {base} functions to carry out the Shapiro test (and any other statistical test) or functions from the {rstatix} package. Here we will use the latter, because {rstatix} has a syntax that is compatible with {dplyr}, which can be an advantage. In any case the name of the functions are very similar. For example, using {base} the Shapiro test function is shapiro.test(); and with {rstatix} the same function is written as: shapiro_test(). If you want to know more about how to carry out simple statistical tests in R, read the EpiRhandbook Chapter on Simple statistical tests\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%\n  \n  group_by(hyper) %&gt;% #we first group by out independent or exposure variable\n  \n  \n  shapiro_test(hospd)  # this function performs the Shapiro-Wilk test for all groups separately\n\n\n\n\n\nhyper\nvariable\nstatistic\np\n\n\n\n\nno\nhospd\n0.9946512\n0.2207864\n\n\nyes\nhospd\n0.9870178\n0.3632204\n\n\n\n\n\n\n\n\nThe null hypothesis of Shapiro’test is that data is normally distributed. As our results were non-significant (p&gt;0.05) we cannot reject such null hypothesis. In other words, we can conclude that data is normally distributed.\nNote: A statistician, or an epidemiologist pretending to be a statistician, would feel very uncomfortable by the conclusion written above. When we are unable to reject a null hypothesis we cannot conclude that this hypothesis (in this case that data is normally distributed) is true, but we should simply state that we were unable to reject it. However, in practical terms, by carrying out a Shapiro test we are assuming that the data is normally distributed….so word it as you like, but the consequences are the same.\n\nQUESTION: Now that we know that the data is normally distributed, what test should we carry out?\n\n Student t-test Wilcoxon rank-sum test\n\n\nTask Carry out the statistical test to ascertain if there are significant differences in length of hospitalisation according to hypertension status\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%\n  \n  t_test(hospd ~ hyper)  # we write first our dependent variable and then the exposure one\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\ndf\np\n\n\n\n\nhospd\nno\nyes\n373\n111\n-2.706422\n181.4443\n0.00745\n\n\n\n\n\n\n\n\n\nQUESTION: Is there a significant difference in the length of hospitalisation among patients with and without hypertension?\n\n Yes No\n\n\nNote: In large datasets (as this one) the Shapiro-Wilk test may reject the normality hypothesis even though the deviation from the normal distribution is rather small. In this case you may still choose to calculate the t-test despite the violation of the normality assumption. This is acceptable because for large samples, the error imposed by t-test approximation is negligible.\n\n\nStep 4.2: Univariate linear regression between hospd and hyper\nThe other way was through univariate regression. Carry out a univariate linear regression with length of hospitalisation (hospd) as the dependent variable and hypertension as the independent variable. Assign this model to an object named: hyper_hospd_lm. If this is the first time doing regression in R, have a look at the Univariate regression chapter of the EpiRhandbook.\n\n\n Click to read a hint\n\n\nAs we are doing linear regression, we could use the lm() function from {base} to analyse the association between hypertension and length of hospitalisation. The syntax is: lm(outcome/dependent variable ~ exposure/independent variable, data = dataframe) You can print the model output in a subsequent command using the summary() function. However, the tidy() function from {broom} provides an overview which can be more easily compiled and used in down-stream analyses if needed.\nThe approach described above is the {base} R approach. You can also perform univariate regression analysis using the function tbl_uvregression() from the {gtsummary} package. If you want to explore this alternative approach further read the dedicated EpiRhandbook chapter \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#First we write the formula (lm stands for linear model) and assign the model to an object\nhyper_hospd_lm &lt;- lm(hospd ~ hyper, data= tbe)\n\n# Prin the detailed output of the model\nsummary(hyper_hospd_lm)\n\n\nCall:\nlm(formula = hospd ~ hyper, data = tbe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.324  -9.324  -1.261   9.676  44.676 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.3244     0.6991  59.113  &lt; 2e-16 ***\nhyperyes      3.9369     1.4598   2.697  0.00724 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.5 on 482 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.01487,   Adjusted R-squared:  0.01282 \nF-statistic: 7.273 on 1 and 482 DF,  p-value: 0.007243\n\n# Print the main model parameters\nresults_hyper_hospd_lm &lt;- tidy(hyper_hospd_lm) \nresults_hyper_hospd_lm\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n41.324397\n0.6990698\n59.113403\n0.0000000\n\n\nhyperyes\n3.936864\n1.4597610\n2.696924\n0.0072434\n\n\n\n\n\n\n\n\n\nQUESTION: Based on the results of this model, is there a significant association between hypertension and hospd?\n\n Yes No\n\n\n\nQUESTION: How much does having hypertension increase length of hospitalisation (in days)?\n\n 3.9 41.3 1.46 1.2\n\n\n\nQUESTION: What % of the variability in length of hospitalisation can be explained by hypertension?\n\n 2.7% 41% 1.3% 3%\n\n\nIn this case the regression equation is: \\(hopsd(predicted)=41.3+3.9⋅hyper\\).\nNow let’s go through the outputs of the model. We will first review the outputs of the tidy() function. As explained before, the advantage of using tidy() is that we can save the output as an object for further manipulation. Also, tidy() keeps what we would normally need in epidemiology for the interpretation of the model:\nWe have four columns and two rows. The rows refer to the intercept (baseline value for hospd when there is NO hypertension: 41.3) and our only exposure/predictor (hypertension). In most cases, you will be interested in the row for hypertension (your predictor) and in the columns estimate and p.value. The estimate of hypertension tells us the estimated change in hospd when you go from NO hypertension to YES hypertension (that’s why it is written as hyperyes). In this case, it’s approximately 3.94, meaning that having hypertension is associated with an increase in 3.94 days of hospitalisation. The column p.value tells us whether this estimate is statistically significant.\nThis is most of what you need to know to be a functional epidemiologist. But if you want to know more, we will explain the rest of the elements of the output of both functions:\nThe column std.error provides an estimate of the variability or uncertainty associated with the estimate. In this case it means that the estimated effect of hypertension on hospitalisation duration (hospd) is expected to vary by about 1.46 days (on average) due to sampling variability. Finally, the statistic column gives us a value of the statistical test (t-test in this case) used to ascertain if the estimate is significantly different from 0 (normally you can ignore this column).\nYou may have noticed that the summary() output has more information than the tidy() output. Here we leave a brief explanation on what each part of the summary() output means:\nCall: This line shows the formula used for the regression model.\nResiduals: These are the differences between the actual hospd values and the predicted values from the regression model. The summary provides statistics like minimum, median, and maximum residuals.\nCoefficients: This is the bit that interests us the most:\n\nIntercept: The estimated intercept (baseline value) for hospd when age is zero. In this case, it’s approximately 41.3.\nhyperyes: The estimated change in hospd when you go from NO hypertension to YES hypertension. Here, it’s approximately 3.9. The t-value and p-value indicate whether this coefficient is statistically significant.\n\nSignificance Codes: Indicate whether the p-value is highly significant (*** p &lt; 0.001) or only marginally significant (* p&lt;0.05)\nResidual Standard Error: This measures the average deviation of the observed hospd values from the regression line. In this case, it’s approximately 13.5.\nMultiple and Adjusted R-squared: These values (0.01487 and 0.01282) represent the proportion of variance in hospd explained by the linear relationship with hypertension. Higher values indicate better fit. The adjusted one is adjusted for the number of predictors. In this case we could say that around 1.3% of the variability in hospd is explained by hypertension.\nF-statistic and p-value: The F-statistic tests whether the overall model (including all predictors) is significant. A low p-value (like this one, &lt; 0.007243) indicates that the model is significant.\n\n\n\nStep 5: Check if there is a linear association between length of hospitalisation and the rest of variables\nWe also need to know if there is a univariate association between our outcome (length of hospitalisation) and the confounders we may want to introduce in our model. We will do this in two steps, first with the continous variable and then with the factor variables.\n\nStep 5.1: Univariate linear regression between length of hospitalisation and age\nTry running a linear regression model with length of hospitalisation (hospd) as the dependent variable and age as the independent variable.\n\nQUESTION: Based on the results of the model, is there a significant association between age and hospd?\n\n Yes No\n\n\n\nQUESTION: How much does an additional year of age increase length of hospitalisation (in days)?\n\n 0.372 24.5 0.271 1.42\n\n\n\nQUESTION: What % of the variability in length of hospitalisation can be explained by age?\n\n 65% 15% 27% 99%\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Run the linear regression and assign the output to age_hospd_lm\nage_hospd_lm &lt;- lm(hospd ~ age, data = tbe)\n\n# Print the results using the summary() function\nsummary(age_hospd_lm)\n\n\nCall:\nlm(formula = hospd ~ age, data = tbe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.676  -7.893   0.521   6.844  32.279 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24.5314     1.4233   17.24   &lt;2e-16 ***\nage           0.3722     0.0278   13.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.61 on 482 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.2711,    Adjusted R-squared:  0.2696 \nF-statistic: 179.2 on 1 and 482 DF,  p-value: &lt; 2.2e-16\n\n# Print the results of the regression analysis with the tidy() function\ntidy(age_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.5314214\n1.4232588\n17.23609\n0\n\n\nage\n0.3722366\n0.0278031\n13.38831\n0\n\n\n\n\n\n\n\n\n\n\nStep 5.2: Univariate linear regression between length of hospitalisation and factor variables\nNow carry out a similar univariate model for each of the factor variables that we have not explored to ascertain which ones are significantly associated with length of hospitalisation.\n\nQUESTION: What of the following variables have a significant linear association with length of hospitalisation?\n\n Sex Other comorbidities Monophasic disease Large tick at removal Vaccinated against TBE\n\n\n\n\n Click to read a hint\n\n\nWe have seen already how to write a linear formula with lm(). You can then use either summary() from {base} or tidy() from {broom} to look at the coefficient and significance value. You could write these functions for each variable, but you could also try to use map() from {purrr} to do them all at once, as we did in the Step 2.2. Have a look at the Chapter on Purrr in the EpiRhandbook if you have any doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n## Here we write one by one for each variable the formula and tidy\n\n#sex\nsex_hospd_lm &lt;- lm(hospd ~ sex, data = tbe)\ntidy(sex_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n46.66559\n0.6932755\n67.31176\n0\n\n\nsexfemale\n-12.41704\n1.1595927\n-10.70810\n0\n\n\n\n\n\n#other comorbidities\nother_hospd_lm &lt;- lm(hospd ~ other, data = tbe)\ntidy(other_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n39.590909\n1.014237\n39.035162\n0.0000000\n\n\notheryes\n4.142857\n1.271413\n3.258466\n0.0011993\n\n\n\n\n\n#vaccination\nvac_hospd_lm    &lt;- lm(hospd ~ vac, data = tbe)\ntidy(vac_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n42.556989\n0.6262426\n67.956072\n0.0000000\n\n\nvacyes\n-8.399095\n3.1607381\n-2.657321\n0.0081379\n\n\n\n\n\n#monophasic course\nmono_hospd_lm   &lt;- lm(hospd ~ mono, data = tbe)\ntidy(mono_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n40.690566\n0.8291071\n49.077574\n0.0000000\n\n\nmonoyes\n3.396192\n1.2325691\n2.755377\n0.0060844\n\n\n\n\n\n#large tick at removal\ntick_hospd_lm   &lt;- lm(hospd ~ tick, data = tbe) \ntidy(tick_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n42.731250\n0.7593945\n56.270158\n0.0000000\n\n\ntickyes\n-1.487348\n1.3045725\n-1.140103\n0.2548094\n\n\n\n\n\n## Here we do it with {purrr} all in one command\ntbe %&gt;% \n  select(-hyper, -age, -hospd) %&gt;% # we first remove the columns we have explored before + the outcome column\n  map(.f = ~lm(hospd ~ .x, data = tbe)) %&gt;%   # here we carry out the model for each column. \".x\" represents all the column\n  map(tidy)                                   # finally we do tidy on each model\n\n$vac\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    42.6      0.626     68.0  4.69e-249\n2 .xyes          -8.40     3.16      -2.66 8.14e-  3\n\n$mono\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    40.7      0.829     49.1  1.30e-189\n2 .xyes           3.40     1.23       2.76 6.08e-  3\n\n$other\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    39.6       1.01     39.0  2.43e-151\n2 .xyes           4.14      1.27      3.26 1.20e-  3\n\n$tick\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    42.7      0.759     56.3  5.50e-214\n2 .xyes          -1.49     1.30      -1.14 2.55e-  1\n\n$sex\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     46.7     0.693      67.3 2.99e-247\n2 .xfemale       -12.4     1.16      -10.7 3.79e- 24\n\n\n\n\nStatistically significant (\\(p&lt;0.05\\)) univariable effects were observed for sex, other comorbidities, vaccination and monophasic disease course but not for large tick at removal. Women had on average 12.4 days shorter hospitalisation stay than men, other comorbidities increase the hospitalisation by 4.1 days, vaccination reduced it by 8.4 days, and monophasic disease course increased it by 3.4 days.\n\n\n\nStep 6: Multivariable analysis\nTo control for possible confounding, we will adjust for the minimum adjustment set that we have identified through the DAG analysis (Goal 1), adding each variable in different steps.\n\nStep 6.1: Add age as a covariate to the model\nAs a first step, add age to the model of the main effect.\n\n\n Click to read a hint\n\n\nWe have already calculated and saved the model of our main effect (hyper_hospd_lm). You can add age to this model by creating a new model and adding the additional variable, so that the formula syntax is model &lt;- lm(hospd ~ hyper + newVariable, data = data)\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Creating a new model and adding age \nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age, data = tbe)\n\ntidy(hyper_hospd_adj_lm)                                   \n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n23.6918245\n1.437666\n16.479366\n0.0000000\n\n\nhyperyes\n3.8015966\n1.245007\n3.053473\n0.0023875\n\n\nage\n0.3715581\n0.027567\n13.478364\n0.0000000\n\n\n\n\n\n\n\n\nAdjusting for age, we find that hypertension is associated with a 3.8 increase in the days of hospitalisations. This is only a slight change compared to the effect in the unadjusted model (where the coefficient was 3.9).\n\n\nStep 6.2: Compare the performance of both models\nWe have the univariate model (hyper_hospd_lm), and now we have one adjusted by age (hyper_hospd_adj_lm). Compare both models calculating model performance metrics such as AIC and logLik.\n\n\n Click to read a hint\n\n\nOne easy function to calculate performance metrics is glance() from the {broom} package. But, of course, there are many others. Choose the one you feel most comfortable with!\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nglance(hyper_hospd_lm)   # performance metrics of the univariate model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0148657\n0.0128219\n13.50128\n7.273399\n0.0072434\n1\n-1945.512\n3897.024\n3909.57\n87861.17\n482\n484\n\n\n\n\n\nglance(hyper_hospd_adj_lm) # performance metrics of the adjusted model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.2849348\n0.2819616\n11.51466\n95.83297\n0\n2\n-1867.974\n3743.948\n3760.676\n63774.52\n481\n484\n\n\n\n\n\n\n\n\nThe output of glance() gives us several performance metrics. If you want to focus on the important bits, you can look at the logLik and the AIC. A larger logLik means a better model; and a lower AIC means also a better model. So, if your logLik goes up and your AIC goes down, you are going in the right direction. The advantage of the logLik is that there is a formal test (Likelihood Ratio Test) to test if two logLiks are significantly different. However, AIC takes more things into consideration than the logLik, such as the complexity of the model (favours simpler models), so it is very useful to compare models with different numbers of parameters (like this case).\nIf you want to know more, let’s review each element of the output given by glance():\n\nr.squared: This is a measure of how well the independent variables in your model explain the variability of the dependent variable. Think of it as a percentage that tells you how much of the changes in the outcome can be predicted by the model. So, in our example, hypertension alone explains 1.5% of the variability in length of hospitalisation, but when we add age, that model explains 28.5% of the variability in our dependent variable.\nadj.r.squared: The adjusted R-squared value adjusts the R-squared value based on the number of predictors in the model. It is a more accurate measure when comparing models with different numbers of predictors (as in this case). Here, it is slightly lower at 28.2%, indicating a small adjustment for the number of predictors.\nsigma: This represents the residual standard error, which is the standard deviation of the residuals.\nstatistic: This is the F-statistic value for the overall significance of the model. It tests whether at least one predictor variable has a non-zero coefficient.\np.value: The p-value associated with the F-statistic.\ndf: Degrees of freedom associated with the model which typically correspond to the number of predictors (1 in the first case and 2 in the second).\nlogLik: The log-likelihood of the model, which is a measure of the model fit. Higher values indicate a better fit. As stated above, one of the main performance measures that you will use as an epidemiologist.\nAIC: Akaike Information Criterion, which is used for model comparison. Lower values indicate a better model.\nBIC: Bayesian Information Criterion, similar to AIC but with a higher penalty for models with more parameters.\ndeviance: This is a measure of the goodness of fit of the model. Lower values indicate a better fit.\ndf.residual: The residual degrees of freedom, which is the number of observations minus the number of parameters estimated.\nnobs: The number of observations used in the model, which is 484 in our case\n\n\n\n(Optional) Step 6.3: Plotting effects of age by hypertension\nIf you remember, in Step 3.2 we checked visually for the association between age and length of hospitalisation by sex. Now, we will do something similar by plotting the estimated effect of age on length of hospitalisation by hypertension status. The final output that we want is a scatterplot displaying the raw data (length of hospitalisation by age), and then the fitted regression line by hypertension status.\n\n\n Click to read a hint\n\n\nIn order to plot the estimated effects of age by hypertension we have to extract the fitted values of “hospd” from the model and add them to the dataframe. This only works if we have removed all rows that contain NAs in the hospd column before. we can then pass this new dataframe to a ggplot() with a geom_point() for the scatterplot and geom_line() for the fitted trend lines. Note: Previously, we used geom_smooth() to add trend lines to the scatterplot. Because we want to plot the predicted values of an existing model and not fit a linear model within ggplot(), we have to use geom_line() and use the predicted values of our model as input for y.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# First add a variable with the fitted values to the dataframe\ndata_fitted &lt;- tbe %&gt;%\n  \n  filter(!is.na(hospd)) %&gt;%   # Remove NAs \n\n  mutate(fit = predict(hyper_hospd_adj_lm)) # Create a new variable with the predicted values\n\n### You can now open data_fitted to have a look at the new column\n\n# Now we do the plot with the fitted lines\n\nplot_fitted &lt;- data_fitted %&gt;%\n  \n  ggplot(mapping = aes(x = age, \n                       y = hospd,\n                       colour = hyper)) +\n  \n  geom_point() +                            # Adding a scatter plot\n  \n  geom_line(aes(y = fit)) +                 # we add a line specifying that we want the fitted and not the observed values in the y-axis\n\n  labs(\n    title = \"Effect of age on TBE hospital stay length for people\\nwith and without hypertension \", # Title of the plot, note that \"\\n\" breaks the title into the next line\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\",\n    colour = \"Hypertension\"\n  ) +\n  \n  theme_bw()                                  # we add a predefined theme\n\nplot_fitted\n\n\n\n\n\n\nWe did not allow the association between age and hospitalisation to vary by levels of hypertension. So we force them to be parallel by the way we specify the model. If we want the slopes to differ, we would need to allow for an interaction term. We’ll get to that later.\n\n\nStep 6.4: Continue building the adjusted model and check quality of the model\nContinue adding the rest of variables that we have to adjust for according to the DAG (TBE vaccination, age, large tick, monophasic course, other comorbidities and sex) to the model hyper_hospd_adj_lm one by one. Check, after you add each variable, model performance metrics to ensure that the model is improving.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n### We first add sex to the previous model\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n28.9661798\n1.3613386\n21.277718\n0.000000\n\n\nhyperyes\n3.4659484\n1.1079250\n3.128324\n0.001865\n\n\nage\n0.3452202\n0.0246333\n14.014368\n0.000000\n\n\nsexfemale\n-11.0377082\n0.9762639\n-11.306071\n0.000000\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4353144\n0.4317851\n10.24315\n123.3435\n0\n3\n-1810.837\n3631.674\n3652.584\n50362.62\n480\n484\n\n\n\n\n\n### TBE vaccination\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n29.0192105\n1.3398585\n21.658415\n0.0000000\n\n\nhyperyes\n3.5401913\n1.0905445\n3.246260\n0.0012512\n\n\nage\n0.3507362\n0.0242813\n14.444681\n0.0000000\n\n\nsexfemale\n-10.9105961\n0.9613218\n-11.349577\n0.0000000\n\n\nvacyes\n-9.6218919\n2.3644063\n-4.069475\n0.0000551\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.454185\n0.4496271\n10.08105\n99.6467\n0\n4\n-1802.611\n3617.223\n3642.315\n48679.6\n479\n484\n\n\n\n\n\n### Monophasic disease course\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n27.9231475\n1.3632885\n20.482199\n0.0000000\n\n\nhyperyes\n3.7307298\n1.0799901\n3.454411\n0.0006006\n\n\nage\n0.3443505\n0.0240868\n14.296254\n0.0000000\n\n\nsexfemale\n-11.0608203\n0.9517650\n-11.621378\n0.0000000\n\n\nvacyes\n-9.8262935\n2.3391721\n-4.200757\n0.0000317\n\n\nmonoyes\n3.1330906\n0.9155900\n3.421936\n0.0006751\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4672362\n0.4616634\n9.970207\n83.84164\n0\n5\n-1796.755\n3607.509\n3636.784\n47515.6\n478\n484\n\n\n\n\n\n### Large tick at removal\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono + tick, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n28.1079813\n1.3968015\n20.1231029\n0.0000000\n\n\nhyperyes\n3.7655402\n1.0821694\n3.4796219\n0.0005482\n\n\nage\n0.3446256\n0.0241066\n14.2959288\n0.0000000\n\n\nsexfemale\n-11.0239613\n0.9542624\n-11.5523372\n0.0000000\n\n\nvacyes\n-9.7114161\n2.3481138\n-4.1358371\n0.0000418\n\n\nmonoyes\n3.0852716\n0.9194697\n3.3554903\n0.0008555\n\n\ntickyes\n-0.5959864\n0.9677751\n-0.6158315\n0.5382995\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4676595\n0.4609634\n9.976687\n69.8405\n0\n6\n-1796.562\n3609.124\n3642.581\n47477.85\n477\n484\n\n\n\n\n\n### Other comorbidities\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono + tick + other, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n27.1251926\n1.4596417\n18.5834593\n0.0000000\n\n\nhyperyes\n2.7139444\n1.1769928\n2.3058292\n0.0215491\n\n\nage\n0.3406119\n0.0240754\n14.1477140\n0.0000000\n\n\nsexfemale\n-11.0797108\n0.9506756\n-11.6545649\n0.0000000\n\n\nvacyes\n-9.2298015\n2.3484899\n-3.9301005\n0.0000975\n\n\nmonoyes\n3.0995048\n0.9157172\n3.3847838\n0.0007712\n\n\ntickyes\n-0.7577796\n0.9665465\n-0.7840074\n0.4334257\n\n\notheryes\n2.3008449\n1.0351078\n2.2228070\n0.0266978\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4731284\n0.4653803\n9.935728\n61.06371\n0\n7\n-1794.063\n3606.126\n3643.765\n46990.1\n476\n484\n\n\n\n\n\n\n\n\nAs you can see, adding the rest of variables identified through the DAG improved the model, according to the metrics. The only exception was the addition of the variable “Large tick at removal”. However, the choice of covariates should be made conceptually through the DAG, and not based on whether there are small differences in AIC/logLik. If you concluded that this variable was a possible confounder, then keep it in the model.\n\n\nStep 6.5: Adding an interaction term\nWe saw in Step 3.2 that the effect of age on length of hospitalisation is modified by sex. We knew this because the slopes were different for males and females. Given this information add now to the model an interaction term between sex and age.\n\n\n Click to read a hint\n\n\nAn interaction can be written as a multiplication of two variables in your regression formula, e.g. age*sex. There is no need to keep both variables individually in the formula AND in the interaction term. By just including the interaction term, R will estimate the coefficients independently for both variables and with the interaction.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + vac + mono + tick + other + age*sex, data = tbe) # formula\n\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.2675847\n1.6629366\n14.5932114\n0.0000000\n\n\nhyperyes\n2.9092258\n1.1650411\n2.4971014\n0.0128588\n\n\nvacyes\n-9.0596630\n2.3224314\n-3.9009389\n0.0001096\n\n\nmonoyes\n2.9451978\n0.9064522\n3.2491487\n0.0012396\n\n\ntickyes\n-0.6988387\n0.9557594\n-0.7311868\n0.4650256\n\n\notheryes\n2.2322254\n1.0235851\n2.1807913\n0.0296887\n\n\nage\n0.3998845\n0.0293313\n13.6333864\n0.0000000\n\n\nsexfemale\n-3.0511594\n2.5045520\n-1.2182456\n0.2237354\n\n\nage:sexfemale\n-0.1728331\n0.0499755\n-3.4583541\n0.0005924\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4860689\n0.4774132\n9.823278\n56.15605\n0\n8\n-1788.045\n3596.09\n3637.911\n45835.97\n475\n484\n\n\n\n\n\n\n\n\n\nQUESTION: Looking at the estimates of the model, for females, what is the net effect of age on length of hospitalisation?\n\n +0.4 +0.23 -0.17 +0.17 -0.32\n\n\nLooking at the results, we see that age has an estimate of 0.4, which means that one additional year of age is associated with an increase in 0.4 days of hospitalisation. We also see that being female is associated with a decrease in 3 days of hospitalisation (compared with being male). The interaction estimate of -0.17 means that for females, each additional year of age is associated with a decrease in 0.17 days of hospitalisation. So, to sum up, all things equal, a female of 41 years, compared to one of 40 years, would have +0.4 days of hospitalisation for having one extra year, but as she is female you have to add the interaction term (-0.17), so the net effect would be +0.23. For males, one extra year would still be associated with +0.4 days of hospitalisation.\nWe also see that adding the interaction term changes the effect estimate for hypertension and improves the model, so we may want to keep the interaction in the model, assuming that this ensures a better control for confounding of “age” and “sex”.\nNote: You should also look for other effect interaction terms in the dataframe. In the interest of the exercise, we will skip this step.\n\n\n\nStep 7: Model diagnostics\nThe multiple linear regression model that we just run, like all models, has several assumptions:\n\nLinearity: The relationship between the independent variables and the dependent variable (length of hospitalisation) is linear. This means that the change in the dependent variable is proportional to the change in the independent variables.\nIndependence: The observations are independent of each other.\nNormality of Residuals: The residuals of the model are normally distributed, meaning that they are symmetrically distributed around 0 and the residuals’ distribution is bell-shaped.\nHomoscedasticity: The residuals (difference between the observed values and the values estimated from the model) have constant variance at every level of the independent variables. This means that the spread of the residuals should be roughly the same across all levels of the independent variables.\nNo Multicollinearity: The independent variables are not highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each independent variable on the dependent variable.\nNo Autocorrelation: The residuals are not correlated with each other. This is particularly important in time series data where observations are ordered in time.\n\nIn this case study we will learn how to check two of these assumptions: Normality of residuals and Homoscedasticity\n\nStep 7.1: Normality assumption\nTo assess whether the residuals of our model are normally distributed or not we will do these three things:\n\nGenerating some diagnostic plots of the model\nCreating an histogram with the residuals\nCarrying out a Shapiro-test\n\nTask: Generate diagnostic plots of the model hyper_hospd_adj_lm. Don’t worry if you don’t find out how to do these. If you struggle, have a look at the hint below.\n\n\n Click to read a hint\n\n\nYou can retrieve diagnostic plots by using the function autoplot() from {ggfortify}. Pass your model object to the autoplot() function to generate three basic diagnostic plots. You can also specify which plot autoplot() needs to generate using the which = argument. In the solution, we specify that we want the first 2 plots.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nautoplot(hyper_hospd_adj_lm, which = 1:2)\n\n\n\n\n\n\nLet’s interpret each graph:\n\nResiduals vs. fitted plot (plot 1, upper left corner): You see a scatterplot with a blue line (LOESS smoothing curve). If the residuals are normally distributed around zero, then the blue line will be horizontal (parallel to the zero line).\nQ-Q plot (plot 2, upper right plot): The Q-Q plot shows you whether or not the residuals are normally distributed. On the X-axis you can see the quantiles of a normal distribution, while on the Y-axis you can see the standardized residuals (residuals dived by their standard deviation). If the residuals are normally distributed, in general, most of the points need to lie on the diagonal.\n\n\nQUESTION: Are residuals normally distributed, based on these plots?\n\n No Yes\n\n\nTask: Build an histogram with the residuals of the hyper_hospd_adj_lm model.\n\n\n Click to read a hint\n\n\nTo do an histogram of the residuals you can follow a similar approach than the one we used in the step 6.3. That is, first adding to the tbe dataframe the residuals as a column (you can use the function resid()) and then using ggplot() to plot this column in the x-axis of a geom_histogram()\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# First add a variable with the residual values to the dataframe\ndata_res &lt;- tbe %&gt;%\n  \n  filter(!is.na(hospd)) %&gt;%   # Remove NAs \n  \n  mutate(res = resid(hyper_hospd_adj_lm)) # Create a new variable with the residual values\n\n### You can now open data_res to have a look at the new column\n\n# Now we do the histogram with the residuals\n\ndata_res %&gt;%\n  \n  ggplot(mapping = aes(x = res)) +\n  \n  geom_histogram(aes(y = ..density..)) +                        # Adding a histogram with density\n  \n  #the function below will add the normal curve. \n  stat_function(fun = dnorm,  #The fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(data_res$res, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(data_res$res, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) +\n  labs(\n    title = \"Histogram of Residuals\", \n    x = \"Residuals\",\n    y = \"Frequency\"\n  ) +\n  \n  theme_bw()                                  # we add a predefined theme\n\n\n\n\n\n\n\nQUESTION: Does the histogram suggest a normal distribution of residuals?\n\n No Yes\n\n\nTask: Carry out a Shapiro-Wilk test to ascertain statistically if residuals are normally distributed\n\n\n Click to read a hint\n\n\nReview Step 4.1 and/or have a look at the EpiRhandbook Chapter on Simple statistical tests if you still have doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ndata_res %&gt;%\n  shapiro_test(res)\n\n\n\n\n\nvariable\nstatistic\np\n\n\n\n\nres\n0.9960796\n0.2762974\n\n\n\n\n\n\n\n\n\nQUESTION: How do you interpret the result of the Shapiro-Wilk test?\n\n Residuals do not follow a normal distribution We cannot reject the null hypothesis that residuals follow a normal distribution We accept the alternative hypothesis\n\n\n\n\nStep 7.2: Homoscedasticity\nNow we will check the other assumption, that the variance of the residuals (i.e., the magnitude of their distance to 0) does not depend on the predictor.\nCheck homoscedasticity by running plot number 3 (Scale-location plot) in the autoplot() function.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Generate the scale-location plot\nautoplot(hyper_hospd_adj_lm, which = 3)\n\n\n\n\n\n\nThis plot is called a scale-location plot and represents the fitted values vs. the square root of the standardized residuals. With this plot it is possible to check for heteroscedasticity (i.e., heterogeneity of variance - the opposite of homoscedasticity). Ideally, the residuals should be evenly spread out across all levels of fitted values. This means there should be no clear pattern or systematic change in the spread of residuals as you move along the x-axis. If the residuals form a funnel shape (narrow at one end and wide at the other), this indicates heteroscedasticity, meaning the variance of the residuals is not constant. The blue smoothed line should be roughly horizontal and close to zero. Significant deviations from this line can indicate issues with homoscedasticity.\n\nQUESTION: Based on this graph, do you think that the assumption of homoscedasticity is reasonably met?\n\n No Yes\n\n\n\n\n\nStep 8: Public health relevance\n\nDiscuss the PH relevance of your findings and any next steps and recommendations that may result from this.\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nThis is the first time that an association between hypertension and severity of TBE (based on the duration of hospitalisation) has been detected. Even though you suspected such an association in the beginning (and thus tested for it) and even though you think that it may be causal based on your DAGs and the control for confounding, there are more steps involved in establishing a causal relationship. We should follow the Bradford Hill Criteria to argue causality. For example, the biological basis for this effect needs to be established. Similar effects have been seen for other infections (e.g., SARS-CoV-2), which may have promoted the hypothesis in the beginning. You may also decide to look for a possible dose-response relationship in further analysis of our data. Furthermore, these results should be repeated independently to rule out a chance finding. Once the causal relationship between hypertension and TBE is further corroborated, vaccination against TBE for persons with hypertension could be recommended."
  },
  {
    "objectID": "pages/oswego.es.html#overview",
    "href": "pages/oswego.es.html#overview",
    "title": "Oswego (ES)",
    "section": "Overview",
    "text": "Overview\nCase study characteristics\n\n\n\n\n\n\n\n\nName\nOswego\n\n\nTool\nR\n\n\nLanguage\nSpanish/Español\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nGastrointestinal\n\n\nKeywords\nGastrointestinal;Outbreak investigation\n\n\nTechnical complexity\nIntermidiate\n\n\nMethodolocial complexity\nIntermidiate\n\n\n\nAuthorship\nOriginal authors: Centre for Disease Prevention and Control (CDC)\nData source: Epi Info, version 3.5.4 (CDC)\nAdapted to R by: Leonel Lerebours Nadal y Alberto Mateo Urdiales"
  },
  {
    "objectID": "pages/oswego.es.html#instructions",
    "href": "pages/oswego.es.html#instructions",
    "title": "Oswego (ES)",
    "section": "Instructions",
    "text": "Instructions\n\nGetting Help\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\nHints and Solutions\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\nPosting a question in the Community Forum\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\nIcons\nYou will see these icons throughout the exercises:\n\n\n\n\n\n\n\nIcon\nMeaning\n\n\n\n\n\nObserve\n\n\n\nAlert!\n\n\n\nAn informative note\n\n\n\nTime for you to code!\n\n\n\nChange to another window\n\n\n\nRemember this for later\n\n\n\n\n\n\nTerms of use\nThis case study has been adapted from an existing tutorial on Epi Info created by the Centre for Disease Prevention and Control (CDC). Epi Info™ is a trademark of CDC. Epi Info™ programs are provided in the public domain to promote public health. Programs might be freely translated, copied, or distributed. No warranty is made or implied for use of the software for any particular purpose.\n Applied Epi Incorporated, 2022 This work is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nPlease email contact@appliedepi.org with questions about the use of these materials for academic courses and epidemiologist training programs."
  },
  {
    "objectID": "pages/oswego.es.html#feedback-suggestions",
    "href": "pages/oswego.es.html#feedback-suggestions",
    "title": "Oswego (ES)",
    "section": "Feedback & suggestions",
    "text": "Feedback & suggestions\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org"
  },
  {
    "objectID": "pages/oswego.es.html#version-1",
    "href": "pages/oswego.es.html#version-1",
    "title": "Oswego (ES)",
    "section": "Version 1",
    "text": "Version 1\n16 November 2023"
  },
  {
    "objectID": "pages/oswego.es.html#disclaimer",
    "href": "pages/oswego.es.html#disclaimer",
    "title": "Oswego (ES)",
    "section": "Disclaimer",
    "text": "Disclaimer\nEsto es un estudio de caso diseñado por el Centre for Disease Prevention and Control (CDC) como tutorial de Epi Info. Puede consultar más detalles en este Enlace"
  },
  {
    "objectID": "pages/oswego.es.html#revisions",
    "href": "pages/oswego.es.html#revisions",
    "title": "Oswego (ES)",
    "section": "Revisions",
    "text": "Revisions\n\n\n\nDate\nChanges made\nVersion\n\n\n\n\n16 November\nAdapted to template\n1"
  },
  {
    "objectID": "pages/oswego.es.html#guía",
    "href": "pages/oswego.es.html#guía",
    "title": "Oswego (ES)",
    "section": "Guía",
    "text": "Guía"
  },
  {
    "objectID": "pages/oswego.es.html#objetivos-de-este-estudio-de-caso",
    "href": "pages/oswego.es.html#objetivos-de-este-estudio-de-caso",
    "title": "Oswego (ES)",
    "section": "Objetivos de este estudio de caso",
    "text": "Objetivos de este estudio de caso\nLos objetivos de este estudio de caso son:\n\nEntender los diferentes pasos en la investigación de un brote de casos de enfermedad gastrointestinal\nAdquirir confianza en el manejo de datos de un listado nominal con el software estadístico R\nAdquirir experience en el análisis descriptivo en R, particularmente curvas epidémicas\nAdquirir experience construyendo tablas 2x2 con exposición y desenlace que nos permitan calcular medidas de asociación\nAplicar los conocimientos adquiridos a posibles actividades de control y prevención de brotes infecciosos de origen alimentario\n\n\nConocimientos previos asumidos\nEn este estudio de caso se asume un conocimiento básico de los principios fundamental de la investigación epidemiológica de brotes gastrointestinales. Se asume también un conocimiento básico de R.\n\n\nPreparation for the case study\nAntes de iniciar este estudio de caso, le aconsejamos que:\n\nDescargue en su computadora la carpeta “oswego_cs_es” y que extraiga todos sus componentes, preferibilmente en su escritorio o en un lugar de fácil acceso. Evite extraerlo en servicios de nube o “drives”\n\nDentro de la carpeta, encontrará un proyecto de R llamado “oswego_cs”. Es un archivo de tipo “R project” y debe siempre asegurarse que está trabajando en RStudio desde el proyecto. La forma más fácil es que habra RStudio cada vez a través de abrir este archivo.\nDentro de la carpeta “oswego_cs_es” encontrará una subcarpeta llamada data en el que encontrará todos los datos necesarios para realizar el análisis en un file llamado “oswego.xlsx”.\nDeberá crear un script dentro de la carpeta scripts en el que usted escribe el código para el análisis. Puede utilizar el script que ya está presente llamado “01_oswego_sol” que contiene el código de análisis si se encuentra atascado o si quiere comparar el código que usted realiza con la solución.\nEn la subcarpeta outputs encontrará todos los gráficos y tablas generadas en el anlálisis.\n\n\nPrimera parte - AntecedentesSegunda parte - El EventoTercera parte - AnálisisCuarta Parte - Conclusión\n\n\n\n\n\n\n\nEl 19 de abril de 1940, el oficial de salud local en el pueblo de Lycoming, condado de Oswego, Nueva York, informó de la ocurrencia de un brote de enfermedad gastrointestinal al Distrito de Salud Oficial en Siracusa. Dr. A. M. Rubin, epidemiólogo en formación, fue asignado para investigar lo ocurrido.\nCuando el Dr. Rubin llegó al campo, determinó a través del oficial de salud que todas las personas que enfermaron había asistido a una cena en la iglesia celebrada la noche anterior, 18 de abril. Otra información importante fue que los familiares que no asistieron a la cena, no enfermaron.\nEn consecuencia, el Dr. Rubin centró la investigación sobre lo ocurrido en la cena. Pudo completar 75 entrevistas de las 80 personas conocidas que asistieron a la cena, recopilando información sobre los ocurrencia y tiempo de aparición de los síntomas, y alimentos consumidos. de las 75 personas entrevistados, 46 personas presentaron síntomas de enfermedad gastrointestinal.\n\nPregunta 1\n\n¿Ante que tipo de situación está presente el Dr Rubin?\n\n Una epidemia Una serie de casos Un brote No se puede establecer\n\n\n\n\n\n Click para leer una pista\n\n\nPuede utilizar este glosario preparado por el Gobierno de México para encontrar la definición que se ajusta más a la situación descrita Enlace \n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nNumero 1 - No es la respuesta correcta; revisa el concepto de epidemia, tiene que ver con la cantidad de personas afectadas.\nNumero 2 - Es posible, pero también debes tomar en cuenta otros factores, como el hecho de que los casos tienen una relación epidemiológica entre ellos.\nNumero 4 - No te preocupes, en este tutorial vas a poder aprender los pasos del trabajo de campo.\n\n\n\n\nPregunta 2\n\nLos pasos de una investigación de brote son:\n\n Determinar la existencia del brote, análisis descriptivo, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Una serie de casos Determinar la existencia del brote, confirmar diagnóstico, contar casos, análisis descriptivo, determinar quién está a riesgo de enfermar, desarrollar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Determinar la existencia del brote, confirmar el diagnóstico, controlar brote, comunicar brote, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final\n\n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\n\nDescripción clínicade los casos\nEl inicio de la enfermedad en todos los casos fue agudo, caracterizada principalmente por náuseas, vómitos, diarrea y dolor abdominal. Ninguno de los enfermos personas reportaron tener un nivel elevado temperatura; todos se recuperaron dentro de las 24 a 30 horas.\nAproximadamente el 20% de los enfermos que visitaron al médico no se les realizó examen de muestras fecales para el examen bacteriológico.\n\nPregunta 3\nEnumere una de las grandes categorías de agentes causales de enfermedades que se deben considerar en el diagnóstico diferencial de un brote de enfermedad gastrointestinal como el de Oswego:\n\n\n\n\nClick para ver la explicación de la solución (¡Inténtelo usted primero!)\n\n\n\nBacterias\nVirus\nParásitos\nToxinas\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\nLos investigadores en Oswego, desconocen el agente causal, pero sospechan de que la génesis de este brote fue través de los alimentos como vehiculo de transmisión entre los afectados.\n\n\nPregunta 4\nEn lenguaje epidemiológico, ¿Qué es un vehículo? ¿Qué es un vector? ¿Cuáles son otros modos?\nPiense en estos conceptos y cuando esté listo, vea la solución propuesta\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEn la jerga epidemiológica, un ‘vehículo’ es un objeto o sustancia inanimada que puede transportar un patógeno y transmitirlo a un huésped susceptible. Ejemplos de vehículos incluyen alimentos o agua contaminados, fómites (objetos inanimados como pomos de puertas o ropa) o partículas transportadas por el aire. En este contexto, un vehículo no es un modo de transporte, sino más bien un medio de transmisión de un agente infeccioso. Por otro lado, un ‘vector’ es un objeto animado, generalmente un artrópodo como un mosquito, una garrapata o una pulga, que puede transportar un agente infeccioso desde un huésped infectado a un huésped susceptible. El vector puede transmitir el patógeno directamente a través de su picadura o indirectamente al depositar el patógeno en una superficie o en una fuente de alimentos o agua. Los otros modos de transmisión de agentes infecciosos incluyen el contacto persona a persona, ya sea directamente a través del contacto físico, como el tacto o el beso, o indirectamente a través de gotas generadas durante la tos o el estornudo, o mediante transmisión aérea en espacios cerrados. Además, algunos agentes infecciosos pueden ser transmitidos a través del contacto sexual, la transmisión perinatal de la madre al hijo o mediante la exposición a fluidos corporales, como la sangre o el semen. Los factores ambientales, como la mala higiene, el hacinamiento o la exposición a animales, también pueden desempeñar un papel en la transmisión de ciertos agentes infecciosos.\n\n\n\nEl Dr. Rubin decidió administrar un cuestionario a los participantes de la cena de la iglesia para averiguar qué alimento podía estar asociado al desarrollo de los síntomas\n\n\nPregunta 5\nSi fuese usted el que administra el cuestionario, ¿qué información recopilaría? Agrupa la información en categorías. Una vez que haya escrito sus categorías, puede ver abajo la solución.\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstos son algunos de los campos que normalmente se includirían en un cuestionario en un brote similar:\n\nInformación demográfica\nInformaión clínica\nDatos de laboratorio si disponibles\nFactores de riesgo (exposición): Alimentos y bebidas ingeridas durante la cena\n\n\n\n\nAdemás de decidir la información que quería recolectar, el Dr. Rubin decidió recolectó los datos entrevistados a través de un listado nominal.\n\n\nPregunta 6\n\n¿En que NO nos ayuda un listado nominal?\n\n Organizar los datos y describirlos por tiempo, lugar y persona Clasificar a los individuos como casos, no casos y sospechosos A diagnosticar a los pacientes A manejar un documento dinámico que se puede actualizar constantemente\n\n\nPor favor, continue el caso entrando en la pestaña “Segunda parte - El Evento” en la parte superior\n\n\n\n\n\nDescripción de la Cena\nLa investigación del Dr. Rubin también implicó averiguar más detalles sobre la cena. Después de hablar con los organizadores, descubrió que la cena se celebró en el sótano del iglesia del pueblo. Los alimentos fueron aportados por numerosos miembros de la congregación. La cena comenzaba a las 6:00 p.m. y continuó hasta 11.00 pm.\nLa comida estaba esparcida sobre una mesa y fue consumida durante un período de varias horas. Los datos sobre el inicio de la enfermedad y los alimentos consumidos por cada una de las 75 personas entrevistados se proporcionan en la listado adjunto.\nLa hora aproximada de participación en el evento solo se recolectó aproximadamente la mitad de las personas que tuvo una enfermedad gastrointestinal.\n\nPregunta 7\n¿Cuál es el valor de una curva epidémica en la investigación de un brote?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son algunos de los usos de las curvas epidémicas cuando se investigan brotes:\n\nNos permite ver la evolución en el tiempo de un evento forma rápida\nAporta información para tomar decisiones para medidas de control\nAyuda a revelar patrones y tendencias sobre un evento\n\n\n\n\n\n\nPregunta 8\n¿Qué nos dice el siguiente gráfico?\n\n\n\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nLa curva epidémica nos dice que:\n\nTodos los casos ocurrieron antes de las 10am del día siguiente (19 de abril)\nDesde la 11pm del 18 a las 3am del 19 de abril ocurrieron la mayoría de los casos\nNos muestra la magnitud del evento y como se propaga, así como ver valores extremos\n\n\n\n\n\n\nPregunta 9\n¿Hay algún caso en el que los tiempos de inicio no coincidan con los generales? ¿experiencia? ¿Cómo podrían explicarse?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nConsidere que: - Hay casos que la hora de inicio de signos y sintomas fueron antes de la cena, pudo ser que se contaminara antes durante los preparativos - Un caso ocurrió 17 horas después de la cena, posiblemente es alguien que comió más tarde o que la infomación es incorrecta (otra enfermedad parecida) o un caso secundario\n\n\n\n\n\n\nListado nominal de individuos del brote de gastroenteritis, Oswego, Nueva York, 1940\n\n\n\n\n\n\n\nPregunta 10\n¿Cómo podrían presentarse mejor los datos en el listado nominal de participantes?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nDos ideas, aunque puede haber más son:\n\nLos datos pudieron ser separados de acuerdo al estatus de enfermedad y tiempo de inicio de sintomas\nSi se hubiese usado el formato del en tiempo militar, (ej. 00:00 o 14:00)\n\n\n\n\n\n\nVersión digital\n\n\n\n\n\nApoyo en análisis de los datos\n\n\n\nAhora vamos a comenzar con uno de los pasos más importantes en la investigación de brote, el análisis de los datos donde a través de este vamos a determinar cual o cuales son las posibles causas del brote, medidas a tomar entre otros pasos. ¡También vamos a usar un poco de R para ayudar con este proceso de análisis!.\n\nSiempre que sea posible, utilizando el listado nominal actualizado, calcule los períodos de incubación e ilustre su distribución con un gráfico apropiado.\n \nPara contestar esta pregunta, vamos hacer los siguientes pasos usando R:\n \n\nen Rstudio, crea un nuevo script para cargar los datos (cargar el archivo “oswego.xlsx”, que está en la carpeta data) y explore las dos columnas que contienen la información necesaria para calcular los períodos de incubación: TimeSupper (hora de la cena) y DateOnset (Fecha inizio síntomas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Cargar los paquetes necesarios\npacman::p_load(rio,\n               here,\n               tidyverse, \n               epitools, \n               lubridate,\n               DT)\n\n#Importar los datos\nlistado &lt;- import(\"../case_studies_to_translate/ESP/oswego_cs_es/data/oswego.xlsx\")\n\n#Explorar las variables de la fecha y hora del almuerzo\nhead(oswego_db$TimeSupper)\n\n[1] NA                        NA                       \n[3] NA                        \"1940-04-18 22:00:00 UTC\"\n[5] \"1940-04-18 19:30:00 UTC\" \"1940-04-18 19:30:00 UTC\"\n\nhead(oswego_db$DateOnset)\n\n[1] \"1940-04-18 23:00:00 UTC\" NA                       \n[3] \"1940-04-18 22:30:00 UTC\" \"1940-04-19 01:00:00 UTC\"\n[5] \"1940-04-19 02:30:00 UTC\" \"1940-04-18 23:30:00 UTC\"\n\n\n\n\n\nPara referencia sobre como importar archivos, ver el capítulo 7 del libro de R para epidemiologos\n \n\nEl listado cargado ya tiene el formato correcto de la fecha y hora de almuerzo y la fecha y hora de inicio de síntomas ahora intente crear un código para crear una nueva variable con los periodos de incubación para cada caso. (recuerda que con las variables de tiempo en R se pueden hacer operaciones matemáticas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Crear la variable del período de incubación\n# puedes copiar este código en rstudio en tu editor de códigos\n#las funciones \"interval\" y \"dhours()\" son las que usaremos para calcular la diferencia en hora\n\nlistado_incubacion &lt;- listado %&gt;%\n  \n  mutate(incubacion=interval(TimeSupper, DateOnset) / dhours(1)) \n\n\n\n\nPara más detalles de como trabajar con fechas, ver el capítulo 9 del libro de R para epidemiologos\n \n\nAhora intente hacer un gráfico de barras con el período de incubación usando ggplot para visualizar la distribución. Puede encontrar pistas sobre como hacer un gráfico de barras en la sección dedicada del EpiRhandbook\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Hacer un gráfico de barra de los periodos de incubación calculados\n# puedes copiar este código en rstudio en tu editor de códigos\n\nlistado_incubacion %&gt;% \n  \n  ggplot(aes(x=incubacion))+\n  \n  geom_bar()+\n  \n  labs(title=\"Casos de enfermedad gastrointestinal por período de incubación en horas\",\n       subtitle = \"Oswego, NY, 18-19 de abril, 1940\",\n       x=\"Período de incubación (Horas)\",\n       y=\"n de casos\")+\n  \n  theme_minimal()\n\nWarning: Removed 53 rows containing non-finite values (`stat_count()`).\n\n\n\n\n#Si quieres asignar este gráfico a un objeto, solo tienes que en la primera línea del código \n#usar un nombre (como grafico1) y escribir el signo de asignación (&lt;-)\n\n\n\n\nPara más detalles de como trabajar con gráficos en general, ver el capítulo 30 del libro de R para epidemiologos\n \n\n\nPregunta 10: ¿Cuantos casos tenian el dato de incubación?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n22\nPara saberlo, puedes verificar viendo el listado directamente o con el siguiente código\n\n\n\n\n\nPregunta 11: Determine el rango y la mediana del período de incubación\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n Para ejecutar esto en R hay varias formas, usando ya sea las funciones base o de otros paquetes. Aquí tiene un ejemplo usando el paquete {dplyr} que viene integrado en el paquete {tidyverse}\n\n# Calcular el rango, la mediana y el periodo de incubacion\n# puedes copiar este código en rstudio en tu editor de códigos\n\nresumen_estadistico &lt;- listado_incubacion %&gt;% \n  filter(!is.na(incubacion)) %&gt;% \n  reframe(mediana=median(incubacion),\n          min=min(incubacion),\n          max=max(incubacion),\n          rango=max-min)\n\nresumen_estadistico\n\n  mediana min max rango\n1       4   3   7     4\n\n\nLa mediana del periodo de incubación fue 4 horas, así como el rango del periodo del periodo de incubación también fue de 4 horas\n\n\n\n\n\nPregunta 12: ¿Cómo ayuda la información sobre el período de incubación y los datos sobre síntomas a establecer diagnósticos diferenciales de la enfermedad? (Si es necesario, consulte adjunto Compendio de enfermedades gastrointestinales agudas transmitidas por los alimentos)\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nNos ayuda porque:\n\nCada enfermedad transmitida por los alimentos tiene un período de incubación característico, síntomas específicos y alimentos con los que es más probable que esté asociada\nEl período de incubación observado es demasiado largo para los metales pesados y demasiado corto para los agentes virales y el botulismo\nLa intoxicación alimentaria estafilocócica tiene un período de incubación promedio de 2 a 4 horas\nEstos datos son insuficientes para saber cual puede ser el agente causal \n\n\n\n\n\nPregunta 13: Usando los datos del listado, complete la tabla a continuación. ¿Cúal o cuales alimentos pudieron ser el vehículo más probable de infección?\n \nPara contestar esta pregunta, intente hacer los siguientes pasos usando R con el mismo script que creaste anteriormente:\n\nCree un objeto “data.frame” con el resumen de los alimentos ingeridos por los que enfermaron y otro con el resumen de los alimentos ingeridos por los que no enfermaron. Recodifique las variables para asignar el valor de 1 si fue consumido y 0 para no consumido.\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Ve copiando el codigo a rstudio (este ejercicio es un poco largo)\n\npacman::p_load(janitor, gtsummary)\n\n\n#Comieron\n#hacer un dataframe con un resumen de los alimentos por los que enfermaron\ntotal_por_alimentos_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"enfermo\") %&gt;% \nslice_tail()\n\n#hacer un dataframe con un resumen de los alimentos por los que no enfermaron\ntotal_por_alimentos_no_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no_enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"no_enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"no_enfermo\") %&gt;% \nslice_tail()\n\n\n\n\nUna los dos objectos “data.frame” y calcule la proporción de personas enfermas y no enfermas que consumieron cada alimento\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que consumieron\ntabla_maestra_a &lt;- bind_rows(total_por_alimentos_casos_a,\n                           total_por_alimentos_no_casos_a) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que no consumieron \ntabla_maestra_b &lt;- bind_rows(total_por_alimentos_casos_b,\n                           total_por_alimentos_no_casos_b) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\ntabla_final &lt;- tabla_maestra_a %&gt;% \n  left_join(tabla_maestra_b,suffix = c(\"_consumieron\", \"_no_consumieron\"), by=\"alimentos\") %&gt;% \n  mutate(tasa_ataque=ptc_enfermo_consumieron/ptc_enfermo_no_consumieron)\n\nView(tabla_final)\n\n\n\n\nCalcule una medida de asociación para estimar qué alimento se asoció en mayor medida a enfermar\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#otra forma.. para obtener los OR de cada alimento\n\n#crear un modelo de regresión logistica\nmodel &lt;- glm(data=listado, enfermo~m_jamon_horneado+\n            m_espinaca+m_pure_papa+m_ensa_repollo+\n            m_gelatina+m_rollos+m_pan+m_lehe+m_cafe+m_agua+\n            m_bizcocho+m_hel_vainilla+m_hel_chocolate+m_ens_fruta,\n            family=binomial())\n#Luego una tabla\ntbl_regression(model, exponentiate = TRUE)\n\ntest &lt;- listado %&gt;% \n  select(enfermo, starts_with(\"m_\")) %&gt;% \n  tbl_summary(by=enfermo)\n\n\n\n\nPara más detalles de como trabajar con transformación de datos y tablas, ver el capítulo 17 del libro de R para epidemiologos\n\n\nPregunta 15: Resuma las investigaciones adicionales que deben llevarse a cabo.\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales investigaciones adicionales que deben llevarse a cabo:\n\nRevisión detallada de la fuente, los ingredientes, la preparación y el almacenamiento de los alimentos incriminados\nIntentar explicar los casos con tiempo de inicio atípico\nSe podría hacer un examen de laboratorio\nDeterminar si se produjo una propagación secundaria en los miembros de la familia\nCálculos adicionales (p. ej., tasas de ataque específicas por edad o género) \n\n\n\n\n\nPregunta 16: ¿Qué medidas de control sugeriría?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales medidas de control y prevención en un brote de estas características:\n\nEvite el consumo del helado de vainilla restante\nPrevenga la recurrencia de eventos similares en el futuro educando a los manipuladores de alimentos\nSe podría hacer un examen de laboratorio\nDeterminar si se trata de un producto comercial\nEliminó cualquier fuente contaminada de alimentos\n\n\n\n\n\n\nPregunta 17: ¿Por qué fue importante trabajar en este brote?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nTrabajar en este brote ayudó a:\n\nDescartar la contaminación de un producto comercial. Si se trata de un producto comercial, la intervención inmediata puede prevenir un número considerable de casos adicionales\nPrevenir futuros brotes mediante la identificación de manipuladores de alimentos infectados, lagunas específicas en la educación o técnicas de manipulación de alimentos\nLos funcionarios de salud pública deben responder a tales problemas de manera oportuna para mantener una relación de cooperación con los departamentos de salud locales, los médicos privados y la comunidad\nUna explicación epidemiológica de la causa del brote puede disipar los temores y preocupaciones de la comunidad\nLa investigación del brote puede brindar oportunidades para que los investigadores respondan preguntas sobre el agente, el huésped, el entorno, el período de incubación, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLo siguiente se cita textualmente del informe preparado por el Dr. Rubin:\nEl helado fue preparado por el Petrie hermanas de la siguiente manera: En la tarde del 17 de abril la leche cruda de la La granja Petrie en Lycoming se desbordó al baño maría se le agrega azúcar y huevos y un poco de harina para darle cuerpo a la mezcla. El se prepararon helado de chocolate y vainilla por separado.\nEl chocolate de Hershey era necesariamente añadido a la mezcla de chocolate. A las 6 pm. los dos las mezclas se llevaban en recipientes tapados al sótano de la iglesia y se dejó reposar durante la noche. Presuntamente no fueron tocados por nadie. durante este período.\nEn la mañana del 18 de abril, el Sr. Coe agregó cinco onzas de vainilla y dos latas de leche condensada a la mezcla de vainilla y tres onzas de vainilla y una lata de leche condensada a la mezcla de chocolate. Luego el helado de vainilla se transfirió a un lata de congelación y se coloca en un congelador eléctrico durante 20 minutos, después de lo cual el helado de vainilla se sacó de la lata del congelador y se envasó en otra lata que había sido previamente lavado con agua hirviendo. Entonces el chocolate la mezcla se puso en la lata del congelador que había sido se enjuaga con agua del grifo y se deja congelardurante 20 minutos.”\nAl concluir esto, ambos las latas se taparon y se colocaron en grandes recipientes de madera recipientes llenos de hielo. Como señaló, el helado de chocolate permaneció en el una lata de congelador.\nTodos los manipuladores del helado fueron examinados. Sin lesiones externas ni respiratorias altas se notaron infecciones. Cultivos de nariz y garganta fueron tomados de dos individuos que prepararon el helado.\nLos exámenes bacteriológicos fueron hechos por el División de Laboratorios e Investigación, Albany, en ambos helados. Su informe es el siguiente:\n‘Un gran número de Staphylococcus aureus y albus se encontraron en la muestra de hielo de vainilla crema. Sólo unos pocos estafilococos fueron demostrado en el helado de chocolate.’\nInforme de los cultivos de nariz y garganta de Los Petries que prepararon el helado decía lo siguiente:\nPresencia de Staphylococcus aureus y hemolítica del cultivo nasal y Staphylococcus albus del cultivo faríngeo de Gracia Petrie. Tambien Staphylococcus albus del cultivo de la nariz de Marian Petrie. Los estreptococos hemolíticos no eran del tipo generalmente asociado con infecciones en el hombre.\nDiscusión sobre la fuente: la fuente de contaminación bacteriana del helado de vainilla no está claro. Cualquiera que sea el método de la introducción de los estafilococos, parece razonable suponer que debe haber ocurrido entre la tarde del 17 de abril y la mañana del 18 de abril. Sin motivo de contaminación Se conoce la peculiaridad del helado de vainilla. “Al dispensar los helados, la misma cuchara se utilizó. Por lo tanto, no es improbable suponer que alguna contaminación al helado de chocolate crema ocurrió de esta manera. Esto parecería ser la explicación más plausible para la enfermedad en los tres individuos que no comieron el helado de vainilla.\nMedidas de Control: El 19 de mayo, todo el helado restantes fue condenado. Todos los demás alimentos en el la cena de la iglesia había sido consumida.\nConclusiones: Un brote de gastroenteritis ocurrió después de una cena en la iglesia en Lycoming. La causa del brote fue helado de vainilla por contaminado. El método de contaminación de helado no se entiende claramente.\nSi el estafilococo dio positivo de la nariz y la garganta de los cultivos realizados en la familia Petrie haba todo lo que tenga que ver con la contaminación es un asunto por nexo epidemiológico.\nNota: El paciente #52 era un niño que mientras viendo el procedimiento de congelación se le dio una plato de helado de vainilla a las 11:00 am en abril 18."
  }
]