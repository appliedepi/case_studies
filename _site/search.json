[
  {
    "objectID": "pages/under_construction.fr.html",
    "href": "pages/under_construction.fr.html",
    "title": "Page en construction",
    "section": "",
    "text": "Page en construction\n\n\n\n\n\n\nEN CONSTRUCTION\n\n\n\nCette page est en cours de développement. Le contenu et l’URL vont changer.\n\n\nPour des instructions sur la façon d’utiliser nos études de cas, consultez notre How-to Guide. Nous accueillons vos retours et suggestions à l’adresse contact@appliedepi.org. Vous pouvez également discuter de l’étude de cas ou des concepts connexes sur la Applied Epi Community."
  },
  {
    "objectID": "pages/oswego.es.html",
    "href": "pages/oswego.es.html",
    "title": "Oswego (ES)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\n\n\n\nName\nOswego\n\n\nTool\nR\n\n\nLanguage\nSpanish/Español\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nGastrointestinal\n\n\nKeywords\nGastrointestinal;Outbreak investigation\n\n\nTechnical complexity\nIntermidiate\n\n\nMethodolocial complexity\nIntermidiate\n\n\n\nAuthorship\nOriginal authors: Centre for Disease Prevention and Control (CDC)\nData source: Epi Info, version 3.5.4 (CDC)\nAdapted to R by: Leonel Lerebours Nadal y Alberto Mateo Urdiales\n\n\n\n\n\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\nYou will see these icons throughout the exercises:\n\n\n\n\n\n\n\nIcon\nMeaning\n\n\n\n\n\nObserve\n\n\n\nAlert!\n\n\n\nAn informative note\n\n\n\nTime for you to code!\n\n\n\nChange to another window\n\n\n\nRemember this for later\n\n\n\n\n\n\n\nThis case study has been adapted from an existing tutorial on Epi Info created by the Centre for Disease Prevention and Control (CDC). Epi Info™ is a trademark of CDC. Epi Info™ programs are provided in the public domain to promote public health. Programs might be freely translated, copied, or distributed. No warranty is made or implied for use of the software for any particular purpose.\n Applied Epi Incorporated, 2022 This work is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nPlease email contact@appliedepi.org with questions about the use of these materials for academic courses and epidemiologist training programs.\n\n\n\n\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\n16 November 2023\n\n\n\nEsto es un estudio de caso diseñado por el Centre for Disease Prevention and Control (CDC) como tutorial de Epi Info. Puede consultar más detalles en este Enlace\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\n\n\n\n\n16 November\nAdapted to template\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLos objetivos de este estudio de caso son:\n\nEntender los diferentes pasos en la investigación de un brote de casos de enfermedad gastrointestinal\nAdquirir confianza en el manejo de datos de un listado nominal con el software estadístico R\nAdquirir experience en el análisis descriptivo en R, particularmente curvas epidémicas\nAdquirir experience construyendo tablas 2x2 con exposición y desenlace que nos permitan calcular medidas de asociación\nAplicar los conocimientos adquiridos a posibles actividades de control y prevención de brotes infecciosos de origen alimentario\n\n\n\nEn este estudio de caso se asume un conocimiento básico de los principios fundamental de la investigación epidemiológica de brotes gastrointestinales. Se asume también un conocimiento básico de R.\n\n\n\nAntes de iniciar este estudio de caso, le aconsejamos que:\n\nDescargue en su computadora la carpeta “oswego_cs_es” y que extraiga todos sus componentes, preferibilmente en su escritorio o en un lugar de fácil acceso. Evite extraerlo en servicios de nube o “drives”\n\nDentro de la carpeta, encontrará un proyecto de R llamado “oswego_cs”. Es un archivo de tipo “R project” y debe siempre asegurarse que está trabajando en RStudio desde el proyecto. La forma más fácil es que habra RStudio cada vez a través de abrir este archivo.\nDentro de la carpeta “oswego_cs_es” encontrará una subcarpeta llamada data en el que encontrará todos los datos necesarios para realizar el análisis en un file llamado “oswego.xlsx”.\nDeberá crear un script dentro de la carpeta scripts en el que usted escribe el código para el análisis. Puede utilizar el script que ya está presente llamado “01_oswego_sol” que contiene el código de análisis si se encuentra atascado o si quiere comparar el código que usted realiza con la solución.\nEn la subcarpeta outputs encontrará todos los gráficos y tablas generadas en el anlálisis.\n\n\nPrimera parte - AntecedentesSegunda parte - El EventoTercera parte - AnálisisCuarta Parte - Conclusión\n\n\n\n\n\n\n\nEl 19 de abril de 1940, el oficial de salud local en el pueblo de Lycoming, condado de Oswego, Nueva York, informó de la ocurrencia de un brote de enfermedad gastrointestinal al Distrito de Salud Oficial en Siracusa. Dr. A. M. Rubin, epidemiólogo en formación, fue asignado para investigar lo ocurrido.\nCuando el Dr. Rubin llegó al campo, determinó a través del oficial de salud que todas las personas que enfermaron había asistido a una cena en la iglesia celebrada la noche anterior, 18 de abril. Otra información importante fue que los familiares que no asistieron a la cena, no enfermaron.\nEn consecuencia, el Dr. Rubin centró la investigación sobre lo ocurrido en la cena. Pudo completar 75 entrevistas de las 80 personas conocidas que asistieron a la cena, recopilando información sobre los ocurrencia y tiempo de aparición de los síntomas, y alimentos consumidos. de las 75 personas entrevistados, 46 personas presentaron síntomas de enfermedad gastrointestinal.\n\n\n\n¿Ante que tipo de situación está presente el Dr Rubin?\n\n Una epidemia Una serie de casos Un brote No se puede establecer\n\n\n\n\n\n Click para leer una pista\n\n\nPuede utilizar este glosario preparado por el Gobierno de México para encontrar la definición que se ajusta más a la situación descrita Enlace \n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nNumero 1 - No es la respuesta correcta; revisa el concepto de epidemia, tiene que ver con la cantidad de personas afectadas.\nNumero 2 - Es posible, pero también debes tomar en cuenta otros factores, como el hecho de que los casos tienen una relación epidemiológica entre ellos.\nNumero 4 - No te preocupes, en este tutorial vas a poder aprender los pasos del trabajo de campo.\n\n\n\n\n\n\nLos pasos de una investigación de brote son:\n\n Determinar la existencia del brote, análisis descriptivo, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Una serie de casos Determinar la existencia del brote, confirmar diagnóstico, contar casos, análisis descriptivo, determinar quién está a riesgo de enfermar, desarrollar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Determinar la existencia del brote, confirmar el diagnóstico, controlar brote, comunicar brote, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final\n\n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\n\n\nEl inicio de la enfermedad en todos los casos fue agudo, caracterizada principalmente por náuseas, vómitos, diarrea y dolor abdominal. Ninguno de los enfermos personas reportaron tener un nivel elevado temperatura; todos se recuperaron dentro de las 24 a 30 horas.\nAproximadamente el 20% de los enfermos que visitaron al médico no se les realizó examen de muestras fecales para el examen bacteriológico.\n\n\nEnumere una de las grandes categorías de agentes causales de enfermedades que se deben considerar en el diagnóstico diferencial de un brote de enfermedad gastrointestinal como el de Oswego:\n\n\n\n\nClick para ver la explicación de la solución (¡Inténtelo usted primero!)\n\n\n\nBacterias\nVirus\nParásitos\nToxinas\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\nLos investigadores en Oswego, desconocen el agente causal, pero sospechan de que la génesis de este brote fue través de los alimentos como vehiculo de transmisión entre los afectados.\n\n\n\nEn lenguaje epidemiológico, ¿Qué es un vehículo? ¿Qué es un vector? ¿Cuáles son otros modos?\nPiense en estos conceptos y cuando esté listo, vea la solución propuesta\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEn la jerga epidemiológica, un ‘vehículo’ es un objeto o sustancia inanimada que puede transportar un patógeno y transmitirlo a un huésped susceptible. Ejemplos de vehículos incluyen alimentos o agua contaminados, fómites (objetos inanimados como pomos de puertas o ropa) o partículas transportadas por el aire. En este contexto, un vehículo no es un modo de transporte, sino más bien un medio de transmisión de un agente infeccioso. Por otro lado, un ‘vector’ es un objeto animado, generalmente un artrópodo como un mosquito, una garrapata o una pulga, que puede transportar un agente infeccioso desde un huésped infectado a un huésped susceptible. El vector puede transmitir el patógeno directamente a través de su picadura o indirectamente al depositar el patógeno en una superficie o en una fuente de alimentos o agua. Los otros modos de transmisión de agentes infecciosos incluyen el contacto persona a persona, ya sea directamente a través del contacto físico, como el tacto o el beso, o indirectamente a través de gotas generadas durante la tos o el estornudo, o mediante transmisión aérea en espacios cerrados. Además, algunos agentes infecciosos pueden ser transmitidos a través del contacto sexual, la transmisión perinatal de la madre al hijo o mediante la exposición a fluidos corporales, como la sangre o el semen. Los factores ambientales, como la mala higiene, el hacinamiento o la exposición a animales, también pueden desempeñar un papel en la transmisión de ciertos agentes infecciosos.\n\n\n\nEl Dr. Rubin decidió administrar un cuestionario a los participantes de la cena de la iglesia para averiguar qué alimento podía estar asociado al desarrollo de los síntomas\n\n\n\nSi fuese usted el que administra el cuestionario, ¿qué información recopilaría? Agrupa la información en categorías. Una vez que haya escrito sus categorías, puede ver abajo la solución.\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstos son algunos de los campos que normalmente se includirían en un cuestionario en un brote similar:\n\nInformación demográfica\nInformaión clínica\nDatos de laboratorio si disponibles\nFactores de riesgo (exposición): Alimentos y bebidas ingeridas durante la cena\n\n\n\n\nAdemás de decidir la información que quería recolectar, el Dr. Rubin decidió recolectó los datos entrevistados a través de un listado nominal.\n\n\n\n\n¿En que NO nos ayuda un listado nominal?\n\n Organizar los datos y describirlos por tiempo, lugar y persona Clasificar a los individuos como casos, no casos y sospechosos A diagnosticar a los pacientes A manejar un documento dinámico que se puede actualizar constantemente\n\n\nPor favor, continue el caso entrando en la pestaña “Segunda parte - El Evento” en la parte superior\n\n\n\n\n\n\nLa investigación del Dr. Rubin también implicó averiguar más detalles sobre la cena. Después de hablar con los organizadores, descubrió que la cena se celebró en el sótano del iglesia del pueblo. Los alimentos fueron aportados por numerosos miembros de la congregación. La cena comenzaba a las 6:00 p.m. y continuó hasta 11.00 pm.\nLa comida estaba esparcida sobre una mesa y fue consumida durante un período de varias horas. Los datos sobre el inicio de la enfermedad y los alimentos consumidos por cada una de las 75 personas entrevistados se proporcionan en la listado adjunto.\nLa hora aproximada de participación en el evento solo se recolectó aproximadamente la mitad de las personas que tuvo una enfermedad gastrointestinal.\n\n\n¿Cuál es el valor de una curva epidémica en la investigación de un brote?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son algunos de los usos de las curvas epidémicas cuando se investigan brotes:\n\nNos permite ver la evolución en el tiempo de un evento forma rápida\nAporta información para tomar decisiones para medidas de control\nAyuda a revelar patrones y tendencias sobre un evento\n\n\n\n\n\n\n\n¿Qué nos dice el siguiente gráfico?\n\n\n\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nLa curva epidémica nos dice que:\n\nTodos los casos ocurrieron antes de las 10am del día siguiente (19 de abril)\nDesde la 11pm del 18 a las 3am del 19 de abril ocurrieron la mayoría de los casos\nNos muestra la magnitud del evento y como se propaga, así como ver valores extremos\n\n\n\n\n\n\n\n¿Hay algún caso en el que los tiempos de inicio no coincidan con los generales? ¿experiencia? ¿Cómo podrían explicarse?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nConsidere que: - Hay casos que la hora de inicio de signos y sintomas fueron antes de la cena, pudo ser que se contaminara antes durante los preparativos - Un caso ocurrió 17 horas después de la cena, posiblemente es alguien que comió más tarde o que la infomación es incorrecta (otra enfermedad parecida) o un caso secundario\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo podrían presentarse mejor los datos en el listado nominal de participantes?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nDos ideas, aunque puede haber más son:\n\nLos datos pudieron ser separados de acuerdo al estatus de enfermedad y tiempo de inicio de sintomas\nSi se hubiese usado el formato del en tiempo militar, (ej. 00:00 o 14:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora vamos a comenzar con uno de los pasos más importantes en la investigación de brote, el análisis de los datos donde a través de este vamos a determinar cual o cuales son las posibles causas del brote, medidas a tomar entre otros pasos. ¡También vamos a usar un poco de R para ayudar con este proceso de análisis!.\n\n\n \nPara contestar esta pregunta, vamos hacer los siguientes pasos usando R:\n \n\nen Rstudio, crea un nuevo script para cargar los datos (cargar el archivo “oswego.xlsx”, que está en la carpeta data) y explore las dos columnas que contienen la información necesaria para calcular los períodos de incubación: TimeSupper (hora de la cena) y DateOnset (Fecha inizio síntomas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Cargar los paquetes necesarios\npacman::p_load(rio,\n               here,\n               tidyverse, \n               epitools, \n               lubridate,\n               DT)\n\n#Importar los datos\nlistado &lt;- import(\"../case_studies_to_translate/ESP/oswego_cs_es/data/oswego.xlsx\")\n\n#Explorar las variables de la fecha y hora del almuerzo\nhead(oswego_db$TimeSupper)\n\n[1] NA                        NA                       \n[3] NA                        \"1940-04-18 22:00:00 UTC\"\n[5] \"1940-04-18 19:30:00 UTC\" \"1940-04-18 19:30:00 UTC\"\n\nhead(oswego_db$DateOnset)\n\n[1] \"1940-04-18 23:00:00 UTC\" NA                       \n[3] \"1940-04-18 22:30:00 UTC\" \"1940-04-19 01:00:00 UTC\"\n[5] \"1940-04-19 02:30:00 UTC\" \"1940-04-18 23:30:00 UTC\"\n\n\n\n\n\nPara referencia sobre como importar archivos, ver el capítulo 7 del libro de R para epidemiologos\n \n\nEl listado cargado ya tiene el formato correcto de la fecha y hora de almuerzo y la fecha y hora de inicio de síntomas ahora intente crear un código para crear una nueva variable con los periodos de incubación para cada caso. (recuerda que con las variables de tiempo en R se pueden hacer operaciones matemáticas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Crear la variable del período de incubación\n# puedes copiar este código en rstudio en tu editor de códigos\n#las funciones \"interval\" y \"dhours()\" son las que usaremos para calcular la diferencia en hora\n\nlistado_incubacion &lt;- listado %&gt;%\n  \n  mutate(incubacion=interval(TimeSupper, DateOnset) / dhours(1)) \n\n\n\n\nPara más detalles de como trabajar con fechas, ver el capítulo 9 del libro de R para epidemiologos\n \n\nAhora intente hacer un gráfico de barras con el período de incubación usando ggplot para visualizar la distribución. Puede encontrar pistas sobre como hacer un gráfico de barras en la sección dedicada del EpiRhandbook\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Hacer un gráfico de barra de los periodos de incubación calculados\n# puedes copiar este código en rstudio en tu editor de códigos\n\nlistado_incubacion %&gt;% \n  \n  ggplot(aes(x=incubacion))+\n  \n  geom_bar()+\n  \n  labs(title=\"Casos de enfermedad gastrointestinal por período de incubación en horas\",\n       subtitle = \"Oswego, NY, 18-19 de abril, 1940\",\n       x=\"Período de incubación (Horas)\",\n       y=\"n de casos\")+\n  \n  theme_minimal()\n\nWarning: Removed 53 rows containing non-finite values (`stat_count()`).\n\n\n\n\n#Si quieres asignar este gráfico a un objeto, solo tienes que en la primera línea del código \n#usar un nombre (como grafico1) y escribir el signo de asignación (&lt;-)\n\n\n\n\nPara más detalles de como trabajar con gráficos en general, ver el capítulo 30 del libro de R para epidemiologos\n \n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n22\nPara saberlo, puedes verificar viendo el listado directamente o con el siguiente código\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n Para ejecutar esto en R hay varias formas, usando ya sea las funciones base o de otros paquetes. Aquí tiene un ejemplo usando el paquete {dplyr} que viene integrado en el paquete {tidyverse}\n\n# Calcular el rango, la mediana y el periodo de incubacion\n# puedes copiar este código en rstudio en tu editor de códigos\n\nresumen_estadistico &lt;- listado_incubacion %&gt;% \n  filter(!is.na(incubacion)) %&gt;% \n  reframe(mediana=median(incubacion),\n          min=min(incubacion),\n          max=max(incubacion),\n          rango=max-min)\n\nresumen_estadistico\n\n  mediana min max rango\n1       4   3   7     4\n\n\nLa mediana del periodo de incubación fue 4 horas, así como el rango del periodo del periodo de incubación también fue de 4 horas\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nNos ayuda porque:\n\nCada enfermedad transmitida por los alimentos tiene un período de incubación característico, síntomas específicos y alimentos con los que es más probable que esté asociada\nEl período de incubación observado es demasiado largo para los metales pesados y demasiado corto para los agentes virales y el botulismo\nLa intoxicación alimentaria estafilocócica tiene un período de incubación promedio de 2 a 4 horas\nEstos datos son insuficientes para saber cual puede ser el agente causal \n\n\n\n\n\n\n \nPara contestar esta pregunta, intente hacer los siguientes pasos usando R con el mismo script que creaste anteriormente:\n\nCree un objeto “data.frame” con el resumen de los alimentos ingeridos por los que enfermaron y otro con el resumen de los alimentos ingeridos por los que no enfermaron. Recodifique las variables para asignar el valor de 1 si fue consumido y 0 para no consumido.\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Ve copiando el codigo a rstudio (este ejercicio es un poco largo)\n\npacman::p_load(janitor, gtsummary)\n\n\n#Comieron\n#hacer un dataframe con un resumen de los alimentos por los que enfermaron\ntotal_por_alimentos_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"enfermo\") %&gt;% \nslice_tail()\n\n#hacer un dataframe con un resumen de los alimentos por los que no enfermaron\ntotal_por_alimentos_no_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no_enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"no_enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"no_enfermo\") %&gt;% \nslice_tail()\n\n\n\n\nUna los dos objectos “data.frame” y calcule la proporción de personas enfermas y no enfermas que consumieron cada alimento\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que consumieron\ntabla_maestra_a &lt;- bind_rows(total_por_alimentos_casos_a,\n                           total_por_alimentos_no_casos_a) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que no consumieron \ntabla_maestra_b &lt;- bind_rows(total_por_alimentos_casos_b,\n                           total_por_alimentos_no_casos_b) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\ntabla_final &lt;- tabla_maestra_a %&gt;% \n  left_join(tabla_maestra_b,suffix = c(\"_consumieron\", \"_no_consumieron\"), by=\"alimentos\") %&gt;% \n  mutate(tasa_ataque=ptc_enfermo_consumieron/ptc_enfermo_no_consumieron)\n\nView(tabla_final)\n\n\n\n\nCalcule una medida de asociación para estimar qué alimento se asoció en mayor medida a enfermar\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#otra forma.. para obtener los OR de cada alimento\n\n#crear un modelo de regresión logistica\nmodel &lt;- glm(data=listado, enfermo~m_jamon_horneado+\n            m_espinaca+m_pure_papa+m_ensa_repollo+\n            m_gelatina+m_rollos+m_pan+m_lehe+m_cafe+m_agua+\n            m_bizcocho+m_hel_vainilla+m_hel_chocolate+m_ens_fruta,\n            family=binomial())\n#Luego una tabla\ntbl_regression(model, exponentiate = TRUE)\n\ntest &lt;- listado %&gt;% \n  select(enfermo, starts_with(\"m_\")) %&gt;% \n  tbl_summary(by=enfermo)\n\n\n\n\nPara más detalles de como trabajar con transformación de datos y tablas, ver el capítulo 17 del libro de R para epidemiologos\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales investigaciones adicionales que deben llevarse a cabo:\n\nRevisión detallada de la fuente, los ingredientes, la preparación y el almacenamiento de los alimentos incriminados\nIntentar explicar los casos con tiempo de inicio atípico\nSe podría hacer un examen de laboratorio\nDeterminar si se produjo una propagación secundaria en los miembros de la familia\nCálculos adicionales (p. ej., tasas de ataque específicas por edad o género) \n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales medidas de control y prevención en un brote de estas características:\n\nEvite el consumo del helado de vainilla restante\nPrevenga la recurrencia de eventos similares en el futuro educando a los manipuladores de alimentos\nSe podría hacer un examen de laboratorio\nDeterminar si se trata de un producto comercial\nEliminó cualquier fuente contaminada de alimentos\n\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nTrabajar en este brote ayudó a:\n\nDescartar la contaminación de un producto comercial. Si se trata de un producto comercial, la intervención inmediata puede prevenir un número considerable de casos adicionales\nPrevenir futuros brotes mediante la identificación de manipuladores de alimentos infectados, lagunas específicas en la educación o técnicas de manipulación de alimentos\nLos funcionarios de salud pública deben responder a tales problemas de manera oportuna para mantener una relación de cooperación con los departamentos de salud locales, los médicos privados y la comunidad\nUna explicación epidemiológica de la causa del brote puede disipar los temores y preocupaciones de la comunidad\nLa investigación del brote puede brindar oportunidades para que los investigadores respondan preguntas sobre el agente, el huésped, el entorno, el período de incubación, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLo siguiente se cita textualmente del informe preparado por el Dr. Rubin:\nEl helado fue preparado por el Petrie hermanas de la siguiente manera: En la tarde del 17 de abril la leche cruda de la La granja Petrie en Lycoming se desbordó al baño maría se le agrega azúcar y huevos y un poco de harina para darle cuerpo a la mezcla. El se prepararon helado de chocolate y vainilla por separado.\nEl chocolate de Hershey era necesariamente añadido a la mezcla de chocolate. A las 6 pm. los dos las mezclas se llevaban en recipientes tapados al sótano de la iglesia y se dejó reposar durante la noche. Presuntamente no fueron tocados por nadie. durante este período.\nEn la mañana del 18 de abril, el Sr. Coe agregó cinco onzas de vainilla y dos latas de leche condensada a la mezcla de vainilla y tres onzas de vainilla y una lata de leche condensada a la mezcla de chocolate. Luego el helado de vainilla se transfirió a un lata de congelación y se coloca en un congelador eléctrico durante 20 minutos, después de lo cual el helado de vainilla se sacó de la lata del congelador y se envasó en otra lata que había sido previamente lavado con agua hirviendo. Entonces el chocolate la mezcla se puso en la lata del congelador que había sido se enjuaga con agua del grifo y se deja congelardurante 20 minutos.”\nAl concluir esto, ambos las latas se taparon y se colocaron en grandes recipientes de madera recipientes llenos de hielo. Como señaló, el helado de chocolate permaneció en el una lata de congelador.\nTodos los manipuladores del helado fueron examinados. Sin lesiones externas ni respiratorias altas se notaron infecciones. Cultivos de nariz y garganta fueron tomados de dos individuos que prepararon el helado.\nLos exámenes bacteriológicos fueron hechos por el División de Laboratorios e Investigación, Albany, en ambos helados. Su informe es el siguiente:\n‘Un gran número de Staphylococcus aureus y albus se encontraron en la muestra de hielo de vainilla crema. Sólo unos pocos estafilococos fueron demostrado en el helado de chocolate.’\nInforme de los cultivos de nariz y garganta de Los Petries que prepararon el helado decía lo siguiente:\nPresencia de Staphylococcus aureus y hemolítica del cultivo nasal y Staphylococcus albus del cultivo faríngeo de Gracia Petrie. Tambien Staphylococcus albus del cultivo de la nariz de Marian Petrie. Los estreptococos hemolíticos no eran del tipo generalmente asociado con infecciones en el hombre.\nDiscusión sobre la fuente: la fuente de contaminación bacteriana del helado de vainilla no está claro. Cualquiera que sea el método de la introducción de los estafilococos, parece razonable suponer que debe haber ocurrido entre la tarde del 17 de abril y la mañana del 18 de abril. Sin motivo de contaminación Se conoce la peculiaridad del helado de vainilla. “Al dispensar los helados, la misma cuchara se utilizó. Por lo tanto, no es improbable suponer que alguna contaminación al helado de chocolate crema ocurrió de esta manera. Esto parecería ser la explicación más plausible para la enfermedad en los tres individuos que no comieron el helado de vainilla.\nMedidas de Control: El 19 de mayo, todo el helado restantes fue condenado. Todos los demás alimentos en el la cena de la iglesia había sido consumida.\nConclusiones: Un brote de gastroenteritis ocurrió después de una cena en la iglesia en Lycoming. La causa del brote fue helado de vainilla por contaminado. El método de contaminación de helado no se entiende claramente.\nSi el estafilococo dio positivo de la nariz y la garganta de los cultivos realizados en la familia Petrie haba todo lo que tenga que ver con la contaminación es un asunto por nexo epidemiológico.\nNota: El paciente #52 era un niño que mientras viendo el procedimiento de congelación se le dio una plato de helado de vainilla a las 11:00 am en abril 18."
  },
  {
    "objectID": "pages/oswego.es.html#overview",
    "href": "pages/oswego.es.html#overview",
    "title": "Oswego (ES)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\n\n\n\nName\nOswego\n\n\nTool\nR\n\n\nLanguage\nSpanish/Español\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nGastrointestinal\n\n\nKeywords\nGastrointestinal;Outbreak investigation\n\n\nTechnical complexity\nIntermidiate\n\n\nMethodolocial complexity\nIntermidiate\n\n\n\nAuthorship\nOriginal authors: Centre for Disease Prevention and Control (CDC)\nData source: Epi Info, version 3.5.4 (CDC)\nAdapted to R by: Leonel Lerebours Nadal y Alberto Mateo Urdiales"
  },
  {
    "objectID": "pages/oswego.es.html#instructions",
    "href": "pages/oswego.es.html#instructions",
    "title": "Oswego (ES)",
    "section": "",
    "text": "There are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\nYou will see these icons throughout the exercises:\n\n\n\n\n\n\n\nIcon\nMeaning\n\n\n\n\n\nObserve\n\n\n\nAlert!\n\n\n\nAn informative note\n\n\n\nTime for you to code!\n\n\n\nChange to another window\n\n\n\nRemember this for later\n\n\n\n\n\n\n\nThis case study has been adapted from an existing tutorial on Epi Info created by the Centre for Disease Prevention and Control (CDC). Epi Info™ is a trademark of CDC. Epi Info™ programs are provided in the public domain to promote public health. Programs might be freely translated, copied, or distributed. No warranty is made or implied for use of the software for any particular purpose.\n Applied Epi Incorporated, 2022 This work is licensed by Applied Epi Incorporated under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\nPlease email contact@appliedepi.org with questions about the use of these materials for academic courses and epidemiologist training programs."
  },
  {
    "objectID": "pages/oswego.es.html#feedback-suggestions",
    "href": "pages/oswego.es.html#feedback-suggestions",
    "title": "Oswego (ES)",
    "section": "",
    "text": "You can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org"
  },
  {
    "objectID": "pages/oswego.es.html#version-1",
    "href": "pages/oswego.es.html#version-1",
    "title": "Oswego (ES)",
    "section": "",
    "text": "16 November 2023"
  },
  {
    "objectID": "pages/oswego.es.html#disclaimer",
    "href": "pages/oswego.es.html#disclaimer",
    "title": "Oswego (ES)",
    "section": "",
    "text": "Esto es un estudio de caso diseñado por el Centre for Disease Prevention and Control (CDC) como tutorial de Epi Info. Puede consultar más detalles en este Enlace"
  },
  {
    "objectID": "pages/oswego.es.html#revisions",
    "href": "pages/oswego.es.html#revisions",
    "title": "Oswego (ES)",
    "section": "",
    "text": "Date\nChanges made\nVersion\n\n\n\n\n16 November\nAdapted to template\n1"
  },
  {
    "objectID": "pages/oswego.es.html#objetivos-de-este-estudio-de-caso",
    "href": "pages/oswego.es.html#objetivos-de-este-estudio-de-caso",
    "title": "Oswego (ES)",
    "section": "",
    "text": "Los objetivos de este estudio de caso son:\n\nEntender los diferentes pasos en la investigación de un brote de casos de enfermedad gastrointestinal\nAdquirir confianza en el manejo de datos de un listado nominal con el software estadístico R\nAdquirir experience en el análisis descriptivo en R, particularmente curvas epidémicas\nAdquirir experience construyendo tablas 2x2 con exposición y desenlace que nos permitan calcular medidas de asociación\nAplicar los conocimientos adquiridos a posibles actividades de control y prevención de brotes infecciosos de origen alimentario\n\n\n\nEn este estudio de caso se asume un conocimiento básico de los principios fundamental de la investigación epidemiológica de brotes gastrointestinales. Se asume también un conocimiento básico de R.\n\n\n\nAntes de iniciar este estudio de caso, le aconsejamos que:\n\nDescargue en su computadora la carpeta “oswego_cs_es” y que extraiga todos sus componentes, preferibilmente en su escritorio o en un lugar de fácil acceso. Evite extraerlo en servicios de nube o “drives”\n\nDentro de la carpeta, encontrará un proyecto de R llamado “oswego_cs”. Es un archivo de tipo “R project” y debe siempre asegurarse que está trabajando en RStudio desde el proyecto. La forma más fácil es que habra RStudio cada vez a través de abrir este archivo.\nDentro de la carpeta “oswego_cs_es” encontrará una subcarpeta llamada data en el que encontrará todos los datos necesarios para realizar el análisis en un file llamado “oswego.xlsx”.\nDeberá crear un script dentro de la carpeta scripts en el que usted escribe el código para el análisis. Puede utilizar el script que ya está presente llamado “01_oswego_sol” que contiene el código de análisis si se encuentra atascado o si quiere comparar el código que usted realiza con la solución.\nEn la subcarpeta outputs encontrará todos los gráficos y tablas generadas en el anlálisis.\n\n\nPrimera parte - AntecedentesSegunda parte - El EventoTercera parte - AnálisisCuarta Parte - Conclusión\n\n\n\n\n\n\n\nEl 19 de abril de 1940, el oficial de salud local en el pueblo de Lycoming, condado de Oswego, Nueva York, informó de la ocurrencia de un brote de enfermedad gastrointestinal al Distrito de Salud Oficial en Siracusa. Dr. A. M. Rubin, epidemiólogo en formación, fue asignado para investigar lo ocurrido.\nCuando el Dr. Rubin llegó al campo, determinó a través del oficial de salud que todas las personas que enfermaron había asistido a una cena en la iglesia celebrada la noche anterior, 18 de abril. Otra información importante fue que los familiares que no asistieron a la cena, no enfermaron.\nEn consecuencia, el Dr. Rubin centró la investigación sobre lo ocurrido en la cena. Pudo completar 75 entrevistas de las 80 personas conocidas que asistieron a la cena, recopilando información sobre los ocurrencia y tiempo de aparición de los síntomas, y alimentos consumidos. de las 75 personas entrevistados, 46 personas presentaron síntomas de enfermedad gastrointestinal.\n\n\n\n¿Ante que tipo de situación está presente el Dr Rubin?\n\n Una epidemia Una serie de casos Un brote No se puede establecer\n\n\n\n\n\n Click para leer una pista\n\n\nPuede utilizar este glosario preparado por el Gobierno de México para encontrar la definición que se ajusta más a la situación descrita Enlace \n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nNumero 1 - No es la respuesta correcta; revisa el concepto de epidemia, tiene que ver con la cantidad de personas afectadas.\nNumero 2 - Es posible, pero también debes tomar en cuenta otros factores, como el hecho de que los casos tienen una relación epidemiológica entre ellos.\nNumero 4 - No te preocupes, en este tutorial vas a poder aprender los pasos del trabajo de campo.\n\n\n\n\n\n\nLos pasos de una investigación de brote son:\n\n Determinar la existencia del brote, análisis descriptivo, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Una serie de casos Determinar la existencia del brote, confirmar diagnóstico, contar casos, análisis descriptivo, determinar quién está a riesgo de enfermar, desarrollar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final Determinar la existencia del brote, confirmar el diagnóstico, controlar brote, comunicar brote, generar hipótesis, confirmar hipótesis, controlar brote, conclusiones y recomendaciones, informe final\n\n\n\n\nClick para ver la explicación (¡Inténtelo usted primero!)\n\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\n\n\nEl inicio de la enfermedad en todos los casos fue agudo, caracterizada principalmente por náuseas, vómitos, diarrea y dolor abdominal. Ninguno de los enfermos personas reportaron tener un nivel elevado temperatura; todos se recuperaron dentro de las 24 a 30 horas.\nAproximadamente el 20% de los enfermos que visitaron al médico no se les realizó examen de muestras fecales para el examen bacteriológico.\n\n\nEnumere una de las grandes categorías de agentes causales de enfermedades que se deben considerar en el diagnóstico diferencial de un brote de enfermedad gastrointestinal como el de Oswego:\n\n\n\n\nClick para ver la explicación de la solución (¡Inténtelo usted primero!)\n\n\n\nBacterias\nVirus\nParásitos\nToxinas\n\nPuede utilizar el sitio de la OPS para profundizar sobre el tema Enlace\n\n\n\nLos investigadores en Oswego, desconocen el agente causal, pero sospechan de que la génesis de este brote fue través de los alimentos como vehiculo de transmisión entre los afectados.\n\n\n\nEn lenguaje epidemiológico, ¿Qué es un vehículo? ¿Qué es un vector? ¿Cuáles son otros modos?\nPiense en estos conceptos y cuando esté listo, vea la solución propuesta\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEn la jerga epidemiológica, un ‘vehículo’ es un objeto o sustancia inanimada que puede transportar un patógeno y transmitirlo a un huésped susceptible. Ejemplos de vehículos incluyen alimentos o agua contaminados, fómites (objetos inanimados como pomos de puertas o ropa) o partículas transportadas por el aire. En este contexto, un vehículo no es un modo de transporte, sino más bien un medio de transmisión de un agente infeccioso. Por otro lado, un ‘vector’ es un objeto animado, generalmente un artrópodo como un mosquito, una garrapata o una pulga, que puede transportar un agente infeccioso desde un huésped infectado a un huésped susceptible. El vector puede transmitir el patógeno directamente a través de su picadura o indirectamente al depositar el patógeno en una superficie o en una fuente de alimentos o agua. Los otros modos de transmisión de agentes infecciosos incluyen el contacto persona a persona, ya sea directamente a través del contacto físico, como el tacto o el beso, o indirectamente a través de gotas generadas durante la tos o el estornudo, o mediante transmisión aérea en espacios cerrados. Además, algunos agentes infecciosos pueden ser transmitidos a través del contacto sexual, la transmisión perinatal de la madre al hijo o mediante la exposición a fluidos corporales, como la sangre o el semen. Los factores ambientales, como la mala higiene, el hacinamiento o la exposición a animales, también pueden desempeñar un papel en la transmisión de ciertos agentes infecciosos.\n\n\n\nEl Dr. Rubin decidió administrar un cuestionario a los participantes de la cena de la iglesia para averiguar qué alimento podía estar asociado al desarrollo de los síntomas\n\n\n\nSi fuese usted el que administra el cuestionario, ¿qué información recopilaría? Agrupa la información en categorías. Una vez que haya escrito sus categorías, puede ver abajo la solución.\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstos son algunos de los campos que normalmente se includirían en un cuestionario en un brote similar:\n\nInformación demográfica\nInformaión clínica\nDatos de laboratorio si disponibles\nFactores de riesgo (exposición): Alimentos y bebidas ingeridas durante la cena\n\n\n\n\nAdemás de decidir la información que quería recolectar, el Dr. Rubin decidió recolectó los datos entrevistados a través de un listado nominal.\n\n\n\n\n¿En que NO nos ayuda un listado nominal?\n\n Organizar los datos y describirlos por tiempo, lugar y persona Clasificar a los individuos como casos, no casos y sospechosos A diagnosticar a los pacientes A manejar un documento dinámico que se puede actualizar constantemente\n\n\nPor favor, continue el caso entrando en la pestaña “Segunda parte - El Evento” en la parte superior\n\n\n\n\n\n\nLa investigación del Dr. Rubin también implicó averiguar más detalles sobre la cena. Después de hablar con los organizadores, descubrió que la cena se celebró en el sótano del iglesia del pueblo. Los alimentos fueron aportados por numerosos miembros de la congregación. La cena comenzaba a las 6:00 p.m. y continuó hasta 11.00 pm.\nLa comida estaba esparcida sobre una mesa y fue consumida durante un período de varias horas. Los datos sobre el inicio de la enfermedad y los alimentos consumidos por cada una de las 75 personas entrevistados se proporcionan en la listado adjunto.\nLa hora aproximada de participación en el evento solo se recolectó aproximadamente la mitad de las personas que tuvo una enfermedad gastrointestinal.\n\n\n¿Cuál es el valor de una curva epidémica en la investigación de un brote?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son algunos de los usos de las curvas epidémicas cuando se investigan brotes:\n\nNos permite ver la evolución en el tiempo de un evento forma rápida\nAporta información para tomar decisiones para medidas de control\nAyuda a revelar patrones y tendencias sobre un evento\n\n\n\n\n\n\n\n¿Qué nos dice el siguiente gráfico?\n\n\n\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nLa curva epidémica nos dice que:\n\nTodos los casos ocurrieron antes de las 10am del día siguiente (19 de abril)\nDesde la 11pm del 18 a las 3am del 19 de abril ocurrieron la mayoría de los casos\nNos muestra la magnitud del evento y como se propaga, así como ver valores extremos\n\n\n\n\n\n\n\n¿Hay algún caso en el que los tiempos de inicio no coincidan con los generales? ¿experiencia? ¿Cómo podrían explicarse?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nConsidere que: - Hay casos que la hora de inicio de signos y sintomas fueron antes de la cena, pudo ser que se contaminara antes durante los preparativos - Un caso ocurrió 17 horas después de la cena, posiblemente es alguien que comió más tarde o que la infomación es incorrecta (otra enfermedad parecida) o un caso secundario\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n¿Cómo podrían presentarse mejor los datos en el listado nominal de participantes?\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nDos ideas, aunque puede haber más son:\n\nLos datos pudieron ser separados de acuerdo al estatus de enfermedad y tiempo de inicio de sintomas\nSi se hubiese usado el formato del en tiempo militar, (ej. 00:00 o 14:00)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAhora vamos a comenzar con uno de los pasos más importantes en la investigación de brote, el análisis de los datos donde a través de este vamos a determinar cual o cuales son las posibles causas del brote, medidas a tomar entre otros pasos. ¡También vamos a usar un poco de R para ayudar con este proceso de análisis!.\n\n\n \nPara contestar esta pregunta, vamos hacer los siguientes pasos usando R:\n \n\nen Rstudio, crea un nuevo script para cargar los datos (cargar el archivo “oswego.xlsx”, que está en la carpeta data) y explore las dos columnas que contienen la información necesaria para calcular los períodos de incubación: TimeSupper (hora de la cena) y DateOnset (Fecha inizio síntomas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Cargar los paquetes necesarios\npacman::p_load(rio,\n               here,\n               tidyverse, \n               epitools, \n               lubridate,\n               DT)\n\n#Importar los datos\nlistado &lt;- import(\"../case_studies_to_translate/ESP/oswego_cs_es/data/oswego.xlsx\")\n\n#Explorar las variables de la fecha y hora del almuerzo\nhead(oswego_db$TimeSupper)\n\n[1] NA                        NA                       \n[3] NA                        \"1940-04-18 22:00:00 UTC\"\n[5] \"1940-04-18 19:30:00 UTC\" \"1940-04-18 19:30:00 UTC\"\n\nhead(oswego_db$DateOnset)\n\n[1] \"1940-04-18 23:00:00 UTC\" NA                       \n[3] \"1940-04-18 22:30:00 UTC\" \"1940-04-19 01:00:00 UTC\"\n[5] \"1940-04-19 02:30:00 UTC\" \"1940-04-18 23:30:00 UTC\"\n\n\n\n\n\nPara referencia sobre como importar archivos, ver el capítulo 7 del libro de R para epidemiologos\n \n\nEl listado cargado ya tiene el formato correcto de la fecha y hora de almuerzo y la fecha y hora de inicio de síntomas ahora intente crear un código para crear una nueva variable con los periodos de incubación para cada caso. (recuerda que con las variables de tiempo en R se pueden hacer operaciones matemáticas)\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Crear la variable del período de incubación\n# puedes copiar este código en rstudio en tu editor de códigos\n#las funciones \"interval\" y \"dhours()\" son las que usaremos para calcular la diferencia en hora\n\nlistado_incubacion &lt;- listado %&gt;%\n  \n  mutate(incubacion=interval(TimeSupper, DateOnset) / dhours(1)) \n\n\n\n\nPara más detalles de como trabajar con fechas, ver el capítulo 9 del libro de R para epidemiologos\n \n\nAhora intente hacer un gráfico de barras con el período de incubación usando ggplot para visualizar la distribución. Puede encontrar pistas sobre como hacer un gráfico de barras en la sección dedicada del EpiRhandbook\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n# Hacer un gráfico de barra de los periodos de incubación calculados\n# puedes copiar este código en rstudio en tu editor de códigos\n\nlistado_incubacion %&gt;% \n  \n  ggplot(aes(x=incubacion))+\n  \n  geom_bar()+\n  \n  labs(title=\"Casos de enfermedad gastrointestinal por período de incubación en horas\",\n       subtitle = \"Oswego, NY, 18-19 de abril, 1940\",\n       x=\"Período de incubación (Horas)\",\n       y=\"n de casos\")+\n  \n  theme_minimal()\n\nWarning: Removed 53 rows containing non-finite values (`stat_count()`).\n\n\n\n\n#Si quieres asignar este gráfico a un objeto, solo tienes que en la primera línea del código \n#usar un nombre (como grafico1) y escribir el signo de asignación (&lt;-)\n\n\n\n\nPara más detalles de como trabajar con gráficos en general, ver el capítulo 30 del libro de R para epidemiologos\n \n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n22\nPara saberlo, puedes verificar viendo el listado directamente o con el siguiente código\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n Para ejecutar esto en R hay varias formas, usando ya sea las funciones base o de otros paquetes. Aquí tiene un ejemplo usando el paquete {dplyr} que viene integrado en el paquete {tidyverse}\n\n# Calcular el rango, la mediana y el periodo de incubacion\n# puedes copiar este código en rstudio en tu editor de códigos\n\nresumen_estadistico &lt;- listado_incubacion %&gt;% \n  filter(!is.na(incubacion)) %&gt;% \n  reframe(mediana=median(incubacion),\n          min=min(incubacion),\n          max=max(incubacion),\n          rango=max-min)\n\nresumen_estadistico\n\n  mediana min max rango\n1       4   3   7     4\n\n\nLa mediana del periodo de incubación fue 4 horas, así como el rango del periodo del periodo de incubación también fue de 4 horas\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nNos ayuda porque:\n\nCada enfermedad transmitida por los alimentos tiene un período de incubación característico, síntomas específicos y alimentos con los que es más probable que esté asociada\nEl período de incubación observado es demasiado largo para los metales pesados y demasiado corto para los agentes virales y el botulismo\nLa intoxicación alimentaria estafilocócica tiene un período de incubación promedio de 2 a 4 horas\nEstos datos son insuficientes para saber cual puede ser el agente causal \n\n\n\n\n\n\n \nPara contestar esta pregunta, intente hacer los siguientes pasos usando R con el mismo script que creaste anteriormente:\n\nCree un objeto “data.frame” con el resumen de los alimentos ingeridos por los que enfermaron y otro con el resumen de los alimentos ingeridos por los que no enfermaron. Recodifique las variables para asignar el valor de 1 si fue consumido y 0 para no consumido.\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Ve copiando el codigo a rstudio (este ejercicio es un poco largo)\n\npacman::p_load(janitor, gtsummary)\n\n\n#Comieron\n#hacer un dataframe con un resumen de los alimentos por los que enfermaron\ntotal_por_alimentos_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"enfermo\") %&gt;% \nslice_tail()\n\n#hacer un dataframe con un resumen de los alimentos por los que no enfermaron\ntotal_por_alimentos_no_casos_a &lt;- listado %&gt;% \n  mutate(tipo_caso=case_when(enfermo==1~\"enfermo\",\n                        TRUE~\"no_enfermo\")) %&gt;% \n  select(tipo_caso, starts_with(\"m_\")) %&gt;% \n  filter(tipo_caso==\"no_enfermo\") %&gt;% \nadorn_totals(\"row\", name = \"no_enfermo\") %&gt;% \nslice_tail()\n\n\n\n\nUna los dos objectos “data.frame” y calcule la proporción de personas enfermas y no enfermas que consumieron cada alimento\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que consumieron\ntabla_maestra_a &lt;- bind_rows(total_por_alimentos_casos_a,\n                           total_por_alimentos_no_casos_a) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\n\n\n#Combinar ambos dataframes, transformarlo a formato extendido y calcular la proporción de los que no consumieron \ntabla_maestra_b &lt;- bind_rows(total_por_alimentos_casos_b,\n                           total_por_alimentos_no_casos_b) %&gt;% \n  pivot_longer(2:ncol(.), names_to = \"alimentos\", values_to = \"n\") %&gt;% \n  pivot_wider(names_from = tipo_caso, values_from = n) %&gt;% \n  mutate(total=enfermo+no_enfermo,\n         ptc_enfermo=enfermo/total,\n         ptc_no_enfermo=no_enfermo/total)\n\ntabla_final &lt;- tabla_maestra_a %&gt;% \n  left_join(tabla_maestra_b,suffix = c(\"_consumieron\", \"_no_consumieron\"), by=\"alimentos\") %&gt;% \n  mutate(tasa_ataque=ptc_enfermo_consumieron/ptc_enfermo_no_consumieron)\n\nView(tabla_final)\n\n\n\n\nCalcule una medida de asociación para estimar qué alimento se asoció en mayor medida a enfermar\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\n\n#otra forma.. para obtener los OR de cada alimento\n\n#crear un modelo de regresión logistica\nmodel &lt;- glm(data=listado, enfermo~m_jamon_horneado+\n            m_espinaca+m_pure_papa+m_ensa_repollo+\n            m_gelatina+m_rollos+m_pan+m_lehe+m_cafe+m_agua+\n            m_bizcocho+m_hel_vainilla+m_hel_chocolate+m_ens_fruta,\n            family=binomial())\n#Luego una tabla\ntbl_regression(model, exponentiate = TRUE)\n\ntest &lt;- listado %&gt;% \n  select(enfermo, starts_with(\"m_\")) %&gt;% \n  tbl_summary(by=enfermo)\n\n\n\n\nPara más detalles de como trabajar con transformación de datos y tablas, ver el capítulo 17 del libro de R para epidemiologos\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales investigaciones adicionales que deben llevarse a cabo:\n\nRevisión detallada de la fuente, los ingredientes, la preparación y el almacenamiento de los alimentos incriminados\nIntentar explicar los casos con tiempo de inicio atípico\nSe podría hacer un examen de laboratorio\nDeterminar si se produjo una propagación secundaria en los miembros de la familia\nCálculos adicionales (p. ej., tasas de ataque específicas por edad o género) \n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nEstas son las principales medidas de control y prevención en un brote de estas características:\n\nEvite el consumo del helado de vainilla restante\nPrevenga la recurrencia de eventos similares en el futuro educando a los manipuladores de alimentos\nSe podría hacer un examen de laboratorio\nDeterminar si se trata de un producto comercial\nEliminó cualquier fuente contaminada de alimentos\n\n\n\n\n\n\n\n\n\n\nClick para ver la solución (¡Inténtelo usted primero!)\n\n\nTrabajar en este brote ayudó a:\n\nDescartar la contaminación de un producto comercial. Si se trata de un producto comercial, la intervención inmediata puede prevenir un número considerable de casos adicionales\nPrevenir futuros brotes mediante la identificación de manipuladores de alimentos infectados, lagunas específicas en la educación o técnicas de manipulación de alimentos\nLos funcionarios de salud pública deben responder a tales problemas de manera oportuna para mantener una relación de cooperación con los departamentos de salud locales, los médicos privados y la comunidad\nUna explicación epidemiológica de la causa del brote puede disipar los temores y preocupaciones de la comunidad\nLa investigación del brote puede brindar oportunidades para que los investigadores respondan preguntas sobre el agente, el huésped, el entorno, el período de incubación, etc.\n\n\n\n\n\n\n\n\n\n\n\n\n\nLo siguiente se cita textualmente del informe preparado por el Dr. Rubin:\nEl helado fue preparado por el Petrie hermanas de la siguiente manera: En la tarde del 17 de abril la leche cruda de la La granja Petrie en Lycoming se desbordó al baño maría se le agrega azúcar y huevos y un poco de harina para darle cuerpo a la mezcla. El se prepararon helado de chocolate y vainilla por separado.\nEl chocolate de Hershey era necesariamente añadido a la mezcla de chocolate. A las 6 pm. los dos las mezclas se llevaban en recipientes tapados al sótano de la iglesia y se dejó reposar durante la noche. Presuntamente no fueron tocados por nadie. durante este período.\nEn la mañana del 18 de abril, el Sr. Coe agregó cinco onzas de vainilla y dos latas de leche condensada a la mezcla de vainilla y tres onzas de vainilla y una lata de leche condensada a la mezcla de chocolate. Luego el helado de vainilla se transfirió a un lata de congelación y se coloca en un congelador eléctrico durante 20 minutos, después de lo cual el helado de vainilla se sacó de la lata del congelador y se envasó en otra lata que había sido previamente lavado con agua hirviendo. Entonces el chocolate la mezcla se puso en la lata del congelador que había sido se enjuaga con agua del grifo y se deja congelardurante 20 minutos.”\nAl concluir esto, ambos las latas se taparon y se colocaron en grandes recipientes de madera recipientes llenos de hielo. Como señaló, el helado de chocolate permaneció en el una lata de congelador.\nTodos los manipuladores del helado fueron examinados. Sin lesiones externas ni respiratorias altas se notaron infecciones. Cultivos de nariz y garganta fueron tomados de dos individuos que prepararon el helado.\nLos exámenes bacteriológicos fueron hechos por el División de Laboratorios e Investigación, Albany, en ambos helados. Su informe es el siguiente:\n‘Un gran número de Staphylococcus aureus y albus se encontraron en la muestra de hielo de vainilla crema. Sólo unos pocos estafilococos fueron demostrado en el helado de chocolate.’\nInforme de los cultivos de nariz y garganta de Los Petries que prepararon el helado decía lo siguiente:\nPresencia de Staphylococcus aureus y hemolítica del cultivo nasal y Staphylococcus albus del cultivo faríngeo de Gracia Petrie. Tambien Staphylococcus albus del cultivo de la nariz de Marian Petrie. Los estreptococos hemolíticos no eran del tipo generalmente asociado con infecciones en el hombre.\nDiscusión sobre la fuente: la fuente de contaminación bacteriana del helado de vainilla no está claro. Cualquiera que sea el método de la introducción de los estafilococos, parece razonable suponer que debe haber ocurrido entre la tarde del 17 de abril y la mañana del 18 de abril. Sin motivo de contaminación Se conoce la peculiaridad del helado de vainilla. “Al dispensar los helados, la misma cuchara se utilizó. Por lo tanto, no es improbable suponer que alguna contaminación al helado de chocolate crema ocurrió de esta manera. Esto parecería ser la explicación más plausible para la enfermedad en los tres individuos que no comieron el helado de vainilla.\nMedidas de Control: El 19 de mayo, todo el helado restantes fue condenado. Todos los demás alimentos en el la cena de la iglesia había sido consumida.\nConclusiones: Un brote de gastroenteritis ocurrió después de una cena en la iglesia en Lycoming. La causa del brote fue helado de vainilla por contaminado. El método de contaminación de helado no se entiende claramente.\nSi el estafilococo dio positivo de la nariz y la garganta de los cultivos realizados en la familia Petrie haba todo lo que tenga que ver con la contaminación es un asunto por nexo epidemiológico.\nNota: El paciente #52 era un niño que mientras viendo el procedimiento de congelación se le dio una plato de helado de vainilla a las 11:00 am en abril 18."
  },
  {
    "objectID": "pages/stegen-en.html",
    "href": "pages/stegen-en.html",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nAn Outbreak of Gastroenteritis in Stegen, Germany\n\n\nLanguage:\nEnglish\n\n\nTool:\nR;\n\n\nLocation:\nGermany\n\n\nScale:\nLocal\n\n\nDiseases:\nGI\n\n\nKeywords:\nGI; Stratified analysis; R\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nBasic"
  },
  {
    "objectID": "pages/stegen-en.html#overview",
    "href": "pages/stegen-en.html#overview",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nAn Outbreak of Gastroenteritis in Stegen, Germany\n\n\nLanguage:\nEnglish\n\n\nTool:\nR;\n\n\nLocation:\nGermany\n\n\nScale:\nLocal\n\n\nDiseases:\nGI\n\n\nKeywords:\nGI; Stratified analysis; R\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nBasic"
  },
  {
    "objectID": "pages/stegen-en.html#instructions",
    "href": "pages/stegen-en.html#instructions",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "Instructions",
    "text": "Instructions\n\nGetting Help\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHints and Solutions\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\nPosting a question in the Community Forum\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\nTerms of Use\nDisclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/\n\n\n\nFeedback & suggestions\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\nVersion and revisions\nWrite date of first version\nWrite any revisions made to the case study\n\n\n\n\n\n\n\n\nDate\nChanges made\nAuthor\n\n\n\n\n2015\nThe case study has been divided in two parts: the first includes descriptive, univariable and stratified analysis as pre-module homework; the second includes logistic and binary regression (not shown here). Unnecessary toponymes were removed.\nAlicia Barrasa (EPIET) and Ioannis Karagiannis (Publich Health England-PHE)\n\n\n2017\nQuestions were rephrased to reflect real life scenarios (rather than academic exercise)\nAlicia Barrasa (EPIET) and Giri Shankar (Public Health Wales-PHW)\n\n\n2017\nThe case study was adapted to include the help on R\nNiklas Willrich (Robert Koch Institute-RKI), Patrick Keating (Austrian Agency for Health and Food Safety-AGES) and Alexander Spina (AGES)\n\n\n2017\nContribution to the R code\nDaniel Gardiner (Public Health England-PHE) and Lukas Richter (AGES)\n\n\n2022\nMinor revisions to the R code and explanations\n\n\n\n2023\nMajor revision of the R code. R code was simplified and R tidyverse code was implemented\nLiese Van Gompel (MedEPIET)\n\n\n2024\nRevision of the R code, i.e. use of EpiStas package for univariable and multivariable analysis, simplification and harmonisation of the R code\nKostas Danis (MediPIET)\n\n\n2017\nRevision of content, structure, R code and adaptation of format to Applied Epi’s template of case studies\nAlberto Mateo Urdiales (ISS)"
  },
  {
    "objectID": "pages/stegen-en.html#guidance",
    "href": "pages/stegen-en.html#guidance",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "Guidance",
    "text": "Guidance\n\nObjectives of this case study\nAt the end of this case study, participants should:\n\nknow and be able to perform the fundamental steps of a descriptive statistical analysis of a foodborne outbreak (including quantitative assessment (frequency distributions, missing values, means/medians/modes, quartiles/SDs) and visualization of the data (histogram, boxplot))\nbe able to perform univariate statistical analysis to identify potential vehicles of a foodborne outbreak (including risk ratios and/or odds ratios, depending on the design)\nknow why and how stratification can be conducted in datasets related to an outbreak investigation\nbe able to conduct a stratified analysis with respect to potential risk factors and confounders/effect modifiers\n\n\n\nPrevious level of expertise assumed\nParticipants are expected to be familiar with data management and basic analysis in R.\n\n\nPreparation for the case study\n\nDownload folder named stengen_mva and extract contents in the local laptop\nCreate an Rstudio project in the folder stengen_mva If you are unsure on how to do that, read the EpiRhandbook Chapter on R projects\nInside the folder stengen_mva: Subfolder “data” contains a raw data file named tira.csv. This is the only data file you will use in this case study. In the same folder you can find the data dictionary with a description of the dataframe variables.\nSubfolder scripts should be used to save any scripts related to the analysis. Inside “backup” you will find a solution script with the code of the case study named stengen_analysis_backup.R.\nSubfolder “outputs” could be used to store all outputs (tables, graphs, documents) that are the result of the analysis"
  },
  {
    "objectID": "pages/stegen-en.html#introduction",
    "href": "pages/stegen-en.html#introduction",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "Introduction",
    "text": "Introduction\nOn 26 June 1998, the St Sebastian High School in Stegen (school A), Germany, celebrated a graduation party, where 250 to 350 participants were expected. Attendees included graduates from that school, their families and friends, teachers, 12th grade students and some graduates from a nearby school (school B).\nA self-service party buffet was supplied by a commercial caterer in Freiburg. Food was prepared on the day of the party and transported in a refrigerated van to the school.\nFestivities started with a dinner buffet which opened from 8:30 pm onwards and were followed by a dessert buffet offered from 10 pm. The party and the buffet extended late during the night and alcoholic beverages were quite popular. All agreed it was a party to be remembered."
  },
  {
    "objectID": "pages/stegen-en.html#the-alert",
    "href": "pages/stegen-en.html#the-alert",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "The alert",
    "text": "The alert\nOn 2nd July 1998, the Freiburg local health office reported to the Robert Koch Institute (RKI) in Berlin the occurrence of many cases of gastroenteritis following the graduation party described above. More than 100 cases were suspected among attendees and some of them were admitted to nearby hospitals. Sick people suffered from fever, nausea, diarrhoea and vomiting that lasted for several days. Most believed that the tiramisu consumed at dinner was responsible for their illness. Salmonella Enteritidis was isolated from 19 stool samples.\nThe Freiburg health office sent a team to investigate the kitchen facilities of the caterer. Food preparation procedures were reviewed. Food samples, except tiramisu (none was left over), were sent to the laboratory of Freiburg University. Microbiological analyses were performed on samples of the following: brown chocolate mousse, caramel cream, remoulade sauce, yoghurt dill sauce and 10 raw eggs.\nThe Freiburg health office requested help from the RKI in the investigation to assess the magnitude of the outbreak and identify potential vehicle(s) and risk factors for transmission in order to better control the outbreak."
  },
  {
    "objectID": "pages/stegen-en.html#the-study",
    "href": "pages/stegen-en.html#the-study",
    "title": "An Outbreak of Gastroenteritis in Stegen, Germany (ENG)",
    "section": "The study",
    "text": "The study\nCases were defined as any person who had attended the party at St Sebastian High School who suffered from diarrhoea (≥ 3 loose stool for 24 hours) between 27 June and 29 June 1998; or who suffered from at least three of the following symptoms: vomiting, fever ≥38.5°C, nausea, abdominal pain, and headache.\nStudents from both schools attending the party were asked through phone interviews to provide names of persons who attended the party.\nOverall, 291 responded to enquiries and 103 cases were identified (attack rate: 35%). Among these cases, 84 (82%) received medical treatment and four were admitted to hospitals. Attack rates by age group were 36.6% for persons &lt;20 years, 32.1% for persons 20 to 29 years, and 36.8% for persons older than 29 years.\n\n\n\nFigure 1: Cases of gastroenteritis by time of onset among attendees of a high-school graduation ceremony (n=103), Germany, June 1998\n\n\nQUESTION 1. What would be your hypothesis concerning the source of the outbreak?\n\n\nClick to see the solution\n\n\nThe shape of the epidemic curve and the attendance to a single event (a buffet) pointed towards a foodborne outbreak related to a point source of infection.\n\n\nQUESTION 2. What study design would you choose to test this hypothesis?\n\n\nClick to see the solution\n\n\nUsing the updated list of attendants, a retrospective cohort study including all attendants to the party (that could be reached) was conducted. All had received a standard questionnaire asking for demographic information, signs, symptoms and duration, admission to hospital, and food and beverages consumption at the party including amount consumed. Food-specific attack rates were computed for more than 50 food items and beverages.\n\n\nQuestion 3. What would your overall plan of analysis be?\n\n\nClick to see the solution\n\n\nPerform data cleaning\n\nFor each variable, look at the range, unexpected and missing values.\nCorrect data using the original forms used if needed\n\nDescribe each variable\n\nFor each variable, describe frequency distributions including missing values and, if needed, means, median, modes, quartiles, standard deviation, outliers\nMake appropriate histograms and box plots\nChoose relevant characteristics to describe the population\n\nIdentify the outbreak vehicle if any\n\nCalculate food-specific attack rates\nLook at the proportions of cases exposed\nChose the appropriate measure of association\nChose the appropriate statistical tests and significance level\nCalculate the percentages of cases exposed to each exposure\nSearch for any dose-response relationship if appropriate\nInterpret the results\n\nPerform a stratified analysis\n\nIdentify the variables that are potential effect modifiers (EM) and confounders\nDesign appropriate stratification tables\nStratify on each level taken by the EM and confounders\nCompute appropriate measurements to identify confounding and effect modification\nConduct appropriate statistical tests\nInterpret the results\n\nPerform a multivariable analysis\n\nThis will be discussed during the module."
  },
  {
    "objectID": "pages/under_construction.html",
    "href": "pages/under_construction.html",
    "title": "Under-construction case study page",
    "section": "",
    "text": "Under-construction case study page\n\n\n\n\n\n\nWEBSITE UNDER CONSTRUCTION\n\n\n\nThis case study is being developed. The content and URL will change.\n\n\nFor instructions on how to use our case studies, see our How-to Guide. We welcome feedback and suggestions via contact@appliedepi.org. You can also discuss the case study or related concepts on the Applied Epi Community."
  },
  {
    "objectID": "pages/fulton.html",
    "href": "pages/fulton.html",
    "title": "Fulton (EN)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName\nFulton County\n\n\nLanguage\nEnglish\n\n\nTool\nR\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nCOVID-19\n\n\nKeywords\nCOVID-19; SARS-COV-2; Outbreak\n\n\nTechnical complexity\nIntermediate\n\n\nMethodological complexity\nBasic\n\n\n\nAuthorship\nOriginal authors: Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer\nData source: Anonymised and jittered data provided by Fulton County for training purposes\nAdapted by: Alberto Mateo Urdiales to the case study template\n\n\n\n\n\n\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community…\n\n\n\nXXXXXXXXXXXXXXXXXXXXX\n\n\n\n\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\nThe first version was written by Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer in August 2021.\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nMar 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a an example R-markdown script which demonstrates how to create an automated outbreak situation report for COVID-19 in Fulton county, USA. The data used comes from an anonymised and fake (scrambled) linelist of COVID-19 cases in Fulton county from the beginning of the pandemic (early 2020) until July 2021.\nThe overall objective is to create an automatic and dynamic report that shows the COVID-19 epidemiological situation in Fulton County.\nIn this case study you will learn:\n\nHow to import, clean and analyse your data.\n\nCarry out descrptive analysis by time, place and person.\n\nUse the above to create an automatic and dynamic report in word using Rmarkdown.\n\n\nFor the purpose of the case study we separate this by descriptive analysis and visualisation (normally this would be mixed together of course). The visualisation section is organised in to place, time and person. This is to simplify flow for didactic delivery.\nAnalysis is loosely based off the monthly epidemiology reports for Fulton county\n\n\n\n\nUsers should have some prior experience with R, including:\n\nR basics: Several packages are required for different aspects of analysis with R. You will need to install these before starting. We install and load packages using the {pacman} package. Its p_load() command will install packages if necessary and load them for use in the current session. This might prove difficult if you have limited administrative rights for your computer. Making sure your IT-department gives you the correct access can save a lot of headache. See this handbook pages on the basics of installing packages and running R from network drives (company computers) for more detail. https://epirhandbook.com/r-basics.html#installation https://epirhandbook.com/r-on-network-drives.html#r-on-network-drives\nR projects: See Chapter 6 R Projects from the EpiRhandbook\nImport and export of data: See Chapter7 Import and export\n\n\n\n\n\nDownload folder fulton_en and extract contents in the local laptop\nOpen the Rstudio project inside the folder called fulton_en.Rproj\nInside the folder you can find the Rmd and the word output (weekly report). You can also find a word template that will be used as the template for the report. The Rmd and the output are there to help you if you struggle, but you should try to recreate these yourself following this case study.\nSubfolder data contains fulton COVID-19 data needed for the analysis\nSubfolder solution_materials has a copy of the Rmd document with the solution and a copy Word document with the output requested\nOpen a new Rmarkdown file in RStudio and save it in the root folder fulton_en. If you have any doubts about how to create an Rmarkdown follow the EpiRhandbook instructors here\nThis Rmarkdown file will be the file used throughout the case study and, rendering it will produce the weekly report in word format"
  },
  {
    "objectID": "pages/fulton.html#overview",
    "href": "pages/fulton.html#overview",
    "title": "Fulton (EN)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName\nFulton County\n\n\nLanguage\nEnglish\n\n\nTool\nR\n\n\nLocation\nUnited States\n\n\nScale\nLocal\n\n\nDiseases\nCOVID-19\n\n\nKeywords\nCOVID-19; SARS-COV-2; Outbreak\n\n\nTechnical complexity\nIntermediate\n\n\nMethodological complexity\nBasic\n\n\n\nAuthorship\nOriginal authors: Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer\nData source: Anonymised and jittered data provided by Fulton County for training purposes\nAdapted by: Alberto Mateo Urdiales to the case study template"
  },
  {
    "objectID": "pages/fulton.html#instructions",
    "href": "pages/fulton.html#instructions",
    "title": "Fulton (EN)",
    "section": "",
    "text": "There are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\nHere is what the “helpers” look like:\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community…\n\n\n\nXXXXXXXXXXXXXXXXXXXXX\n\n\n\n\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\nThe first version was written by Alex Spina, Neale Batra, Mathilde Musset, Henry Laurenson-Schafer in August 2021.\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nMar 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is a an example R-markdown script which demonstrates how to create an automated outbreak situation report for COVID-19 in Fulton county, USA. The data used comes from an anonymised and fake (scrambled) linelist of COVID-19 cases in Fulton county from the beginning of the pandemic (early 2020) until July 2021.\nThe overall objective is to create an automatic and dynamic report that shows the COVID-19 epidemiological situation in Fulton County.\nIn this case study you will learn:\n\nHow to import, clean and analyse your data.\n\nCarry out descrptive analysis by time, place and person.\n\nUse the above to create an automatic and dynamic report in word using Rmarkdown.\n\n\nFor the purpose of the case study we separate this by descriptive analysis and visualisation (normally this would be mixed together of course). The visualisation section is organised in to place, time and person. This is to simplify flow for didactic delivery.\nAnalysis is loosely based off the monthly epidemiology reports for Fulton county\n\n\n\n\nUsers should have some prior experience with R, including:\n\nR basics: Several packages are required for different aspects of analysis with R. You will need to install these before starting. We install and load packages using the {pacman} package. Its p_load() command will install packages if necessary and load them for use in the current session. This might prove difficult if you have limited administrative rights for your computer. Making sure your IT-department gives you the correct access can save a lot of headache. See this handbook pages on the basics of installing packages and running R from network drives (company computers) for more detail. https://epirhandbook.com/r-basics.html#installation https://epirhandbook.com/r-on-network-drives.html#r-on-network-drives\nR projects: See Chapter 6 R Projects from the EpiRhandbook\nImport and export of data: See Chapter7 Import and export\n\n\n\n\n\nDownload folder fulton_en and extract contents in the local laptop\nOpen the Rstudio project inside the folder called fulton_en.Rproj\nInside the folder you can find the Rmd and the word output (weekly report). You can also find a word template that will be used as the template for the report. The Rmd and the output are there to help you if you struggle, but you should try to recreate these yourself following this case study.\nSubfolder data contains fulton COVID-19 data needed for the analysis\nSubfolder solution_materials has a copy of the Rmd document with the solution and a copy Word document with the output requested\nOpen a new Rmarkdown file in RStudio and save it in the root folder fulton_en. If you have any doubts about how to create an Rmarkdown follow the EpiRhandbook instructors here\nThis Rmarkdown file will be the file used throughout the case study and, rendering it will produce the weekly report in word format"
  },
  {
    "objectID": "pages/fulton.html#step-1-rmarkdown-set-up",
    "href": "pages/fulton.html#step-1-rmarkdown-set-up",
    "title": "Fulton (EN)",
    "section": "Step 1: Rmarkdown set up",
    "text": "Step 1: Rmarkdown set up\nRemember that this case study is created in Rmarkdown and that code goes within “chunks”, which is different from a standard R script. The first steps will be to define the language in which you want the report, the default chunk options and to install/load the necessary packages.\n\nStep 1.1: Define R language\nDepending on where you are and how to carried out R installation, your language “locale” might be different from the language of the report that you want to produce. For example, a french person might have a french “locale”. If that is the case, when creating a graph by day of the week, Monday will be displayed as “lundi”. If that french person wants to create an English report, as for this case study, the language “locale” should be changed.\nTask: Ensure your “locale” is in English and change it into English if it is not.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\n\nStep 1.2: Default chunk options\nChange the default chunk options of your Rmarkdown script to:\n\nhide all code chunks in the report\ndo not show messages or warnings\nshow errors if they appear, but to not stop the rendering\nset up the default figure width to 7 and the figure height to 6\nto show the figure titles on top of the plots by default\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# hide all code chunks in the output, but show errors \nknitr::opts_chunk$set(echo = FALSE,  # hide all code chunks in output\n                      error = TRUE,  # show errors if they appear, but don't stop (produce the word doc)\n                      warning = FALSE, # do not show warnings in the output word doc \n                      message = FALSE, # do not show  messages in the output word doc\n                      fig.width = 7,         # Figure width\n                      fig.height = 6,        # Figure height\n                      fig.topcaption = TRUE  # show figure titles on top of plot\n                     )\n\n\n\n\n\nStep 1.3: Install/load packages\nInstall the following packages that will be needed to carry out the analysis: officedown, officer, rio, here, skimr, janitor, lubridate, epikit, tidyverse, flextable, sf, scales, gtsummary, labelled, ggspatial, patchwork, apyramid and incidence2.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  officedown, # format MS word document output\n  officer,    # add table of contents to output\n  rio,        # importing data  \n  here,       # relative file pathways \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # age_categories() function\n  flextable,  # converting tables to pretty images\n  sf,         # manage spatial data using a Simple Feature format\n  scales,     # define colour schemes for flextables \n  gtsummary,  # summary statistics, tests and regressions \n  labelled,   # create variable labels to be displayed in table outputs\n  ggspatial,  # basemaps and scalebars \n  patchwork,  # combining multiple ggplots \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n\n)"
  },
  {
    "objectID": "pages/fulton.html#step-2-data-import-and-exploration",
    "href": "pages/fulton.html#step-2-data-import-and-exploration",
    "title": "Fulton (EN)",
    "section": "Step 2: Data import and exploration",
    "text": "Step 2: Data import and exploration\n\nStep 2.1: Data import\n\nImport the COVID-19 linelist called covid_example_data.xlsx that can be found in the following path: data/covid_example_data/.\nImport also the csv files named fulton_population.csv found in data/covid_example_data needed to retrieve the population in Fulton County.\n\n\n\nClick to see a solution code (try it yourself first!)\n\n\n\nlinelist_raw &lt;- rio::import(\n  file = here::here(\"data\", \"covid_example_data\", \"covid_example_data.xlsx\"),\n  which = \"in\"\n)\n\n# import population data by zipcode to calculate incidence\npop &lt;- import(\n     here(\"data\", \"covid_example_data\", \"fulton_population.csv\")\n)\n\n\n\n\n\nStep 2.2: Data exploration\nExplore the linelist to understand better the data.\n\nQuestion 2.1: How many rows are present in linelist_raw?\n\n 48 31 82101 5\n\nQuestion 2.2: How many columns are of class numeric?\n\n 8 4 19 31\n\n\n\n\nClick to see a solution code (try it yourself first!)\n\n\n\n# view your whole dataset interactively (in an excel style format)\nView(linelist_raw)\n\n# get mean, median and max values of numeric variables; counts for categorical variables and NAs with summary\nsummary(linelist_raw)\n\n# get information about each variable in a dataset \nskim(linelist_raw)\n\n# view unique values contained in variables - useful for categorical variables\nunique(linelist_raw$case_gender)"
  },
  {
    "objectID": "pages/fulton.html#step-3-data-cleaning",
    "href": "pages/fulton.html#step-3-data-cleaning",
    "title": "Fulton (EN)",
    "section": "Step 3: Data cleaning",
    "text": "Step 3: Data cleaning\n\nStep 3.1: Create date objects\nCreate an object called surveillance_date defined as 7 days prior to the reporting date (30 June 2021). Then, create another object rounding it to the closest Wednesday. Create two daily sequences of dates, one as the 14 days prior to the surveillance_date and another as 14-28 days prior to the same date. We will use these throughout the case study\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# create a date object for the surveillance\n# Minus 7 days from the date of report (see YAML) to account for lag in reporting lab results\nsurveillance_date &lt;- as.Date(\"2021-06-30\") - 7\n\n# create an epiweek object from the date \n# floor_date() rounds down to the closest week here\nsurveillance_week &lt;- floor_date(surveillance_date,\n                          # round by weeks\n                          unit = \"week\", \n                          # define week to start on Wednesday\n                          week_start = 3)\n\n# define recent (past 14 days) and previous (28 to 14 days prior)\nrecent_period   &lt;- seq(surveillance_week  - 13, surveillance_week, by = 1)\nprevious_period &lt;- seq(surveillance_week  - 27, surveillance_week - 14, by = 1)\n\n# define a text label of date range for the recent period (for table headers)\nrecent_period_labels &lt;- str_glue(\n  format(min(recent_period), format = \"%m/%d\"), \n  \"-\", \n  format(max(recent_period), format = \"%m/%d\")\n)\n\n# define text label of date range for previous period (for table headers) \nprevious_period_labels &lt;- str_glue(\n  format(min(previous_period), format = \"%m/%d\"), \n  \"-\", \n  format(max(previous_period), format = \"%m/%d\")\n)\n\n\n# define a label for past 28 days (for table captions)\nfull_period_labels &lt;- str_glue(\n  format(min(previous_period), format = \"%B %d\"), \n  \"-\", \n  format(surveillance_week, format = \"%B %d, %Y\")\n)\n\n\n\n\n\nStep 3.2: Clean column names\nClean the column names ensuring that names do not contain special characters. Rename the following columns from the raw data:\n\nDate of report (reprt_creationdt_FALSE) to date_report\nDate of birth (case_dob_FALSE) to date_dob\nDate of symptom onset (sym_startdt_FALSE) to date_onset\nDate of positive testing (pos_sampledt_FALSE) to date_positive\nDate of recovery (sym_resolveddt_FALSE) to date_recovery\nDate of hospitalisation (hosp_admidt_FALSE) to date_hospitalized\nDate of discharge (hosp_dischdt_FALSE) to date_discharge\nDate of death (died_dt_FALSE) to date_died\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist_raw %&gt;% \n     clean_names() %&gt;% \n     # NEW name = OLD name\n  rename( \n    date_report         = reprt_creationdt_false,      \n    date_dob            = case_dob_false,              \n    date_onset          = sym_startdt_false,\n    date_recovery       = sym_resolveddt_false, \n    date_hospitalized   = hosp_admidt_false,\n    date_discharge      = hosp_dischdt_false,\n    date_died           = died_dt_false,\n    date_positive       = pos_sampledt_false\n    )\n\n\n\n\n\nStep 3.3: Remove duplicated rows\nRemove rows that have duplicated information on: patient id, gender and date of birth. Keep duplicates in a separate dataframe.\n\n\n Click to read a hint\n\n\nTo store duplicates in a new dataframe you can use the function get_dupes() from the {janitor} package\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# get a data frame of all the duplicates. This is mostly to inspect manually, but can be used for analysing those dropped\nduplicates &lt;- linelist %&gt;% \n     get_dupes(pid, case_gender, date_dob)\n\n# find duplicates based on unique ID, gender and date of birth. Only keep the first occurrence \nlinelist &lt;- linelist %&gt;% \n  distinct(pid, case_gender, date_dob, .keep_all = TRUE)\n\n\n\n\nQuestion 3.2: How many duplicated rows were present in the raw data?\n\n 28 31 38 124\n\n\n\n\nStep 3.4: Change column class and remove data inconsistencies\nUsing the across() function from {dplyr} make the following:\n\nEnsure that dates are considered dates by R\nClean date columns dealing with values that are not compatible with the period under analysis (early 2020 to July 2021)\nMake the column age of numeric class\nSet us NA those with negative ages and missing Date of birth\nMake the zip code column a factor class column\n\n\n\n Click to read a hint\n\n\nThe across() allows to apply the same modification to multiple columns in an easy way. So, these two options are equivalent:\n\n# Without across()\n\nlinelist &lt;- linelist %&gt;% \n  mutate(date_report = as.Date(date_report)) %&gt;% \n  mutate(date_dob = as.Date(date_dob)) %&gt;% \n  mutate(date_onset = as.Date(date_onset)) %&gt;% \n  mutate(date_hospitalized = as.Date(date_hospitalized)) %&gt;% \n  mutate(date_discharge = as.Date(date_discharge)) %&gt;% \n  mutate(date_died = as.Date(date_died)) %&gt;% \n  mutate(date_positive = as.Date(date_positive))\n\n\n# With across()\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(.cols = contains(\"date\"), .fns = ~as.Date(.x)))\n\nYou can read more about across() in the EpiRhandbook section\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n  mutate(across(\n    .cols = contains(\"date\"),\n    .fns = ~as.Date(.x)\n  )) %&gt;%\n  \n  # mark as missing onset dates prior to 2020\n  mutate(across(\n    .cols = c(date_report, date_onset, date_hospitalized, date_discharge, date_died),\n    .fns  = ~replace(.x, .x &lt; as.Date(\"2020-01-01\"), NA)\n    )) %&gt;% \n\n  # mark as missing dates after the surveillance_date (for this report) from all date columns\n  mutate(across(\n    .cols = contains(\"date\"),\n    .fns  =  ~replace(.x, .x &gt; surveillance_date, NA)\n    )) %&gt;%\n     \n  # transform age into numeric class\n  mutate(\n    # ensure that age is a numeric variable\n    case_age = as.numeric(case_age),\n    # set those with negative ages and missing DOB to missing \n    # otherwise just leave the age value as is\n          # nb. NA_real_ just ensures the variable class is not changed\n    case_age = if_else(case_age &lt; 0 & is.na(date_dob), NA_real_, case_age)\n  ) %&gt;% \n     \n  # create a factor from a default numeric class\n  mutate(case_zip = as_factor(case_zip)) \n\n\n\n\nQuestion 3.3: Which one of the following could NOT be used to transform the column sym_startdt_FALSE from the raw data frame into a date object?\n\n base::as.Date() lubridate::as_date() lubridate::ymd() lubridate::dmy()\n\n\n\n\nStep 3.5: Create a column for weeks\nCreate a column named “epiweek” using the function floor_date() from the {lubridate} package rounding the report date to the nearest week, taking “Wednesday” as the start of the week.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n  # create an \"epiweek\" column from the report date. Use floor_date() to round down to the closest week\n  mutate(epiweek = floor_date(date_report,\n                          # round by weeks\n                          unit = \"week\", \n                          # define week to start on Wednesday\n                          week_start = 3)\n  )\n\n\n\n\n\nStep 3.6: Create time difference columns\nIn this step we ask you to create columns with various time differences that will be used later on in the case study. Please, try to create:\n\nA column with the number (numeric) of days from date of symptom onset to the date of hospitalization\nIn this new column, set as missing those cases where the difference is longer than 30 days (interval is too long for the hospitalization to be due to the infection), and those less than 0 (cannot be hospitalized before the symptom onset)\nUsing the function coalesce() from {dplyr} create a new column for the date of outcome among hospitalized cases, using date of death or date of discharge, depending on whether cases died or not\nCreate a new column with the length of hospitalization in days, calculated as the time difference between date of hospitalization and the recently created date of outcome.\nIn this newly created column mark as missing cases in which the difference between the date of hospitalization and the date of death/discharge was longer than 60 days or lower than 0 days\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;%\n     \n  # delay from onset to hospitalization\n  mutate(\n    # calculate time differences\n    days_onset_hosp = as.numeric(date_hospitalized - date_onset),\n    # set those under 0 or over 30 to missing\n    days_onset_hosp = replace(days_onset_hosp, days_onset_hosp &lt; 0, NA),\n    days_onset_hosp = replace(days_onset_hosp, days_onset_hosp &gt; 30, NA)\n  ) %&gt;%\n     \n  # length of hospitalization\n  mutate(\n    # create outcome date based on whether died or was discharged\n    date_outcome = coalesce(date_died, date_discharge),\n    # calculate time difference\n    days_hosp = as.numeric(date_outcome - date_hospitalized),\n    # set those under 0 or over 60 to missing\n    days_hosp = replace(days_hosp, days_hosp &lt; 0, NA),\n    days_hosp = replace(days_hosp, days_hosp &gt; 60, NA)\n  )\n\n\n\n\n\nStep 3.7: Create age groups\nCreate a column with 10 year age groups up until 70 (and 70+ afterwards) using the age_group() function from the package {epikit}. You can also use any other alternative\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;%\n     # create age group variable\n     mutate(\n       age_group = age_categories(case_age,\n        # define break points\n        c(0, 10, 20, 30, 40, 50, 60, 70),\n        # whether last break should be highest category\n        ceiling = FALSE\n     ))\n\n\n\n\n\nStep 3.8: Recode character/categorical columns\nRecode the following columns:\n\nIn the column named died_covid recode the category “Under Review” to “Unknown”\nIn the column named confirmed_case recode the category “Pending” to “Unknown”\nForce categorical columns to use consistent cases\nAcross character/factor columns recode the category “Unk” to “Unknown”\nAcross the different character/factor columns recode NA to “Unknown”\nIn the column named sym_resolved recode categories into “Yes”, “No” or “Unknown”\nTransform the gender column into a factor with these levels: “Female”, “Male” and “Unknown”\nTransform all columns that have the categories: “Yes”, “No” and “Unknown” into factors with the order of the levels as “Yes”, “No” and “Unknown”\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n     \n     # recode one value and leave the rest as they are \n     mutate(\n       died_covid = if_else(died_covid == \"Under Review\",\n                            \"Unknown\", died_covid), \n       confirmed_case = if_else(confirmed_case == \"Pending\", \n                                \"Unknown\", confirmed_case), \n     \n        # force categorical variables to use consistent cases (this can be done for others) \n        sym_myalgia = str_to_title(sym_myalgia),\n      ) %&gt;% \n     \n     #replace one value and leave the rest, across multiple variables\n      mutate(across(\n       .cols = c(contact_household, contains(\"sym_\")),\n       .fns  = ~if_else(.x == \"Unk\", \"Unknown\", .x)\n     )) %&gt;% \n     \n        # replace missing with \"Unknown\" where relevant \n     mutate(across(\n       .cols = c(case_gender, case_race, case_eth, case_zip,\n                 contact_id, contact_household, \n                 hospitalized, died, died_covid, confirmed_case,\n                 contains(\"sym_\"), age_group),\n       .fns  = ~fct_na_value_to_level(.x, level = \"Unknown\")\n     )) %&gt;% \n     \n          # recode with searching for string patterns \n     mutate(sym_resolved = case_when(\n          str_detect(sym_resolved, \"Yes\")     ~ \"Yes\", \n          str_detect(sym_resolved, \"No\")      ~ \"No\", \n          str_detect(sym_resolved, \"Unknown\") ~ \"Unknown\", \n          TRUE                                ~ \"Unknown\"\n     )) %&gt;% \n     \n      # set levels of a factor (define order)\n     mutate(case_gender      = fct_relevel(case_gender, \"Female\", \"Male\", \"Unknown\")) %&gt;% \n     \n          # set levels of all factors that are yes/no/unknown \n     mutate(across(\n          .cols = c(contact_household, hospitalized, died, died_covid,\n                    confirmed_case, contains(\"sym_\")), \n          .fns = ~fct_relevel(.x, \"Yes\", \"No\", \"Unknown\")\n     )) \n\n\n\n\n\nStep 3.9: Merge ethnicity and race\nThe linelist contains a column for ethnicity (case_eth) and a column for race (case_race). Create a new column merging information from these two existing columns. The new column should:\n\nContain a category “Hispanic, all races” when case_eth is “HISPANIC/LATINO”. For those cases where this condition is not met:\n\nShould have a category for those whose race is “Asian”, another for those whose race is “Black” and another for those whose race is “White”.\nCreate an “Other” category for the rest of races and an “Unknown” category for those with missing race\nEnsure all categories have consistent cases\n\nTransform the newly formed column into a factor with the “Unknown” category as the last level\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlinelist &lt;- linelist %&gt;% \n          # create a composite category from race and ethnicitiy  \n     mutate(eth_race = case_when(\n          eth  == \"HISPANIC/LATINO\"                           ~ \"Hispanic, all races\", \n          race == \"ASIAN\"                                     ~ \"Asian, NH\", \n          race == \"BLACK\"                                     ~ \"Black, NH\",\n          race == \"WHITE\"                                     ~ \"White, NH\",\n      # find all instances of NATIVE (covers AMERICAN INDIAN/ALASKA NATIVE **AND** NATIVE HAWAIIAN/PACIFIC ISLANDER)\n          str_detect(race, \"NATIVE\")                          ~ \"Other, NH\",\n          race == \"OTHER\"                                     ~ \"Other, NH\", \n          TRUE                                                ~ \"Unknown\"\n     )) %&gt;% \n     mutate(eth_race = factor(eth_race, levels=c(\n          \"Black, NH\", \"White, NH\", \"Hispanic, all races\",\n          \"Asian, NH\", \"Other, NH\", \"Unknown\"\n     )))\n\n\n\n\nQuestion 3.4: A column that has ordinal data, what class should it have?\n\n logical character factor integer\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n\n\nStep 3.10: Filter data frame\nFilter the data to keep only confirmed cases whose date of report is not above the date of the report (June 30, 2021). Consider also keeping records with missing date of report.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n##############       FILTER     ##############        \n\n# store those which do not meet our filter criteria \ndropped &lt;- linelist %&gt;% \n     filter(confirmed_case != \"Yes\" |\n              date_report &gt; surveillance_date & \n                !is.na(date_report))\n\n\n# drop the cases that dont meet the criteria \nlinelist &lt;- linelist %&gt;% \n     filter(confirmed_case == \"Yes\" & \n              date_report &lt;= surveillance_date & \n                 !is.na(date_report))"
  },
  {
    "objectID": "pages/fulton.html#step-4-start-the-report-with-a-summary-of-the-findings",
    "href": "pages/fulton.html#step-4-start-the-report-with-a-summary-of-the-findings",
    "title": "Fulton (EN)",
    "section": "Step 4: Start the report with a summary of the findings",
    "text": "Step 4: Start the report with a summary of the findings\n\nWrite in rmarkdown three bullet points summarising the data we imported, showing the number of cases by the date of analysis, the number of hospitalisations and the number of deaths.\nWrite it in a dynamic way, so that the dates and numbers are updated automatically if you get a new updated dataset\n\n\n\nClick to see a solution (try it yourself first!)\n\n This is an example of how the code should look like in your rmarkdown file:"
  },
  {
    "objectID": "pages/fulton.html#step-5.-analysis-by-time",
    "href": "pages/fulton.html#step-5.-analysis-by-time",
    "title": "Fulton (EN)",
    "section": "Step 5. Analysis by time",
    "text": "Step 5. Analysis by time\n\nStep 5.1: Table weekly number of cases\nCreate a table with the number of cases per reporting week to see how the epidemic evolved by time in Fulton County\n\nQuestion 5.1: During which week do we observe the peak in cases by date of reporting?\n\n The week starting on March 02, 2021 The week starting on December 16, 2020 The week starting on January 13, 2021 The week starting on December 30, 2020\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# save a quick descriptive table of number of cases reported by week\nepiweek_table &lt;- linelist %&gt;% \n  # get counts and percentages \n  tabyl(epiweek) %&gt;% \n  # add the overall counts as a row\n  adorn_totals() %&gt;%  \n  # change from proportions to percentages (do not add a % sign)\n  adorn_pct_formatting(affix_sign = FALSE) \n\n# transform it into flextable for better visualisation\nepiweek_flextable &lt;- epiweek_table %&gt;% \n     qflextable()\n\n\n\n\n\nStep 5.2: Epicurve\nCreate an epicurve by reporting week, with the colour of the bins based on whether the cases were hospitalised or not\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n     # we first define the dataset to be used, the x axis which will be reporting week and the colour (fill) of the bins which will depend on hospitalisation outcome\nggplot(\n     data = linelist,\n     mapping = aes(\n          x = epiweek,\n          fill = hospitalized\n     )) + \n     \n     geom_histogram() + \n     \n     # we define that we want breaks by month and formated with scales::label_date_short()\n     scale_x_date(\n          date_breaks = \"month\",\n          labels = label_date_short()\n     ) +\n     \n     # we change the name of the different elements of the graph\n     labs(\n          x = \"\",\n          y = \"Weekly number of cases\",\n          fill = \"Hospitalised\",\n          caption = paste0(\"Data as of \", format(surveillance_date, \"%d %b %Y\"))\n          \n     ) + \n     \n     # we apply one of the predefined themes\n     theme_bw()"
  },
  {
    "objectID": "pages/fulton.html#step-6.-analysis-by-person",
    "href": "pages/fulton.html#step-6.-analysis-by-person",
    "title": "Fulton (EN)",
    "section": "Step 6. Analysis by person",
    "text": "Step 6. Analysis by person\n\nStep 6.1: Table with demographic information\nCreate a table summarising, with counts and percentages, the total cumulative number of cases and deaths, as well the cases and deaths notified in the last 28 days by demographic characteristics: sex, age and race.\n\nQuestion 6.1: In which age group do we observe the largest proportion of cumulative cases?\n\n 0-9 30-39 20-29 70+\n\nQuestion 6.2: In which race do we observe the largest proportion of deaths in the last 28 days?\n\n Black White Asian Hispanic\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# get counts tables for measures of interest \n############################################\n\n# we generate 3 summary tables and bind them together\n# summary demographic table for gender\ndem_gender &lt;- linelist %&gt;% \n  tabyl(gender) %&gt;% \n  select(Characteristic = gender, n, percent)\n\n# summary demographic table for age\ndem_age &lt;- linelist %&gt;% \n  tabyl(age_group) %&gt;% \n  select(Characteristic = age_group, n, percent)\n\n# summary demographic table for ethnicity and race\ndem_eth_race &lt;- linelist %&gt;% \n  tabyl(eth_race) %&gt;% \n  select(Characteristic = eth_race, n, percent)\n\n# bind all tables together\ntotal_cases &lt;- bind_rows(list(dem_gender, dem_age, dem_eth_race))\n\n# counts of new cases (last 28 days) \nrecent_cases &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter the linelist for dates on or after 28 days ago\n  .f = ~filter(linelist, \n          date_report &gt;= (surveillance_date - 28)) %&gt;% \n        # get counts based on filtered data\n        tabyl(.x) %&gt;% \n        # nb we dont keep the characteristic column because it would be duplicated\n        select(n_cases_recent = n,\n               perc_cases_recent = percent)\n  ) %&gt;%\n  bind_rows()\n\n# counts of total deaths \ntotal_deaths &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter for those who died \n  .f = ~filter(linelist, \n          died_covid == \"Yes\") %&gt;% \n        # get counts based on filtered data \n        tabyl(.x, show_na = TRUE) %&gt;%\n        select(n_deaths_total = n, perc_deaths_total = percent)\n  ) %&gt;% \n  bind_rows()\n\n# counts of new deaths (last 28 days)\nrecent_deaths &lt;- purrr::map(\n  # for each variable listed\n  .x = demographic_vars, \n  # filter to those who died in the last 28 days\n  .f = ~filter(linelist, \n          died_covid == \"Yes\" & \n          date_died &gt;= (surveillance_date - 28)) %&gt;% \n        # get counts based on filtered data\n        tabyl(.x) %&gt;% \n        select(n_deaths_recent = n, perc_deaths_recent = percent) %&gt;% \n        # add in a variable column (used for colouring later) \n        mutate(variable = .x)\n  ) %&gt;% \n  bind_rows()\n\n\n# total counts for all of the above measures (not by demographic)\noverall &lt;- linelist %&gt;% \n  summarise(\n    # add in row label \n    Characteristic = \"Total\",\n    # counts of total cases \n    n_cases_total = n(),\n    # leave all percentages empty (would just be 100)\n    perc_cases_total  = NA, \n    # counts of new cases (last 28 days) \n    n_cases_recent = sum(date_report &gt;= (surveillance_date - 28)), \n    perc_cases_recent  = NA, \n    # counts of total deaths \n    n_deaths_total = sum(died_covid == \"Yes\"), \n    perc_deaths_total = NA, \n    # counts of new deaths (last 28 days)\n    n_deaths_recent = sum(died_covid == \"Yes\" & \n                          date_died &gt;= (surveillance_date - 28)),\n    perc_deaths_recent = NA, \n    # add in a variable column (used for colouring later) \n    variable = \"Overall\"\n  )\n\n\n# merge tables together \n#######################\n\n# combine all the demographic tables - side by side\ndemographics_counts &lt;- bind_cols(total_cases, recent_cases, total_deaths, recent_deaths) %&gt;% \n  # mutate each of the proportion columns to be percentages\n  mutate(across(\n    .cols = contains(\"perc\"),\n    .fns = ~round(.x * 100, digits = 1)\n    )) \n# add in the totals row at the top of the merged demographics table\ndemographics_counts &lt;- bind_rows(overall, demographics_counts)\n\n\n# define colour scheme \n######################\n\n# get the column numbers that are percentages (based on the name) \npercentage_cols &lt;- names(demographics_counts) %&gt;% \n  str_detect(\"perc\") %&gt;% \n  which()\n\n# define colour cut-offs for gender column \ngender_colours &lt;- scales::col_bin(\n  # choose colours \n  palette = c(\"#91CF60\", \"#FC8D59\"), \n  # choose min and max (range)\n  domain  = c(0, 100),\n  # choose how to split (in this case above and below 50)\n  bins    = 2\n)\n\n# define colour cut-offs for age column \nage_colours &lt;- scales::col_bin(\n  # choose colours\n  palette = c(\"#91CF60\",\"#FFFFBF\", \"#FC8D59\"),\n  # choose min and max (range)\n  domain  = c(0, 100), \n  # choose cut-off categories \n  bins    = c(0, 5, 20, 100)\n)\n\n# define colour cut-offs for ethnicity column \neth_colours &lt;- scales::col_bin(\n  palette = c(\"#91CF60\",\"#FFFFBF\", \"#FC8D59\"),\n  domain  = c(0, 100), \n  bins    = c(0, 10, 40, 100)\n)\n\n\n# create styled table  \n######################\n\ndemographics_counts %&gt;%\n  # initiate flextable to produce styled output table\n  flextable(\n    # retain variable column for formatting but do not display it\n    col_keys = names(demographics_counts)[-10]\n  ) %&gt;%\n  # redefine column names based on original names\n  set_header_labels(\n    \"n_cases_total\"       = \"Total Confirmed Cases\",\n    \"perc_cases_total\" = \"% of Total Cases\",\n    \"n_cases_recent\"       = \"Confirmed Cases past 28 days\",\n    \"perc_cases_recent\" = \"% of Confirmed Cases past 28 days\",\n    \"n_deaths_total\"       = \"Total Confirmed Deaths\",\n    \"perc_deaths_total\" = \"% of Total Deaths\",\n    \"n_deaths_recent\"       = \"Confirmed Deaths past 28 days\",\n    \"perc_deaths_recent\" = \"% of Confirmed Deaths past 28 days\"\n  ) %&gt;%\n  # move the header text to the centre\n  align(align = \"center\", part = \"header\") %&gt;%\n  # make header text bold\n  bold(part = \"header\") %&gt;%\n  # make the totals row bold (i.e. first row)\n  bold(i = 1, part = \"body\") %&gt;%\n  # fill in the cells\n  # choose the rows with gender counts\n  bg(i = ~variable == \"gender\",\n     # choose the columns with percentages in them\n     j = percentage_cols,\n     # fill in based on previous defined cut-offs\n     bg = gender_colours) %&gt;%\n  bg(i = ~variable == \"age_group\",\n     j = percentage_cols, bg = age_colours) %&gt;%\n  bg(i = ~variable == \"eth_race\",\n     j = percentage_cols, bg = eth_colours) %&gt;%\n  # add horizontal lines after the cells with totals and unknowns\n    # (short-cut to find row ending of each demographic variable)\n  hline(i = ~Characteristic %in% c(\"Total\", \"Unknown\")) %&gt;%\n  # add in footnotes for rows counting unknowns (reference in first column)\n  footnote(i = ~Characteristic == \"Unknown\", j = 1, part = \"body\", ref_symbols = c(\"a\"),\n           value = as_paragraph(\"Unknown includes cases not yet interviewed\")) %&gt;%\n  # add in footnote for deaths counts (ref in the header)\n  footnote(i = 1, j = c(6, 8), part = \"header\", ref_symbols = c(\"b\"),\n           value = as_paragraph(\"Deaths refer to all persons who had a positive PCR test result\n                                for Covid-19 and there is evidence that COVID-19 was the cause of\n                                death or a significant contributor to their death.\")) %&gt;%\n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\") %&gt;% \n  # decrease the fontsize in the header and body for aesthetic purposes in the document\n  fontsize(part = \"all\", size = 8)\n\n\n\n\n\nStep 6.2: Age pyramid\nCreate an age pyramid with the percentage of cases by age group and sex.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# prepare dataset\n\n# start a new dataframe (as dont want to overwrite the original)\nlinelist_2g &lt;- linelist %&gt;% \n  # update the gender and age_group columns\n  mutate(across(.cols = c(gender, age_group), \n                .fns = ~{\n                  # replace \"Unknown\" with NA\n                  .x = na_if(.x, \"Unknown\") \n                  # drop \"Unknown\" from the factor levels \n                  .x = fct_drop(.x)\n                }))\n\n# plot age pyramid \nage_pyramid(\n  data = linelist_2g,\n  age_group = \"age_group\",\n  split_by = \"gender\",\n  # Show as percentages of total cases\n  proportional = TRUE,\n  # remove guide line for mid-point\n  show_midpoint = FALSE) +\n  # set theme to basic \n  theme_minimal() +\n  # add labels \n  labs(\n    title = \"\",\n    subtitle = ,\n    x = \"Age group\",\n    y = \"Percent of total\",\n    fill = \"Gender\",\n    # use str_glue to set dynamic captions \n    # {missing} is defined in the second argument below\n    caption = str_glue(\n      \"{missing} cases missing either age or gender are not shown. \\n Fictional COVID-19 data\",\n      missing = linelist_2g %&gt;%\n        filter(is.na(gender) | is.na(age_group)) %&gt;%\n        nrow()\n      )\n    )\n\n\n\n\n\nStep 6.3: Scatter plot\nCreate a scatter plot showing the relation between age and duration of hospital stay. Colour the points based on whether cases died or not.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#################### C) SCATTER PLOT ####################\n# open a plot with the linelist data\nggplot(data = linelist) +\n  # add points \n  geom_point(\n    mapping = aes(\n      # plot age on the x and days hospitalised on the y axis \n      x = age,\n      y = days_hosp,\n      # color points by outcome\n      color = died),  \n    # all points 3x size\n    size = 3, \n    # opacity of 30% (i.e. relatively see-through)\n    alpha = 0.3) +      \n  # make the x and y axes start at the origin \n  scale_y_continuous(expand = c(0, 0)) + \n  scale_x_continuous(expand = c(0, 0)) + \n  # add in labels \n  labs(\n    x = \"Age (years)\",\n    y = \"Duration (days)\",\n    caption = \"Fulton COVID-19 data\",\n    color = \"Deceased\"\n    ) + \n     theme_bw()\n\n\n\n\n\nStep 6.4: Bar plot\nCreate a bar stacked bar plot showing the absolute number of cases by race and vital status\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# open a plot with the linelist data\nggplot(linelist) +\n  # add bars \n  geom_bar(\n    mapping = aes(\n      # plot the number of cases by ethnicity (ordered in reverse frequency)\n      x = fct_rev(fct_infreq(eth_race)),\n      # stack bars and colour by died (ordered in reverse frequency)\n      fill = fct_rev(fct_infreq(died))\n    )\n  ) +\n  # flip the x and y axes \n  coord_flip() +\n  # make the x axes start at the origin (nb axes flipped)\n  scale_y_continuous(expand = c(0, 0), \n                     # define where to label xaxis (nb axes flipped )\n                     breaks = seq(from = 0,\n                                  to = 35000,\n                                  by = 5000)) + \n  # add in labels \n  labs(\n    # set the axes titles (nb axes flipped)\n    x = \"Race and Ethnicity\",\n    y = \"Cases (n)\",\n    caption = \"Fictional COVID-19 data\",\n    fill = \"Deceased\"\n    ) + \n  # apply a defined theme\n     theme_bw()"
  },
  {
    "objectID": "pages/fulton.html#step-7.-analysis-by-place",
    "href": "pages/fulton.html#step-7.-analysis-by-place",
    "title": "Fulton (EN)",
    "section": "Step 7. Analysis by place",
    "text": "Step 7. Analysis by place\nCreate a table by zip code in which you show the incidence in the most recent 14 days period, the incidence in the previous 14 days period and the percentage change in incidence between these periods.\n\nQuestion 7.1: What is the change in incidence observed between periods in the zip code number 30337?\n\n +20% +36% -62.5% -25%\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n################### TABLE BY ZIP CODE\n\nzip_counts &lt;- linelist %&gt;% \n  group_by(zip) %&gt;% \n  # count cases in the appropriate period \n  summarise(\n    recent   = sum(date_report %in% recent_period),\n    previous = sum(date_report %in% previous_period)\n  ) %&gt;% \n  adorn_totals() %&gt;% \n  # a percentage change column and round the digits\n  mutate(\n    perc_change = round((recent - previous) / previous * 100, digits = 1)\n    )\n\n# extract population counts for each zip from the shapefile\nzip_pop &lt;- shapefile %&gt;% \n  # change to tibble (otherwise geo-data gets pulled with)\n  as_tibble() %&gt;% \n  # only keep zip code and population counts\n  select(ZipCode, Population) %&gt;% \n  # add a row with overall counts\n  adorn_totals()\n  \n# merge case counts and population counts\n# zip (or ZipCode in the shapefile) variable is the unique identifier\nzip_counts &lt;- left_join(zip_counts, \n                        zip_pop, \n                        by = c(\"zip\" = \"ZipCode\")\n                        ) %&gt;% \n  # calculate the incidence \n  mutate(across(\n      # for each period (recent and previous)\n      .cols = c(recent, previous), \n      # divide each variable by population (and round the outcome)\n      .fns = ~round(.x / Population * 10000, digits = 1), \n      # for each period create a new variable with _inc on the end\n      .names = \"{.col}_inc\"), \n    \n    # replace NAs in incidence with 0\n    across(\n      .cols = contains(\"inc\"),\n      .fns = ~replace_na(.x, 0)),\n    \n    perc_change = case_when(\n      # fix the outliers: set missing to 0 and infinity (divided by 0) to 100\n      is.na(perc_change)       ~ 0,\n      is.infinite(perc_change) ~ 100, \n      TRUE                     ~ perc_change\n    ))\n\n\n# choose colours to fill in cells  \nrow_colour &lt;- case_when(\n  # those less than zero will be green (decreasing cases)\n  zip_counts$perc_change &lt; 0 ~ \"#91CF60\", \n  # over zero red (increasing)\n  zip_counts$perc_change &gt; 0 ~ \"#FC8D59\", \n  # missing or zero orange\n  TRUE                       ~ \"#FFFFBF\")\n\n\nzip_counts %&gt;% \n  # keep the columns of interest and define order\n  select(zip, recent, recent_inc, previous, previous_inc, perc_change) %&gt;% \n  # initiate {flextable} to produce styled output table\n  flextable() %&gt;% \n  # fill in cells - choose the column and then pass our colour-scheme defined above\n  bg(j = \"perc_change\", \n     bg = row_colour\n     ) %&gt;% \n  # add in a header for labeling counts and incidence by period \n    # note the empty columns (\"\") to fit to the original table headers\n  add_header_row(\n    values = c(\"\", \n               str_c(\"Recent 14-day reporting period\\n\", recent_period_labels), \n               \"\", \n               str_c(\"Previous 14-day reporting period\\n\", previous_period_labels), \n               \"\", \n               \"Change between reporting periods\"\n               )) %&gt;% \n  # redefine column names based on original names\n    # note the different syntax to dplyr::select, here it is old_name = new_name\n  set_header_labels(\n    zip          = \"Zip Code\", \n    recent       = \"n\", \n    recent_inc   = \"Incidence\", \n    previous     = \"n\", \n    previous_inc = \"Incidence\", \n    perc_change  = \"%\"\n  ) %&gt;% \n  # combine the headers cells for the appropriate periods \n  # (i defines rows, j defines columns)\n  merge_at(i = 1, j = 2:3, part = \"header\") %&gt;% \n  merge_at(i = 1, j = 4:5, part = \"header\") %&gt;% \n  # move the header text to the centre\n  align(align = \"center\", part = \"header\") %&gt;% \n  # make header text bold \n  bold(part = \"header\") %&gt;% \n  # make the row with totals in it bold (i.e. the last row in the dataframe)\n  bold(i = nrow(zip_counts), part = \"body\") %&gt;% \n  # add in footnotes for variables (referencing the header cells)\n  footnote(j = c(3, 5), part = \"header\", ref_symbols = c(\"a\"),\n           value = as_paragraph(\"Incidence calculated as cases per 10,000 population by zip code\")) %&gt;% \n  footnote(j = 6, part = \"header\", ref_symbols = c(\"b\"),\n           value = as_paragraph(\"These reflect the percentage increase or decrease of new diagnoses \n                                between the 14 days preceding the past 7 days and the 14 days\n                                preceding that.\")) %&gt;% \n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\")"
  },
  {
    "objectID": "pages/fulton.html#step-8.-analysis-of-risk-factors-for-mortality",
    "href": "pages/fulton.html#step-8.-analysis-of-risk-factors-for-mortality",
    "title": "Fulton (EN)",
    "section": "Step 8. Analysis of risk factors for mortality",
    "text": "Step 8. Analysis of risk factors for mortality\n\nCreate a table in which you assess, with the appropriate statistical tests, whether the demographic characteristics of those dying from Covid-19 are significantly different from cases who did not die from it.\nFor each of the variables used in the table that you just created, carry out univariate regression using each demographic variable as the independent variable and the outcome (dead, not dead) as the dependent variables. Create a table with the estimates -alongside 95% CI - of the estimates.\n\n\nQuestion 8.1: According to the results of the univariate analysis, how was having a sore throat associated with mortality from Covid-19\n\n It was a risk factor for mortality It was a protective factor for mortality It was not associated with mortality Impossible to know\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# define a list of variables for looping over later\nsymptom_vars &lt;- linelist %&gt;% \n     # choose all columns that contain \"sym_\" in the name but exclude \"sym_resolved\"\n     select(c(contains(\"sym_\"), -sym_resolved)) %&gt;% \n     # pull the names out \n     names()\n\n# define variables of interest (save typing them out later) \ndescriptive_vars &lt;- c(\"gender\", \n                      \"age_group\",\n                      \"eth_race\",\n                      symptom_vars,\n                      \"hospitalized\",\n                      \"days_hosp\")\n\n# filter dataset  \nrf_data &lt;- linelist %&gt;% \n  # only keep variables of interest\n  select(died_covid, age, all_of(descriptive_vars)) %&gt;% \n  # set unknown back to NA for all factor variables\n  mutate(across(\n    .cols = where(is.factor),\n    .fns = ~fct_recode(.x, NULL = \"Unknown\"))) %&gt;% \n  # flip factor levels (so that the reference values are correct)\n  mutate(eth_race = fct_infreq(eth_race)) %&gt;% \n  mutate(gender = fct_relevel(gender, \"Female\", \"Male\")) %&gt;% \n  mutate(across(all_of(c(\"died_covid\", symptom_vars, \"hospitalized\")), \n                ~fct_relevel(.x, \"No\", \"Yes\")\n                )) %&gt;% \n  # only keep rows with complete data for all variables of interest\n  # note that this will drop rows where **ANY** of the listed variables are NA\n  drop_na(any_of(c(\"died_covid\", \"age\", descriptive_vars)))\n\n\n# define variable labels to show in output tables \nrf_data &lt;- rf_data %&gt;%\n  set_variable_labels(\n    died_covid = \"Died\",\n    age = \"Age (years)\",\n    gender = \"Gender\",\n    age_group = \"Age group (years)\",\n    eth_race = \"Ethnicity\",\n    sym_fever = \"Fever\",\n    sym_subjfever = \"Subjective fever\",\n    sym_myalgia = \"Myalgia\",\n    sym_losstastesmell = \"Loss taste/smell\",\n    sym_sorethroat = \"Sore throat\",\n    sym_cough = \"Cough\",\n    sym_headache = \"Headache\",\n    hospitalized = \"Hospitalized\",\n    days_hosp = \"Days in hospital\"\n  )\n\n\n\nrf_data %&gt;%\n  # keep variables of interest\n  select(died_covid, gender, eth_race, age, days_hosp) %&gt;%\n  # produce summary table and specify grouping variable\n  tbl_summary(\n    by = died_covid\n  ) %&gt;%\n  # specify what test to perform\n  add_p(\n    list(\n      all_continuous() ~ \"kruskal.test\",\n      eth_race ~ \"kruskal.test\",\n      all_dichotomous() ~ \"chisq.test\"\n    )\n  ) %&gt;%\n  # edit what the column headers say (using {gtsummary})\n  # nb. {n} automatically shows the number in that group and \\n is a linebreak\n  modify_header(update = list(\n    stat_1 ~ \"**Dead**\\n (N={n})\",\n    stat_2 ~ \"**Alive**\\n (N={n})\"\n  )) %&gt;%\n  # edit what it says in the footnote (using {gtsummary})\n  modify_footnote(update = list(\n    all_stat_cols() ~ \"n (%) for categorical;\\n median (IQR) for continuous\",\n    p.value ~ \"Pearson's Chi-squared test for dichotomous;\\n Kruskal-Wallis rank sum test for continuous and categorical\"\n  )) %&gt;%\n  # change to flextable format\n  as_flex_table() %&gt;%\n  # make header text bold (using {flextable})\n  bold(part = \"header\")\n\n###################### B) UNIVARIATE REGRESSION ANALYSIS ####################################\n\n\n# produce table with regression estimates\nregress_tab &lt;- rf_data %&gt;%\n  # drop variables not interested in \n  select(-age_group) %&gt;%\n  # produce univariate table\n  tbl_uvregression(\n    # define outcome variable\n    y = died_covid, \n    # define regression want to run (generalised linear model)\n    method = glm, \n    # define what type of glm want to run (logistic)\n    method.args = list(family = binomial), \n    # exponentiate to produce odds ratios (rather than log odds)\n    exponentiate = TRUE, \n    # do not show the overall counts (this is done in cross_tab below)\n    hide_n = TRUE,\n    ## uncomment this line if you want to not show reference rows\n    # show_single_row = c(symptom_vars, gender, hospitalized),\n    ## note: NULL at the end allows you to have a comma before a commented out row\n    NULL\n  )\n\n# produce table with counts by outcome (using the data fed to the regression above)\ncross_tab &lt;- regress_tab$inputs$data %&gt;%\n  tbl_summary(\n    # group by outcome \n    by = died_covid,\n    ## uncomment this line if you only want to show the \"Male\" row for gender\n    ## this would be run if you also uncommented the single_row in regression above\n    # value = list(gender ~\"Male\"),\n    ## show all levels (otherwise only shows the \"Yes\" level)\n    type = list(all_dichotomous() ~ \"categorical\"),\n    ## note: NULL at the end allows you to have a comma before a commented out row\n    NULL\n  )\n\n# combine tables \ntbl_merge(list(cross_tab, regress_tab)) %&gt;%\n  # edit what it says in the grouping headers \n  modify_spanning_header(update = list(\n    c(\"stat_1_1\",\"stat_2_1\") ~ \"Died\",\n    c(\"estimate_2\", \"ci_2\", \"p.value_2\") ~ \"Univariate regression\")\n    ) %&gt;% \n  # edit what it says in the footnote (using {gtsummary})\n  modify_footnote(update = list(\n    all_stat_cols() ~ \"n (%) for categorical;\\n median (IQR) for continuous\")\n    ) %&gt;% \n  # change to flextable format\n  as_flex_table() %&gt;%\n  # make header text bold (using {flextable})\n  bold(part = \"header\") %&gt;% \n  # make your table fit to the maximum width of the word document\n  set_table_properties(layout = \"autofit\")"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Epi Open Case Study Repository",
    "section": "",
    "text": "WEBSITE UNDER CONSTRUCTION\n\n\n\nThis is a draft version of this page. The content and URL will change.\n\n\nHere, you can practice technical and analytical skills with real-life scenarios. The case studies, shared by FETPs and health ministries, use training data that’s either generated or anonymized.\nA case study is a fun, interactive way to learn—guiding you through scenarios with tasks and questions. You’ll find a variety of topics, skill levels, and diseases below, most of which take just a few hours to complete. You can filter and order the rows to pick the one that fits you best. Click the link corresponding to your preferred language to open it up.\nFor more information on how to use the case studies, click the How-to Guide."
  },
  {
    "objectID": "index.html#welcome-to-our-open-case-study-repository",
    "href": "index.html#welcome-to-our-open-case-study-repository",
    "title": "Applied Epi Open Case Study Repository",
    "section": "",
    "text": "WEBSITE UNDER CONSTRUCTION\n\n\n\nThis is a draft version of this page. The content and URL will change.\n\n\nHere, you can practice technical and analytical skills with real-life scenarios. The case studies, shared by FETPs and health ministries, use training data that’s either generated or anonymized.\nA case study is a fun, interactive way to learn—guiding you through scenarios with tasks and questions. You’ll find a variety of topics, skill levels, and diseases below, most of which take just a few hours to complete. You can filter and order the rows to pick the one that fits you best. Click the link corresponding to your preferred language to open it up.\nFor more information on how to use the case studies, click the How-to Guide."
  },
  {
    "objectID": "index.html#about-us",
    "href": "index.html#about-us",
    "title": "Applied Epi Open Case Study Repository",
    "section": "About us",
    "text": "About us\nApplied Epi is a nonprofit organisation and grassroots movement of frontline epidemiologists from around the world. Your encouragement and feedback is most welcome:\n\nVisit our website and join our contact list\ncontact@appliedepi.org, tweet @appliedepi, or LinkedIn\nSubmit issues to our Github repository"
  },
  {
    "objectID": "pages/instructions.html",
    "href": "pages/instructions.html",
    "title": "How-to Guide",
    "section": "",
    "text": "Choose a case study that fits your training needs based on topic, complexity, and language—details are available on the homepage and at the top of each case study page.\nYou can complete the case study individually or in a group. If facilitating, guide the group through the sections. There’s no facilitator’s guide; all learning materials are on the case study page, including the scenario, data download, tasks, and interactive hints and solutions.\n\n\n\nAside from opening up the case study, you need to:\n\nHave the relevant program on your computer, like R or Microsoft Excel.\nOrganize yourself and put any related files in a project folder.\n\nFor projects using R, see more detail in the ‘New to RStudio Projects’? section below.\n\n\n\nThere are several ways to get help:\n\nLook for the hints and solutions. They look like this:\n\n\n\n Click to read a hint\n\nHere you will see a helpful hint!\n\n\n\nClick to see the solution\n\nHere you will see the solution! It will typically be the actual code or technical answer required for the task.\n\n# Example of a solution\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\n\n\nTake a look at the EpiRHandbook\nPost a question in Applied Epi Community with reference to this case study\n\n\n\n\nWe encourage open-source material and sharing for learning purposes. However, case studies will be covered by different licenses. Check the terms of use at the bottom of each case study to see if it is suitable for modification. If you have any questions, send us an email at contact@appliedepi.org.\n\n\n\nAll case studies are based on real or plausible scenarios. Some will use simulated data, while others will feature open-access or provided de-identified data. The type of data used will be clearly indicated at the start of each case study.\n\n\n\nWe recommend that you create an RStudio Project when working R. Check out the details below.\n\n\nIf this is your first time using R and RStudio, welcome! The Epidemiologist R Handbook or EpiRhandbook has a wealth of information to support you along the way. It has everything thing you need to get started with R basics, including installing and updating R and RStudio.\nIf you are new to RStudio, it might be a good idea to spend some time reviewing that page and others before trying out any case studies. Don’t forget that we also have free self-paced R tutorials if you’d like even more guidance and practice.\n\n\n\nOnce you are ready to start your case study project itself, choose a location on your computer to create a dedicated folder. Once you’ve decided on the best spot, set up a folder with the case study’s name to keep everything organized from the start.\nIn your case study folder, you should have a:\n\nsubfolder “scripts” to save any scripts related to the analysis\nsubfolder “data” which will contain the raw data you will use\nsubfolder “outputs” can be used to store any outputs (tables, graphs, documents) that are the result of the analysis\n\nMake sure your folders and subfolders are well-organised as this will save you a headache later!\n\n\n\nCreate an Rstudio project in the case study folder. If you are unsure on how to do that, read the EpiRhandbook on RStudio projects.\n\n\n\nOnce you have created an RStudio project, start a new R script with an appropriate name (example case_study_name) and save it in the subfolder “scripts”.\nIf you are familiar with R markdown, you may decide to use this type of file instead of a standard R script. See below for more instructions on using R markdown.\nNo matter what type of file you choose to use, make sure the purpose, date last updated, and author are written as comments at the top.\n\n#Purpose: To practice new skills using this case study\n#Author: Your Name\n#Date: Mmm dd, yyyy or whatever format you like best\n\n\n\n\nSome of our case studies use R markdown and the code goes within “chunks”, which is different from a standard R script. If you want to create a reproducible workflow, you need a place to save your code so you can run it again if you need to. You want all your files easily organised so you don’t get lost later on. You need to begin by setting the default chunk options.\nTypically, you want to change the default chunk options of your R markdown script to:\n\nhide all code chunks in the report\nnot show messages or warnings in the output\nshow errors if they appear, but to not stop the rendering\nset up the default figure width to 7 and the figure height to 6\nshow the figure titles on top of the plots by default\n\n\n# hide all code chunks in the output, but show errors \nknitr::opts_chunk$set(echo = FALSE,  # hide all code chunks in output\n                      error = TRUE,  # show errors if they appear, but don't stop (produce the word doc)\n                      warning = FALSE, # do not show warnings in the output word doc \n                      message = FALSE, # do not show  messages in the output word doc\n                      fig.width = 7,         # Figure width\n                      fig.height = 6,        # Figure height\n                      fig.topcaption = TRUE  # show figure titles on top of plot\n                     )\n\nBe sure to review Reports with R Markdown in the EpiRhandbook before jumping in!\n\n\n\nDepending on where you are and how you carried out R installation, your language “locale” might be different from the language of the report that you want to produce.\nFor example, a French-speaking person might have a French ‘locale’. If that is the case, when creating a graph by day of the week, “Monday” will be displayed as “lundi”. If that person wants to create an English report, as for this case study, the language ‘locale’ should be changed.\nTo ensure your ‘locale’ is set to English, use the following code:\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\nAt the start of every R project, you will need to install the necessary packages. We do this with the {pacman} package. Its p_load() command will install packages if necessary and load them for use in the current session. If a listed package has already been installed, it will just load it. Each case study specifies at the beginning what packages you need to have installed.\nYou can find more about installing/loading packages in the suggested packages section of the EpiRhandbook.\nExample code to install packages:\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # to create age categories\n  gtsummary,  # summary statistics, tests and regressions \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n)\n\nIf this step is not working, you may have limited administrative rights for your computer. Making sure your IT-department gives you the correct access can save a lot of headache. See these EpiRhandbook pages on the basics of installing packages and running R from network drives (company computers) for more detail."
  },
  {
    "objectID": "pages/instructions.html#how-should-i-use-these-case-studies",
    "href": "pages/instructions.html#how-should-i-use-these-case-studies",
    "title": "How-to Guide",
    "section": "",
    "text": "Choose a case study that fits your training needs based on topic, complexity, and language—details are available on the homepage and at the top of each case study page.\nYou can complete the case study individually or in a group. If facilitating, guide the group through the sections. There’s no facilitator’s guide; all learning materials are on the case study page, including the scenario, data download, tasks, and interactive hints and solutions."
  },
  {
    "objectID": "pages/instructions.html#what-do-i-need-to-complete-a-case-study",
    "href": "pages/instructions.html#what-do-i-need-to-complete-a-case-study",
    "title": "How-to Guide",
    "section": "",
    "text": "Aside from opening up the case study, you need to:\n\nHave the relevant program on your computer, like R or Microsoft Excel.\nOrganize yourself and put any related files in a project folder.\n\nFor projects using R, see more detail in the ‘New to RStudio Projects’? section below."
  },
  {
    "objectID": "pages/instructions.html#how-can-i-get-help",
    "href": "pages/instructions.html#how-can-i-get-help",
    "title": "How-to Guide",
    "section": "",
    "text": "There are several ways to get help:\n\nLook for the hints and solutions. They look like this:\n\n\n\n Click to read a hint\n\nHere you will see a helpful hint!\n\n\n\nClick to see the solution\n\nHere you will see the solution! It will typically be the actual code or technical answer required for the task.\n\n# Example of a solution\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\n\n\nTake a look at the EpiRHandbook\nPost a question in Applied Epi Community with reference to this case study"
  },
  {
    "objectID": "pages/instructions.html#can-i-edit-this-case-study",
    "href": "pages/instructions.html#can-i-edit-this-case-study",
    "title": "How-to Guide",
    "section": "",
    "text": "We encourage open-source material and sharing for learning purposes. However, case studies will be covered by different licenses. Check the terms of use at the bottom of each case study to see if it is suitable for modification. If you have any questions, send us an email at contact@appliedepi.org."
  },
  {
    "objectID": "pages/instructions.html#where-do-the-case-studies-come-from",
    "href": "pages/instructions.html#where-do-the-case-studies-come-from",
    "title": "How-to Guide",
    "section": "",
    "text": "All case studies are based on real or plausible scenarios. Some will use simulated data, while others will feature open-access or provided de-identified data. The type of data used will be clearly indicated at the start of each case study."
  },
  {
    "objectID": "pages/instructions.html#new-to-rstudio-and-rstudio-projects",
    "href": "pages/instructions.html#new-to-rstudio-and-rstudio-projects",
    "title": "How-to Guide",
    "section": "",
    "text": "We recommend that you create an RStudio Project when working R. Check out the details below.\n\n\nIf this is your first time using R and RStudio, welcome! The Epidemiologist R Handbook or EpiRhandbook has a wealth of information to support you along the way. It has everything thing you need to get started with R basics, including installing and updating R and RStudio.\nIf you are new to RStudio, it might be a good idea to spend some time reviewing that page and others before trying out any case studies. Don’t forget that we also have free self-paced R tutorials if you’d like even more guidance and practice.\n\n\n\nOnce you are ready to start your case study project itself, choose a location on your computer to create a dedicated folder. Once you’ve decided on the best spot, set up a folder with the case study’s name to keep everything organized from the start.\nIn your case study folder, you should have a:\n\nsubfolder “scripts” to save any scripts related to the analysis\nsubfolder “data” which will contain the raw data you will use\nsubfolder “outputs” can be used to store any outputs (tables, graphs, documents) that are the result of the analysis\n\nMake sure your folders and subfolders are well-organised as this will save you a headache later!\n\n\n\nCreate an Rstudio project in the case study folder. If you are unsure on how to do that, read the EpiRhandbook on RStudio projects.\n\n\n\nOnce you have created an RStudio project, start a new R script with an appropriate name (example case_study_name) and save it in the subfolder “scripts”.\nIf you are familiar with R markdown, you may decide to use this type of file instead of a standard R script. See below for more instructions on using R markdown.\nNo matter what type of file you choose to use, make sure the purpose, date last updated, and author are written as comments at the top.\n\n#Purpose: To practice new skills using this case study\n#Author: Your Name\n#Date: Mmm dd, yyyy or whatever format you like best\n\n\n\n\nSome of our case studies use R markdown and the code goes within “chunks”, which is different from a standard R script. If you want to create a reproducible workflow, you need a place to save your code so you can run it again if you need to. You want all your files easily organised so you don’t get lost later on. You need to begin by setting the default chunk options.\nTypically, you want to change the default chunk options of your R markdown script to:\n\nhide all code chunks in the report\nnot show messages or warnings in the output\nshow errors if they appear, but to not stop the rendering\nset up the default figure width to 7 and the figure height to 6\nshow the figure titles on top of the plots by default\n\n\n# hide all code chunks in the output, but show errors \nknitr::opts_chunk$set(echo = FALSE,  # hide all code chunks in output\n                      error = TRUE,  # show errors if they appear, but don't stop (produce the word doc)\n                      warning = FALSE, # do not show warnings in the output word doc \n                      message = FALSE, # do not show  messages in the output word doc\n                      fig.width = 7,         # Figure width\n                      fig.height = 6,        # Figure height\n                      fig.topcaption = TRUE  # show figure titles on top of plot\n                     )\n\nBe sure to review Reports with R Markdown in the EpiRhandbook before jumping in!\n\n\n\nDepending on where you are and how you carried out R installation, your language “locale” might be different from the language of the report that you want to produce.\nFor example, a French-speaking person might have a French ‘locale’. If that is the case, when creating a graph by day of the week, “Monday” will be displayed as “lundi”. If that person wants to create an English report, as for this case study, the language ‘locale’ should be changed.\nTo ensure your ‘locale’ is set to English, use the following code:\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\nAt the start of every R project, you will need to install the necessary packages. We do this with the {pacman} package. Its p_load() command will install packages if necessary and load them for use in the current session. If a listed package has already been installed, it will just load it. Each case study specifies at the beginning what packages you need to have installed.\nYou can find more about installing/loading packages in the suggested packages section of the EpiRhandbook.\nExample code to install packages:\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # to create age categories\n  gtsummary,  # summary statistics, tests and regressions \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n)\n\nIf this step is not working, you may have limited administrative rights for your computer. Making sure your IT-department gives you the correct access can save a lot of headache. See these EpiRhandbook pages on the basics of installing packages and running R from network drives (company computers) for more detail."
  },
  {
    "objectID": "pages/r_practical.html",
    "href": "pages/r_practical.html",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Tool: R | Technical complexity: Basic | Methodological complexity: Basic\nSource: ECDC EI Group (simulated data)\nPrior knowledge required: R basics (Using Rstudio; R packages, functions and arguments, using pipes)\n\n\n\nFor instructions on how to use our case studies, see our How-to Guide. We welcome feedback and suggestions via contact@appliedepi.org. You can also discuss the case study or related concepts on the Applied Epi Community.\n\n\n\nIt is May 2022 and Mpox has just been reported for the first time across 5 countries in Europe: Countries “A”, “B”, “C”, “D”, and “E”. You have been requested to provide a basic descriptive analysis to the European Centre for Disease Prevention and Control (ECDC).\nYou are given access to:\n\nA dataset with aggregate case counts, submitted to ECDC by the five countries as part of routine European reporting\nA linelist with cases, submitted by the five countries to ECDC for this particular analysis\n\nLet’s go!\n\n\n\nIn this case study you will:\n\nExplore different types of files and how they can be imported in R.\nPerform basic data cleaning, e.g., changing the variable type, recoding variables, aggregating and filtering.\nPerform a basic descriptive analysis using tables and graphs\n\n\n\n\n\n\nStart by setting up a reproducible and well-organized workflow. This will make it easy to rerun your analysis whenever needed.\nTasks:\n\nSet up an RStudio project\nSet up clear sub-folders where your code, data, and outputs will go\nCreate an R script, or an R Markdown file if you prefer. Make sure the script purpose, date, and author are written as comments at the top.\nExtra: Ensure your working language in RStudio is appropriate (e.g. English for this exercise)\n\n\n\n Click to read a hint\n\n\n\nCreate a folder where all the work in this case study will go. For example, create ‘mpox_analysis’ on your computer desktop. Create your RStudio project to be based in this folder.\nWe suggest creating the following sub-folders: scripts (for your code), data (for your data), and outputs (for your analytical outputs).\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nCreate a folder (e.g. ‘mpox_analysis’ on your Desktop) for your work. To create an Rstudio project in your new folder, click New Project… in the top left of your R Studio, then Existing Directory, then Browse to select your new folder. For more information, look at the R projects section of the Epi R Handbook.\nStart a new R script by clicking New File… in the top left of your R Studio, then R Script. Save it immediately in the appropriate place, e.g. in a ‘scripts’ subfolder of your R Project.\nAt the top of your new R script, write some essential information like your name, the purpose of the file, and the date.\nYour R locale determines the language and regional settings used for things like date formats and translations. If your locale is different from the language you want for your report (e.g., a French locale vs. an English report), you can change it to English by running Sys.setlocale(\"LC_ALL\", \"English\"). Include this in your script if needed, or skip it if your locale is usually appropriate. This is explained in more detail in the How-to Guide.\n\n\n\n\nNext in your R script, you need to install and load the necessary R packages. This ensures that the functions you need are available for your analysis.\nYou will need the following packages: rio (for importing data), janitor (for cleaning data), lubridate (for cleaning dates), skimr (for reviewing data), epikit (for epi-related tasks), gtsummary (for presentation-ready tables), apyramid (for age-sex pyramids), and tidyverse (for general data manipulation/science tasks).\nAs you start, your trusted colleague nudges you and whispers “I’ve heard that a great way to manage your packages is with the pacman package”.\nOver to you!\n\n\nClick to see a solution (try it yourself first!)\n\n\nUse the function p_load() from pacman for this task. You provide the function with a list of packages that you want to use. It will take two steps per package: 1) Check if the package is installed on your computer, and install it if necessary, then 2) Load the package so it can be used during this R session.\nIf you don’t already have pacman installed, you will need to install it the “traditional way” first, with install.packages().\nNote that the order of packages in your p_load function can be important. If two packages have the same function names (e.g. select() in the package MASS and select() in tidyverse, which do different things), then R will use the function from the most recently loaded package. To prioritize functions from tidyverse, which are commonly used for data manipulation and visualization, load tidyverse last.\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # to create age categories\n  gtsummary,  # summary statistics, tests and regressions \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n)\n\n\n\n\n\n\n\nECDC provides you with two files for your analysis, both updated as of 31st August 2022:\n\nA case-level linelist (“mpox_linelist.xlsx”) with case information from five countries (countries A - E)\nAn aggregate table (“mpox_aggregate_table.csv”) for those countries with cumulative case counts per day.\n\nThey provide it to you via AppliedEpi’s very useful data repository, which you can access using the {appliedepidata} package. So first you need to download these two files to your own computer, as follows:\n\nInstall the {appliedepidata} package from GitHub using the install_github() function in the {remotes} package. Install {remotes} if you need to first.\n\n\n# Install remotes if you need to (so you can install a package from GitHub)\npacman::p_load(\"remotes\")\n\n# Use the install_github function from remotes to install appliedepidata\nremotes::install_github(\"appliedepi/appliedepidata\")\n\n\nSave the two datasets into a specific folder using the save_data() function from {appliedepidata}, by running the code below. The example below saves the data into a ‘data’ subfolder within the RStudio project. Note that if you do not specify a location within the ‘path’ argument of the function, a window will pop up asking you to manually select a folder.\n\n\n# Save down the two mpox files using the save_data() function from appliedepidata\nappliedepidata::save_data(\"mpox_linelist\",\n                        path = \"data\")\n\nappliedepidata::save_data(\"mpox_aggregate_table\",\n                          path = \"data\")\n\n\n\n\nGreat! Thanks ECDC and Applied Epi! Now it’s time to import the data from that folder into RStudio, so you can analyse it.\nTask: Import the downloaded case-based and aggregated data into your R environment. Ideally you want to use one function for both datasets, despite one being a csv and the other an xlsx file.\n\n\n Click to read a hint\n\n\nUse the import function from the {rio} package, which can recognize and import different file types. It replaces importing functions that are specific to the file type, such as read.csv() from {base} for .csv files and read_excel() from {readxl} to import .xlsx files.\nIf you feel you need to know more about importing functions, read the Import and export chapter of the EpiRhandbook.\n\n\n\nClick to see a solution (try it yourself first!)\n\nBelow we use the import function to bring in both files. Note how we are assigning the imported data to two objects, one called mpox_linelist_raw, and one called mpox_agg_raw. We add the ‘raw’ suffix to distinguish this data from the cleaned versions we will make later.\n\n# Import data  --------------\n\n# Case-based data\nmpox_linelist_raw &lt;- import(\"data/mpox_linelist.xlsx\")\n\n# Aggregated data\nmpox_agg_raw &lt;- import(\"data/mpox_aggregate_table.csv\")\n\n\n\n\n\n\nThe data’s in, and now it’s time to see what story it tells. Take an initial look at your data to check its quality and how you can best use it.\nTasks: Take a look at the different data frames and determine:\n\nThe number of columns and observations (e.g. their dimensions)\nThe class of their columns and whether it matches its nature (e.g., are “dates” considered “dates” by R?)\nIf the contents of columns are clean and standardized in the mpox linelist (e.g. gender, clinical symptoms, outcome, hiv status and sexual orientation). Do you need to recode any of them?\nHow unknown or missing data is categorized in these columns. Do these values need to be standardized?\n\n\n\n Click to read a hint\n\n\nAn efficient function for initial data exploring is skim() from the {skimr} package, as it gives you a lot of information on data structure and content, including the classes of columns.\nYou can use the function tabyl() from {janitor}, to get counts and percentages of every category in the data column, one by one. These get printed to your RStudio console.\nAlso - we recommend just looking at the data itself! A good function for this is view(), a baseR function.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nUsing the skim commands you can see the rows and columns of each dataset, and you can see how most of the columns in mpox_linelist_raw (including those containing dates) are character classes. (Results not shown on this exercise page)\n\n# Explore the dimensions of the two data objects \nskim(mpox_linelist_raw)\nskim(mpox_agg_raw)\n\nTake a look at the overall data using view(). It will pop up in the Data Viewer tab and you will get a good sense of how clean the data is and what the missingness is like. This preview shows just 5 rows from the linelist data.\n\nview(mpox_linelist_raw)\n\n\n\n\n\n\n\n\nBelow is an example of using the tabyl() function from {janitor}, to look at the distribution of clinical symptoms. You can see 12 cases have missing clinical information and that many cases have a mix of symptoms.\n\ntabyl(mpox_linelist_raw, HIVStatus)  \n\n HIVStatus   n percent valid_percent\n       NEG 525  0.2625     0.4107981\n       POS 307  0.1535     0.2402191\n       UNK 446  0.2230     0.3489828\n      &lt;NA&gt; 722  0.3610            NA\n\n\nYou can explore further columns one by one (results not shown):\n\n# Explore the values of different categorical columns in the mpox linelist: with tabyl\ntabyl(mpox_linelist_raw, Gender)\n\ntabyl(mpox_linelist_raw, ClinicalSymptoms)\n\ntabyl(mpox_linelist_raw, Outcome)\n\ntabyl(mpox_linelist_raw, SexualOrientation)\n\nYou could add extra arguments to tabyl() to customize the tables, such as adding totals and changing the proportions to percentages so they are easier to read. See the table on clinical symptoms below. But remember - this is just an initial look so don’t go too crazy.\n\ntabyl(mpox_linelist_raw, ClinicalSymptoms) %&gt;%    # Tabulate symptoms \n  adorn_totals() %&gt;%                              # Add totals to bottom of table\n  adorn_pct_formatting(digits = 2)                # Format percentages\n\n                 ClinicalSymptoms    n percent valid_percent\n                          Lesions   14   0.70%         0.70%\n                             Rash  257  12.85%        12.93%\n                    Rash, Lesions  323  16.15%        16.25%\n          Rash, Systemic symptoms  676  33.80%        34.00%\n Rash, Systemic symptoms, Lesions  654  32.70%        32.90%\n                Systemic symptoms   28   1.40%         1.41%\n       Systemic symptoms, Lesions   36   1.80%         1.81%\n                             &lt;NA&gt;   12   0.60%             -\n                            Total 2000 100.00%       100.00%\n\n\nFinally, as an alternative approach to tabyl(), you could use tbl_summary() from the {gtsummary} package. We will describe this later.\n\n\n\n\n\n\n\n\nTest yourself!\n\nHow many columns does the aggregated data have?\n\n 2000 13 3 101\n\n\n\nWhat is the class of the column DateOfNotification in the mpox linelist?\n\n Date Character Numeric Factor\n\n\n\nFor how many cases is the HIV status Unknown or missing?\n\n 1168 722 900 446\n\n\n\n\n\n\n\n\n\n\nSo! The good news: you have information on geography, dates, demographic characteristics, and clinical details. A promising descriptive analysis lies ahead.\nBUT! You may noticed that there are a few things to fix before the real detective work begins.\nFor example:\n\nColumn names have capital letters. This isn’t outright a problem, but can lead to mistakes since R treats ColumnName and Columnname as different.\nDate columns are recognized as character classes, not dates, which would cause issues like incorrect ordering (alphabetical) in epicurves.\nSome columns have values that are unclear or unsuitable for presentation. For example gender is categorized with “F”, “M”, “O” and “UNK”. The column Outcome is “A” and “UNK”.\nMissing data is inconsistently handled, for instance with both “UNK” and NA in the HIV status column. R thinks “UNK” is a valid value, which it treats differently to true missing data (indicated by NA)\n\nTasks:\n\nCreate a clean version of your case-based data making all cleaning changes in a single piping command\nChange all column names to lower case.\nConvert all date columns to class “Date”.\nConvert all missing/unknown values to NA (to be recognized by R as missing)\nRecode non-missing “Gender” categories into: “Female”, “Male”, and “Other”\nRecode non-misising HIV status into: “Positive”, “Negative” and “Unknown”\nRecode non-missing sexual orientation into: “Bisexual”, “Heterosexual”, and “MSM/homo or bisexual male”.\nRecode non-missing “outcome” categories into: “Alive” and “Dead”.\nCheck that all changes have been made correctly\n\n\n\n Click to read a hint\n\n\nTo convert all column names to lower case at once rather than renaming each column, use the function clean_names() from the {janitor} package.\nUse {lubridate} functions to transform date columns into “Date” class. You can do this one by one, or you could do all at the same time using the across() function from {dplyr}. If you feel you need to know more about transforming dates read the chapter Working with Dates from the EpiRhandbook. If you are not sure how to use the across() function, you can also read the section on Transform multiple columns.\nThere are different functions that we can use to recode values. We propose three: The function recode() from {dplyr}, the function ifelse() from {base} and the function case_when() from {dplyr}. If you want to know more about these functions, look that the section on Re-code values from the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nHere we clean the data using a ‘chain’ of commands connected by pipes (%&gt;%), which is the grammar of the functions in the {Tidyverse}. The output is assigned to a new object called mpox_linelist to differentiate it from the raw data. It can be helpful to have both the cleaned and raw data available in the environment to compare to the original data if needed.\nSee the series of functions and the explanation in the comments.\n\n# Create a new object called mpox_linelist which is the clean version of the raw data\nmpox_linelist &lt;- mpox_linelist_raw %&gt;% \n  \n  # standardises names and puts all into lower case \n  clean_names() %&gt;% \n  \n  #transform ONE column into date (note the column names are lower case now)\n  mutate(date_of_notification = ymd(date_of_notification)) %&gt;%  \n\n  #transforms ALL columns starting with \"date\" into dates\n  mutate(across(starts_with(\"date\"), \n                .fns = ~ ymd(.x))) %&gt;%  \n  \n  #transforms UNK to NA across all character columns \n  mutate(across(where(is.character), \n                .fns = ~ ifelse(.x %in% c(\"UNK\", \"Unknown\"), NA_character_, .x)))  %&gt;% \n\n  # Recode the gender values to be more obvious  \n  mutate(gender = recode(gender,\n                         \"F\" = \"Female\",\n                         \"M\" = \"Male\",\n                         \"O\" = \"Other\")) %&gt;%\n  \n  #recode with ifelse to change only one or two categories based on a rule. \n  mutate(outcome = ifelse(outcome == \"A\", \"Alive\", outcome)) %&gt;%   \n  \n  #recode with case_when for more complex recoding \n  mutate(hiv_status = case_when(hiv_status == \"NEG\" ~ \"Negative\",    \n                                hiv_status == \"POS\" ~ \"Positive\")) %&gt;% \n  \n  mutate(sexual_orientation = case_when(sexual_orientation == \"BISEXUAL\" ~ \"Bisexual\",\n                                        sexual_orientation == \"HETERO\" ~ \"Heterosexual\",\n                                        sexual_orientation == \"MSM\" ~ \"MSM/homo or bisexual male\")) \n\nYou can then review your data by tabulating across all the different columns you have cleaned. See the preview of the HIV table below - it looks tidier now with more understandable categories, and all missing data is classified as ‘Unknown’.\n\n# Check that all changes have been made correctly\n\nskim(mpox_linelist)\n\ntabyl(mpox_linelist, gender)\n\ntabyl(mpox_linelist, clinical_symptoms)\n\ntabyl(mpox_linelist, outcome)\n\ntabyl(mpox_linelist, hiv_status)\n\ntabyl(mpox_linelist, sexual_orientation)\n\n\n\n hiv_status    n percent valid_percent\n   Negative  525  0.2625     0.6310096\n   Positive  307  0.1535     0.3689904\n       &lt;NA&gt; 1168  0.5840            NA\n\n\nIMPORTANT: If ‘unknown’ and NA had meaningful differences, combining them wouldn’t be appropriate (e.g., if ‘unknown’ meant the case was asked but didn’t want to respond, while NA meant they weren’t asked). Here, we assume no meaningful difference and want R to recognize them as missing.\n\n\n\n\n\n\n\n\nTest yourself!\n\nIs it always appropriate to combine different types of unknown data? (e.g. missing, unknown, did not respond, NA)\n\n Yes Depends on the meaning of those values No - never do this\n\n\n\nHow many male cases do we have in the data frame?\n\n 36 1960 65 1523\n\n\n\nHow many cases have ‘alive’ as an outcome?\n\n 1405 None 595\n\n\n\n\n\n\n\n\nIn a similar way, clean the aggregated data by:\n\nStandardising names to lower case\nEnsuring that date of reporting is of class “Date”\nCreating a column called “week_date” with the week of reporting starting on Monday\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nWe can first check the class of the DateRep column, which shows us that it was already recognized as a date column on import.\n\n# Check class of date of reporting column\nclass(mpox_agg_raw$DateRep)\n\nThen create a new object for the clean aggregate data, and write your cleaning coded connected with pipes.\n\n# Create a new object called mpox_agg which is the clean version of the raw data, applying the cleaning functions\n\nmpox_agg &lt;- mpox_agg_raw %&gt;% \n  \n  # standardises names and puts all into lower case\n  clean_names() %&gt;%  \n  \n  # create week column with Monday start\n  mutate(week_date = floor_date(date_rep, \n                              unit = \"week\",\n                              week_start = \"Monday\")) \n\n\n\n\n\n\n\n\n\nTest yourself!\n\nTake a look at the aggreate data. Which country reported the largest cumulative number of cases during the week 2022-04-11?\n\n Country A Country B Country C Country D Country E\n\n\n\n\n\n\n\n\n\nNow we’re getting to the heart of the investigation. Who is affected? Which locations are most affected, and how quickly is it spreading? Your ability to tell the classic “person, place, and time” story will be crucial to guiding the response. Pinpoint those hotspots and trends!\n\n\nTask: Using the mpox case linelist, create a table showing the total number of cases by country. This time, make the table more publication-friendly.\n\n\n Click to read a hint\n\n\nYou could use tabyl() like before, but an easy way to produce publication-ready tables is with the function tbl_summary() from {gtsummary} package. This formats the table for you. It will print to your Viewer rather than the console.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nCreate a new object with the table output - as this is a key output that you can then integrate into a document later rather than just viewing for now.\n\n# Create an object with the table\ncb_country_table &lt;- mpox_linelist %&gt;%\n\n  #select the column that we want to use in the table\n  select(country) %&gt;% \n  \n  # create the table. No need to specify columns; it will tabulate all available columns (selected above)\n  tbl_summary() \n\n# Print the table\ncb_country_table\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    country\n\n        CountryA\n816 (41%)\n        CountryB\n391 (20%)\n        CountryC\n474 (24%)\n        CountryD\n217 (11%)\n        CountryE\n102 (5.1%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nTest yourself!\n\nWhat country has the largest percentage of cases?\n\n Country C Country D Country B Country E Country A\n\n\n\n\n\n\n\n\nOkay so Country A has the most cases in total based on most recent data. But how does that change look over time?\nTasks:\n\nUsing the mpox case linelist, create an epicurve by week of notification\nUsing the mpox case linelist, create an epicurve by week of notification to enable a comparison of trends by country.\nUsing the mpox case linelist, create a heat plot with the number of cases by country and week of notification.\n\n\n\n Click to read a hint\n\n\nPrepare your data for the epicurve first. You can create a “week_date” column using the function floor_date() from {lubridate}. Take a look at the documentation to understand how it works and how to pick the starting day of the week.\nTo create the epicurve, you can use ggplot() and geom_bar(), which visualizes the number of rows within a group - e.g. number of cases per week. To compare trends in different countries, consider using the facet_wrap() function. If you are unsure on how ggplot() works, read the EpiRhandbook chapter on Epidemic curves.\nTo create a heatmap, you will need to create a table of counts by country and week of notification. You can do this using the functions group_by() and summarise() from {dplyr}. If you are unsure on how to do this, review the Grouping data chapter of the EpiRhandbook. Then, use the geom geom_tile() to create a heat plot. If you’re unsure on how to do this, read the EpiRhanbook section on Heat Plots\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nPrepare your data by creating the new column using mutate() and floor_date():\n\nmpox_linelist &lt;- mpox_linelist %&gt;% \n  # create week column with Monday start \n  mutate(week_date = floor_date(date_of_notification, unit = \"week\", week_start = \"Monday\")) \n\nThe code below creates an epicurve using ggplot() and the geom_bar() function, then applies further formatting. With geom_bar(), you only need to specify the x axis, and the function will visualize the number of rows per unique x axis value.\n\n# Open up the plot production with ggplot() function, specifying object and columns\nepicurve_mpox &lt;- ggplot(data = mpox_linelist,          \n                        aes(x = week_date)) +    \n  \n  geom_bar(fill=\"darkgreen\",                     #colour inside the bins\n                 color=\"white\",                  #outline colour of the bins\n                 alpha=0.8) +                    #transparency of the bins\n  \n  scale_x_date(breaks = \"2 weeks\") +             #set the x axis labels to two week intervals\n\n  labs(title=\"Mpox cases reported in 2022 in Countries A, B, C, D, and E\",\n       subtitle = \"Date as of August 31st 2022\") +  #add a title\n  \n  theme_minimal() +                             #assign a predefined theme\n  \n  theme(axis.text = element_text(size=9),       #define the font size of the axis text\n        axis.title = element_blank(),           #remove the titles of the x and y axis \n        axis.text.x = element_text(angle=90))   #rotate the x axis text\n           \n# Print the epicurve\nepicurve_mpox\n\n\n\n\nTo examine how the outbreak spread by country, add facet_wrap() to your ggplot code. This splits the graph into multiple smaller ones. As shown below, you can even simply add the function to the national epicurve object.\nAn alternative approach would be to create a stacked epicurve, i.e. retain the single epicurve but split each bar into colors per country. You would do this by adding fill = country to the aes() in the epicurve code. However, we don’t recommend this for comparing trends, as stacked bars make it harder to see individual patterns.\n\nepicurve_epox_country &lt;- epicurve_mpox + \n \n   # Facet wrap to make mini-plots, specifying that you want two columns of plots. \n  facet_wrap(.~country,\n             ncol = 1) \n\n# Print the epicurve\nepicurve_epox_country\n\n\n\n\nFinally, if you want to demonstrate this as a weekly heatmap, you can use geom_tile(). First, aggregate the data by week. Then pipe into a ggplot(), as shown below.\n\n# Assign the output of your ggplot code to a new object\nhp_mpox &lt;- mpox_linelist %&gt;% \n  \n  #first count the number of cases by country and notification week\n  count(country, week_date) %&gt;% \n\n  #you can pipe directly into the ggplot\n    ggplot(aes(x = week_date, # notification week along the x axis\n           y = country,       # country along the y axis\n           fill = n)) +       # colour in the heatmap tiles by number\n  \n  # specify that you want this to be a heatmap with geom_tile()\n  geom_tile(colour = \"black\") +   # black is the outline of each tile\n  \n  #define the gradient of the colours\n  scale_fill_gradient(            \n    low = \"lightgreen\",\n    high = \"red\") +\n  \n  #set the x axis labels to two week intervals\n  scale_x_date(breaks = \"2 weeks\") +             \n  \n  # Add titles\n  labs(\n    title= \"Mpox cases by country and week of notification\",\n    fill = \"Number of cases\"                               \n  ) +\n  \n  # Apply an overall theme to your plot\n  theme_bw() +\n  \n  # Customize other appearance details\n  theme(legend.position = \"bottom\",       #legend position to bottom\n        axis.text = element_text(size=9),     #define axis font \n        axis.title = element_blank(),         #remove the axis titles\n        axis.text.x = element_text(angle=90)) #rotate the x axis text\n    \n\n# Print the heatmap\nhp_mpox \n\n\n\n\n\n\n\n\n\nNext, describe the age, gender, and sexual orientation of cases. What is interesting?\nTask:\n\nCreate a single table showing overall distribution of age, gender, and sexual orientation\nCreate an age-gender pyramid showing age as 10-year age bands\n\n\n\n Click to read a hint\n\n\nTo quickly create a presentation-ready table showing the breakdowns for three different columns, consider using the function tbl_summary() from {gtsummary}.\nTo create an age-gender pyramid, first create a new column with the function age_categories() from the {epikit} package. Then explore the function age_pyramid() from the {apyramid} package.You can find more about this function in the EpiRhandbook chapter Demographic pyramids and Likert-scales\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nSee below the code to quickly generate one table with the breakdown of different variables. The function tbl_summary() by default summarizes columns differently depending on their class:\n\nAge is a numeric column, so is summarized with a median and interquartile range.\nGender and sexual orientation are character values, so are described in terms of counts and percentages.\n\nYou can customize this further; explore the documentation by typing ?tbl_summary() in your console.\nNote that tbl_summary() by default does not include NAs in the counts and percentages, allowing you to see the distribution of non-missing values.\n\n# Create table of all three variables\ntab_demographics &lt;- mpox_linelist %&gt;% \n  \n  # select the columns of interest for\n  select(age, gender, sexual_orientation) %&gt;% \n  \n  # use tbl_summary() to create the table\n  tbl_summary() \n\ntab_demographics\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    age\n37 (31, 45)\n        Unknown\n3\n    gender\n\n        Female\n36 (1.8%)\n        Male\n1,960 (98%)\n        Other\n1 (&lt;0.1%)\n        Unknown\n3\n    sexual_orientation\n\n        Bisexual\n7 (0.8%)\n        Heterosexual\n46 (5.2%)\n        MSM/homo or bisexual male\n833 (94%)\n        Unknown\n1,114\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nCreate the new age group column as follows. You can add this to the cleaning section of your script (which we covered 4.1).\n\nmpox_linelist &lt;- mpox_linelist %&gt;% \n  # Use the age_categories function to create age categories\n  mutate(age_group = age_categories(age, lower = 0, #set up the lower age\n                                    upper = 70, #set up the upper age\n                                    by = 10)) #set up the age breaks\n\nThen make the age-gender pyramid using the age_pyramid() function. It is a function that builds on ggplot, so you can then continue to add on customization, such as the theme_bw() below.\n\n# Create table of all three variables\nfigure_agesex &lt;- mpox_linelist %&gt;% \n  \n  # Filter to male and female only\n  filter(gender %in% c(\"Male\", \"Female\")) %&gt;% \n  \n  # select the columns of interest for\n  age_pyramid(age_group = \"age_group\",\n              split_by = \"gender\") +\n  \n  # change theme\n  theme_bw()\n\nfigure_agesex\n\n\n\n\n\n\n\n\n\n\n\n\nTest yourself!\n\nWhich demographic group is more affected by Mpox?\n\n Females 60-69 Males 40-49 Females 10-19 Males 30-39\n\n\n\nWhat proportion of mpox cases were homosexual or bisexual men?\n\n 41% 42% 5% 94%\n\n\n\n\n\n\n\n\nThe media is starting to call your office and are asking what symptoms the public should look out for. Just in luck - you can check that out in the data too!\nTasks:\n\nCreate a table with the distribution of different symptoms and outcomes.\n\nNo hints! You should know this one by now!\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Table with number and percentage of cases by outcome\n\ntab_outcome &lt;- mpox_linelist %&gt;% \n  \n  # Select the columns for tabulation\n  select(outcome, clinical_symptoms) %&gt;% \n  \n  # Use tbl_summary() - note that this time we are adding on labels to change how the column name is displayed\n  tbl_summary(label = list(\n    clinical_symptoms = \"Symptoms\",\n    outcome = \"Reported outcome\")) \n\ntab_outcome\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    Reported outcome\n\n        Alive\n1,405 (100%)\n        Unknown\n595\n    Symptoms\n\n        Lesions\n14 (0.7%)\n        Rash\n257 (13%)\n        Rash, Lesions\n323 (16%)\n        Rash, Systemic symptoms\n676 (34%)\n        Rash, Systemic symptoms, Lesions\n654 (33%)\n        Systemic symptoms\n28 (1.4%)\n        Systemic symptoms, Lesions\n36 (1.8%)\n        Unknown\n12\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n\n\nYou’ve described a lot now, but you want to make sure you understand how timely and complete your mpox linelist is, especially if it will be the basis of making decisions.\nFor example - is it possible that there are very different reporting delays between countries, meaning current case counts are not directly comparable? Oh dear, must check.\n\n\nTasks\n\nCalculate median time from symptom onset to diagnosis and from diagnosis to notification, both overall and by country\nAssess visually the number of cases by calendar period and type of date (onset, diagnosis and notification)\n\n\n\n Click to read a hint\n\n\nTo plot together the different dates you may need to transform your data from “wide” to “long” form. What we call “pivoting” in R. The objective is to have a column with the different date categories (onset, diagnosis and notification) and another column with their date value. If you are unsure on how to do this, have a look at the Pivoting data chapter of the EpiRhandbook. Then, try to plot with the daily values, but if that’s not easy to interpret you may want to aggregate cases by week.\n\n\n\n\n\n\n\n\nTest yourself!\n\nIs there a difference in the delay from diagnosis to notification by country?\n\n Yes No\n\n\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nFirst create the required columns for this analysis.\n\n# Create two columns in linelist to assess delays\ndelay_db &lt;- mpox_linelist %&gt;% \n  \n  # Time between onset and diagnosis (converted to a number)\n  mutate(delay_diag = as.numeric(date_of_diagnosis - date_of_onset)) %&gt;%   \n\n  # Time between diagnosis and notification (converted to a number)\n  mutate(delay_not = as.numeric(date_of_notification - date_of_diagnosis)) \n\nUse the summary function from base R to quickly view the median, mean, interquartile range, and rang.\n\n# Summarize the delays to diagnosis\nsummary(delay_db$delay_diag) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -2.000   4.000   7.000   7.758  10.000  66.000     897 \n\n# Summarize the delays from diagnosis to notification\nsummary(delay_db$delay_not)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-46.0000  -2.0000   0.0000  -0.6078   1.0000  23.0000      715 \n\n\nUse group_by() and summarize() to create a table with median delays per country.\n\ndelay_country &lt;- delay_db %&gt;% \n  \n  # Group by country\n  group_by(country) %&gt;% \n  \n  # Create columns for each delay\n  summarise(median_delay_diag = median(delay_diag, na.rm = T),\n            median_delay_not = median(delay_not, na.rm = T))\n\ndelay_country\n\n# A tibble: 5 × 3\n  country  median_delay_diag median_delay_not\n  &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 CountryA                 7                0\n2 CountryB                 7                0\n3 CountryC                 6                0\n4 CountryD                 7                0\n5 CountryE                 6                0\n\n\nTo explore how the trends in cases over time differ when using different dates, you can reshape the linelist to create a dataset with one row per date type per case.\n\n# Prepare the data\ndates_longer &lt;- mpox_linelist %&gt;% \n  \n  select(age, gender, sexual_orientation, starts_with(\"date_\")) %&gt;% \n\n  pivot_longer(\n    \n      # all columns starting with \"date_\" will be pivoted from wide to long \n      cols=starts_with(\"date_\"),         \n    \n      # put names of the columns into a single column called \"indicator\"\n      names_to = \"indicator\",   \n      \n      # the date values will be placed in a column called \"date\"\n      values_to = \"date\")                \n\nThe data will then look like this, with three rows per case:\n\n\n\n\n\n\n\nThen tabulate cases by week per indicator\n\n# Create new object\ndates_longer_week &lt;- dates_longer  %&gt;% \n\n  # Create a new week column\n  mutate(week_date = floor_date(date, unit = \"week\", week_start = \"Monday\")) %&gt;%  \n  \n  # Within each combination of indicator and week, calculate the number of cases\n  group_by(indicator, week_date) %&gt;% \n  summarise(n=n()) %&gt;%   \n  \n  # drop the cases with no data on dates  \n  drop_na(week_date)                     \n\nThe data will then look like this, with three rows per case:\n\n\n\n\n\n\n\nFinally, create a plot with ggplot() and geom_line().\n\nplot_date_delay &lt;-   ggplot(data = dates_longer_week,\n                            aes(x = week_date, \n                                y = n, \n                                color=indicator)) +\n  \n  geom_line(linewidth = 1.5) +\n  \n  scale_x_date(breaks = \"2 weeks\")+\n  \n  theme_bw() +\n  \n  theme(legend.position = \"bottom\", \n        axis.text = element_text(size=9),\n        axis.title = element_blank(),\n        axis.text.x = element_text(angle=90),\n        legend.title = element_blank()) +\n  labs(title=\"Mpox cases reported in 2022, by date of onset, diagnosis and notification.\")\n\nplot_date_delay\n\n\n\n\n\n\n\n\n\nFinally, you remember that all-along you’ve had these aggregate counts from routine surveillance. You find out that these numbers are actually already being published.\nBefore you share your own numbers, you’d better check how different they are from already-published statistics!\nTask: Create a plot comparing the number of cases reported to through the case-based flow and through the aggregated flow in each country.\nNOTE: Take into consideration that the column on cases in the aggregated data frame reports the cumulative number of cases.\n\n\n\n\n\n\nTest yourself!\n\nWhich country is not reporting aggregated data?\n\n A B C D E\n\n\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nFirst, create a data frame of country totals from the aggregate data.\n\nmpox_agg_country &lt;- mpox_agg %&gt;% \n \n  # as we have cumulative data, we keep only the last week per country \n  group_by(country) %&gt;% \n  filter(date_rep == max(date_rep)) %&gt;% \n  \n  # remove unnecessary columns\n  select(-date_rep, -week_date) %&gt;%     \n\n  # create this column to distinguish the numbers from the linelist data\n  mutate(source = \"aggregated\")         \n\nThen create a data frame of country totals from the case linelist, and append it to the totals from the aggregate data.\n\nmpox_linelist_country &lt;- mpox_linelist %&gt;%\n  \n  # count cases by country, use the same column name as in the aggregate data\n  group_by(country) %&gt;% \n  summarise(cases = n()) %&gt;% \n  \n  # create this column to distinguish the numbers from the linelist data\n  mutate(source = \"case_based\")       \n  \n\n# Append both data frames. Remember this is different from merging\ntotal_data &lt;- bind_rows(mpox_linelist_country, mpox_agg_country)\n\nYou can now use this data to compare the cases reported in both sources, using ggplot().\n\ngraph_comp &lt;- ggplot(data = total_data,\n                     aes(x = source, \n                         y = cases, \n                         fill = source)) +\n  \n  #position dodge puts bars one next to each other, instead of \"stacked\"\n  geom_col(position = \"dodge\") +            \n  \n  # this command gives us one graph per country. The argument scales allows each y axis scales to adjust to the data\n  facet_wrap(~ country, scales = \"free_y\") +  \n\n  # changes the colours, but with the argument \"labels\" we can change the text of each fill.\n  scale_fill_viridis_d(\n    labels = c(\"Aggregated\", \"Case-based\")) +\n  \n  labs(\n    title = \"Number of cases of Mpox reported in 2022 according to source of data\",\n    fill = \"Source\",\n    x = \"\",\n    y = \"Total number of cases\"\n  ) + \n  \n  theme_bw() +\n  \n  # we remove the text of the x axis because it is already present in the legend\n  theme(axis.text.x = element_blank(),   \n        \n   # we also remove the ticks for aesthetic purposes\n        axis.ticks.x = element_blank())    \n\ngraph_comp\n\n\n\n\n\nInteresting! There are some differences - and this probably will be worth flagging with stakeholders and/or explaining in a footnote somewhere.\n\n\n\n\n\nWell done! Through your analysis you now understand the magnitude of the outbreak so far, where and when it spread, which demographic groups are most affected, and how the disease actually manifests in terms of symptoms and severity. ECDC is very happy with your work.\nBy coding this up in R, this analysis should be reproducible, meaning you can quickly update it with new data and keep monitoring the outbreak.\nOf course, the above data is not real. If you want to see a paper on the actual outbreak that occured in Europe in 2022, you can take a look at this Eurosurveillance paper. This ECDC page on Mpox also publishes updates on the status of mpox in Europe.\nTo further practise reproducible reports, [link to RMarkdown].\n\n\n\nAuthorship\nOriginal authors: Xanthi Andrianou, Gianfranco Spiteri (ECDC EI Group)\nData source: Fictional data provided by ECDC EI Group for training purposes\n\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nOctober 2021\nFirst draft\n1\nXanthi Andrianou\n\n\nJune 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales\n\n\nSeptember 2024\nRevise for case study repository\n1.2\nPaula Blomquist and Alanah Jansen\n\n\n\n\n\n\nDisclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/"
  },
  {
    "objectID": "pages/r_practical.html#scenario",
    "href": "pages/r_practical.html#scenario",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "It is May 2022 and Mpox has just been reported for the first time across 5 countries in Europe: Countries “A”, “B”, “C”, “D”, and “E”. You have been requested to provide a basic descriptive analysis to the European Centre for Disease Prevention and Control (ECDC).\nYou are given access to:\n\nA dataset with aggregate case counts, submitted to ECDC by the five countries as part of routine European reporting\nA linelist with cases, submitted by the five countries to ECDC for this particular analysis\n\nLet’s go!"
  },
  {
    "objectID": "pages/r_practical.html#objectives",
    "href": "pages/r_practical.html#objectives",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "In this case study you will:\n\nExplore different types of files and how they can be imported in R.\nPerform basic data cleaning, e.g., changing the variable type, recoding variables, aggregating and filtering.\nPerform a basic descriptive analysis using tables and graphs"
  },
  {
    "objectID": "pages/r_practical.html#step-1.-set-up",
    "href": "pages/r_practical.html#step-1.-set-up",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Start by setting up a reproducible and well-organized workflow. This will make it easy to rerun your analysis whenever needed.\nTasks:\n\nSet up an RStudio project\nSet up clear sub-folders where your code, data, and outputs will go\nCreate an R script, or an R Markdown file if you prefer. Make sure the script purpose, date, and author are written as comments at the top.\nExtra: Ensure your working language in RStudio is appropriate (e.g. English for this exercise)\n\n\n\n Click to read a hint\n\n\n\nCreate a folder where all the work in this case study will go. For example, create ‘mpox_analysis’ on your computer desktop. Create your RStudio project to be based in this folder.\nWe suggest creating the following sub-folders: scripts (for your code), data (for your data), and outputs (for your analytical outputs).\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nCreate a folder (e.g. ‘mpox_analysis’ on your Desktop) for your work. To create an Rstudio project in your new folder, click New Project… in the top left of your R Studio, then Existing Directory, then Browse to select your new folder. For more information, look at the R projects section of the Epi R Handbook.\nStart a new R script by clicking New File… in the top left of your R Studio, then R Script. Save it immediately in the appropriate place, e.g. in a ‘scripts’ subfolder of your R Project.\nAt the top of your new R script, write some essential information like your name, the purpose of the file, and the date.\nYour R locale determines the language and regional settings used for things like date formats and translations. If your locale is different from the language you want for your report (e.g., a French locale vs. an English report), you can change it to English by running Sys.setlocale(\"LC_ALL\", \"English\"). Include this in your script if needed, or skip it if your locale is usually appropriate. This is explained in more detail in the How-to Guide.\n\n\n\n\nNext in your R script, you need to install and load the necessary R packages. This ensures that the functions you need are available for your analysis.\nYou will need the following packages: rio (for importing data), janitor (for cleaning data), lubridate (for cleaning dates), skimr (for reviewing data), epikit (for epi-related tasks), gtsummary (for presentation-ready tables), apyramid (for age-sex pyramids), and tidyverse (for general data manipulation/science tasks).\nAs you start, your trusted colleague nudges you and whispers “I’ve heard that a great way to manage your packages is with the pacman package”.\nOver to you!\n\n\nClick to see a solution (try it yourself first!)\n\n\nUse the function p_load() from pacman for this task. You provide the function with a list of packages that you want to use. It will take two steps per package: 1) Check if the package is installed on your computer, and install it if necessary, then 2) Load the package so it can be used during this R session.\nIf you don’t already have pacman installed, you will need to install it the “traditional way” first, with install.packages().\nNote that the order of packages in your p_load function can be important. If two packages have the same function names (e.g. select() in the package MASS and select() in tidyverse, which do different things), then R will use the function from the most recently loaded package. To prioritize functions from tidyverse, which are commonly used for data manipulation and visualization, load tidyverse last.\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  lubridate,  # working with dates\n  epikit,     # to create age categories\n  gtsummary,  # summary statistics, tests and regressions \n  apyramid,   # plotting age pyramids \n  tidyverse  # data management and visualization\n)"
  },
  {
    "objectID": "pages/r_practical.html#step-2-download-and-import-the-data",
    "href": "pages/r_practical.html#step-2-download-and-import-the-data",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "ECDC provides you with two files for your analysis, both updated as of 31st August 2022:\n\nA case-level linelist (“mpox_linelist.xlsx”) with case information from five countries (countries A - E)\nAn aggregate table (“mpox_aggregate_table.csv”) for those countries with cumulative case counts per day.\n\nThey provide it to you via AppliedEpi’s very useful data repository, which you can access using the {appliedepidata} package. So first you need to download these two files to your own computer, as follows:\n\nInstall the {appliedepidata} package from GitHub using the install_github() function in the {remotes} package. Install {remotes} if you need to first.\n\n\n# Install remotes if you need to (so you can install a package from GitHub)\npacman::p_load(\"remotes\")\n\n# Use the install_github function from remotes to install appliedepidata\nremotes::install_github(\"appliedepi/appliedepidata\")\n\n\nSave the two datasets into a specific folder using the save_data() function from {appliedepidata}, by running the code below. The example below saves the data into a ‘data’ subfolder within the RStudio project. Note that if you do not specify a location within the ‘path’ argument of the function, a window will pop up asking you to manually select a folder.\n\n\n# Save down the two mpox files using the save_data() function from appliedepidata\nappliedepidata::save_data(\"mpox_linelist\",\n                        path = \"data\")\n\nappliedepidata::save_data(\"mpox_aggregate_table\",\n                          path = \"data\")\n\n\n\n\nGreat! Thanks ECDC and Applied Epi! Now it’s time to import the data from that folder into RStudio, so you can analyse it.\nTask: Import the downloaded case-based and aggregated data into your R environment. Ideally you want to use one function for both datasets, despite one being a csv and the other an xlsx file.\n\n\n Click to read a hint\n\n\nUse the import function from the {rio} package, which can recognize and import different file types. It replaces importing functions that are specific to the file type, such as read.csv() from {base} for .csv files and read_excel() from {readxl} to import .xlsx files.\nIf you feel you need to know more about importing functions, read the Import and export chapter of the EpiRhandbook.\n\n\n\nClick to see a solution (try it yourself first!)\n\nBelow we use the import function to bring in both files. Note how we are assigning the imported data to two objects, one called mpox_linelist_raw, and one called mpox_agg_raw. We add the ‘raw’ suffix to distinguish this data from the cleaned versions we will make later.\n\n# Import data  --------------\n\n# Case-based data\nmpox_linelist_raw &lt;- import(\"data/mpox_linelist.xlsx\")\n\n# Aggregated data\nmpox_agg_raw &lt;- import(\"data/mpox_aggregate_table.csv\")"
  },
  {
    "objectID": "pages/r_practical.html#step-3-explore-the-data",
    "href": "pages/r_practical.html#step-3-explore-the-data",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "The data’s in, and now it’s time to see what story it tells. Take an initial look at your data to check its quality and how you can best use it.\nTasks: Take a look at the different data frames and determine:\n\nThe number of columns and observations (e.g. their dimensions)\nThe class of their columns and whether it matches its nature (e.g., are “dates” considered “dates” by R?)\nIf the contents of columns are clean and standardized in the mpox linelist (e.g. gender, clinical symptoms, outcome, hiv status and sexual orientation). Do you need to recode any of them?\nHow unknown or missing data is categorized in these columns. Do these values need to be standardized?\n\n\n\n Click to read a hint\n\n\nAn efficient function for initial data exploring is skim() from the {skimr} package, as it gives you a lot of information on data structure and content, including the classes of columns.\nYou can use the function tabyl() from {janitor}, to get counts and percentages of every category in the data column, one by one. These get printed to your RStudio console.\nAlso - we recommend just looking at the data itself! A good function for this is view(), a baseR function.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nUsing the skim commands you can see the rows and columns of each dataset, and you can see how most of the columns in mpox_linelist_raw (including those containing dates) are character classes. (Results not shown on this exercise page)\n\n# Explore the dimensions of the two data objects \nskim(mpox_linelist_raw)\nskim(mpox_agg_raw)\n\nTake a look at the overall data using view(). It will pop up in the Data Viewer tab and you will get a good sense of how clean the data is and what the missingness is like. This preview shows just 5 rows from the linelist data.\n\nview(mpox_linelist_raw)\n\n\n\n\n\n\n\n\nBelow is an example of using the tabyl() function from {janitor}, to look at the distribution of clinical symptoms. You can see 12 cases have missing clinical information and that many cases have a mix of symptoms.\n\ntabyl(mpox_linelist_raw, HIVStatus)  \n\n HIVStatus   n percent valid_percent\n       NEG 525  0.2625     0.4107981\n       POS 307  0.1535     0.2402191\n       UNK 446  0.2230     0.3489828\n      &lt;NA&gt; 722  0.3610            NA\n\n\nYou can explore further columns one by one (results not shown):\n\n# Explore the values of different categorical columns in the mpox linelist: with tabyl\ntabyl(mpox_linelist_raw, Gender)\n\ntabyl(mpox_linelist_raw, ClinicalSymptoms)\n\ntabyl(mpox_linelist_raw, Outcome)\n\ntabyl(mpox_linelist_raw, SexualOrientation)\n\nYou could add extra arguments to tabyl() to customize the tables, such as adding totals and changing the proportions to percentages so they are easier to read. See the table on clinical symptoms below. But remember - this is just an initial look so don’t go too crazy.\n\ntabyl(mpox_linelist_raw, ClinicalSymptoms) %&gt;%    # Tabulate symptoms \n  adorn_totals() %&gt;%                              # Add totals to bottom of table\n  adorn_pct_formatting(digits = 2)                # Format percentages\n\n                 ClinicalSymptoms    n percent valid_percent\n                          Lesions   14   0.70%         0.70%\n                             Rash  257  12.85%        12.93%\n                    Rash, Lesions  323  16.15%        16.25%\n          Rash, Systemic symptoms  676  33.80%        34.00%\n Rash, Systemic symptoms, Lesions  654  32.70%        32.90%\n                Systemic symptoms   28   1.40%         1.41%\n       Systemic symptoms, Lesions   36   1.80%         1.81%\n                             &lt;NA&gt;   12   0.60%             -\n                            Total 2000 100.00%       100.00%\n\n\nFinally, as an alternative approach to tabyl(), you could use tbl_summary() from the {gtsummary} package. We will describe this later.\n\n\n\n\n\n\n\n\nTest yourself!\n\nHow many columns does the aggregated data have?\n\n 2000 13 3 101\n\n\n\nWhat is the class of the column DateOfNotification in the mpox linelist?\n\n Date Character Numeric Factor\n\n\n\nFor how many cases is the HIV status Unknown or missing?\n\n 1168 722 900 446"
  },
  {
    "objectID": "pages/r_practical.html#step-4-clean-the-data",
    "href": "pages/r_practical.html#step-4-clean-the-data",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "So! The good news: you have information on geography, dates, demographic characteristics, and clinical details. A promising descriptive analysis lies ahead.\nBUT! You may noticed that there are a few things to fix before the real detective work begins.\nFor example:\n\nColumn names have capital letters. This isn’t outright a problem, but can lead to mistakes since R treats ColumnName and Columnname as different.\nDate columns are recognized as character classes, not dates, which would cause issues like incorrect ordering (alphabetical) in epicurves.\nSome columns have values that are unclear or unsuitable for presentation. For example gender is categorized with “F”, “M”, “O” and “UNK”. The column Outcome is “A” and “UNK”.\nMissing data is inconsistently handled, for instance with both “UNK” and NA in the HIV status column. R thinks “UNK” is a valid value, which it treats differently to true missing data (indicated by NA)\n\nTasks:\n\nCreate a clean version of your case-based data making all cleaning changes in a single piping command\nChange all column names to lower case.\nConvert all date columns to class “Date”.\nConvert all missing/unknown values to NA (to be recognized by R as missing)\nRecode non-missing “Gender” categories into: “Female”, “Male”, and “Other”\nRecode non-misising HIV status into: “Positive”, “Negative” and “Unknown”\nRecode non-missing sexual orientation into: “Bisexual”, “Heterosexual”, and “MSM/homo or bisexual male”.\nRecode non-missing “outcome” categories into: “Alive” and “Dead”.\nCheck that all changes have been made correctly\n\n\n\n Click to read a hint\n\n\nTo convert all column names to lower case at once rather than renaming each column, use the function clean_names() from the {janitor} package.\nUse {lubridate} functions to transform date columns into “Date” class. You can do this one by one, or you could do all at the same time using the across() function from {dplyr}. If you feel you need to know more about transforming dates read the chapter Working with Dates from the EpiRhandbook. If you are not sure how to use the across() function, you can also read the section on Transform multiple columns.\nThere are different functions that we can use to recode values. We propose three: The function recode() from {dplyr}, the function ifelse() from {base} and the function case_when() from {dplyr}. If you want to know more about these functions, look that the section on Re-code values from the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nHere we clean the data using a ‘chain’ of commands connected by pipes (%&gt;%), which is the grammar of the functions in the {Tidyverse}. The output is assigned to a new object called mpox_linelist to differentiate it from the raw data. It can be helpful to have both the cleaned and raw data available in the environment to compare to the original data if needed.\nSee the series of functions and the explanation in the comments.\n\n# Create a new object called mpox_linelist which is the clean version of the raw data\nmpox_linelist &lt;- mpox_linelist_raw %&gt;% \n  \n  # standardises names and puts all into lower case \n  clean_names() %&gt;% \n  \n  #transform ONE column into date (note the column names are lower case now)\n  mutate(date_of_notification = ymd(date_of_notification)) %&gt;%  \n\n  #transforms ALL columns starting with \"date\" into dates\n  mutate(across(starts_with(\"date\"), \n                .fns = ~ ymd(.x))) %&gt;%  \n  \n  #transforms UNK to NA across all character columns \n  mutate(across(where(is.character), \n                .fns = ~ ifelse(.x %in% c(\"UNK\", \"Unknown\"), NA_character_, .x)))  %&gt;% \n\n  # Recode the gender values to be more obvious  \n  mutate(gender = recode(gender,\n                         \"F\" = \"Female\",\n                         \"M\" = \"Male\",\n                         \"O\" = \"Other\")) %&gt;%\n  \n  #recode with ifelse to change only one or two categories based on a rule. \n  mutate(outcome = ifelse(outcome == \"A\", \"Alive\", outcome)) %&gt;%   \n  \n  #recode with case_when for more complex recoding \n  mutate(hiv_status = case_when(hiv_status == \"NEG\" ~ \"Negative\",    \n                                hiv_status == \"POS\" ~ \"Positive\")) %&gt;% \n  \n  mutate(sexual_orientation = case_when(sexual_orientation == \"BISEXUAL\" ~ \"Bisexual\",\n                                        sexual_orientation == \"HETERO\" ~ \"Heterosexual\",\n                                        sexual_orientation == \"MSM\" ~ \"MSM/homo or bisexual male\")) \n\nYou can then review your data by tabulating across all the different columns you have cleaned. See the preview of the HIV table below - it looks tidier now with more understandable categories, and all missing data is classified as ‘Unknown’.\n\n# Check that all changes have been made correctly\n\nskim(mpox_linelist)\n\ntabyl(mpox_linelist, gender)\n\ntabyl(mpox_linelist, clinical_symptoms)\n\ntabyl(mpox_linelist, outcome)\n\ntabyl(mpox_linelist, hiv_status)\n\ntabyl(mpox_linelist, sexual_orientation)\n\n\n\n hiv_status    n percent valid_percent\n   Negative  525  0.2625     0.6310096\n   Positive  307  0.1535     0.3689904\n       &lt;NA&gt; 1168  0.5840            NA\n\n\nIMPORTANT: If ‘unknown’ and NA had meaningful differences, combining them wouldn’t be appropriate (e.g., if ‘unknown’ meant the case was asked but didn’t want to respond, while NA meant they weren’t asked). Here, we assume no meaningful difference and want R to recognize them as missing.\n\n\n\n\n\n\n\n\nTest yourself!\n\nIs it always appropriate to combine different types of unknown data? (e.g. missing, unknown, did not respond, NA)\n\n Yes Depends on the meaning of those values No - never do this\n\n\n\nHow many male cases do we have in the data frame?\n\n 36 1960 65 1523\n\n\n\nHow many cases have ‘alive’ as an outcome?\n\n 1405 None 595\n\n\n\n\n\n\n\n\nIn a similar way, clean the aggregated data by:\n\nStandardising names to lower case\nEnsuring that date of reporting is of class “Date”\nCreating a column called “week_date” with the week of reporting starting on Monday\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nWe can first check the class of the DateRep column, which shows us that it was already recognized as a date column on import.\n\n# Check class of date of reporting column\nclass(mpox_agg_raw$DateRep)\n\nThen create a new object for the clean aggregate data, and write your cleaning coded connected with pipes.\n\n# Create a new object called mpox_agg which is the clean version of the raw data, applying the cleaning functions\n\nmpox_agg &lt;- mpox_agg_raw %&gt;% \n  \n  # standardises names and puts all into lower case\n  clean_names() %&gt;%  \n  \n  # create week column with Monday start\n  mutate(week_date = floor_date(date_rep, \n                              unit = \"week\",\n                              week_start = \"Monday\")) \n\n\n\n\n\n\n\n\n\nTest yourself!\n\nTake a look at the aggreate data. Which country reported the largest cumulative number of cases during the week 2022-04-11?\n\n Country A Country B Country C Country D Country E"
  },
  {
    "objectID": "pages/r_practical.html#step-5-describe-outbreak-by-person-place-and-time",
    "href": "pages/r_practical.html#step-5-describe-outbreak-by-person-place-and-time",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Now we’re getting to the heart of the investigation. Who is affected? Which locations are most affected, and how quickly is it spreading? Your ability to tell the classic “person, place, and time” story will be crucial to guiding the response. Pinpoint those hotspots and trends!\n\n\nTask: Using the mpox case linelist, create a table showing the total number of cases by country. This time, make the table more publication-friendly.\n\n\n Click to read a hint\n\n\nYou could use tabyl() like before, but an easy way to produce publication-ready tables is with the function tbl_summary() from {gtsummary} package. This formats the table for you. It will print to your Viewer rather than the console.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nCreate a new object with the table output - as this is a key output that you can then integrate into a document later rather than just viewing for now.\n\n# Create an object with the table\ncb_country_table &lt;- mpox_linelist %&gt;%\n\n  #select the column that we want to use in the table\n  select(country) %&gt;% \n  \n  # create the table. No need to specify columns; it will tabulate all available columns (selected above)\n  tbl_summary() \n\n# Print the table\ncb_country_table\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    country\n\n        CountryA\n816 (41%)\n        CountryB\n391 (20%)\n        CountryC\n474 (24%)\n        CountryD\n217 (11%)\n        CountryE\n102 (5.1%)\n  \n  \n  \n    \n      1 n (%)\n    \n  \n\n\n\n\n\n\n\n\n\n\n\n\nTest yourself!\n\nWhat country has the largest percentage of cases?\n\n Country C Country D Country B Country E Country A\n\n\n\n\n\n\n\n\nOkay so Country A has the most cases in total based on most recent data. But how does that change look over time?\nTasks:\n\nUsing the mpox case linelist, create an epicurve by week of notification\nUsing the mpox case linelist, create an epicurve by week of notification to enable a comparison of trends by country.\nUsing the mpox case linelist, create a heat plot with the number of cases by country and week of notification.\n\n\n\n Click to read a hint\n\n\nPrepare your data for the epicurve first. You can create a “week_date” column using the function floor_date() from {lubridate}. Take a look at the documentation to understand how it works and how to pick the starting day of the week.\nTo create the epicurve, you can use ggplot() and geom_bar(), which visualizes the number of rows within a group - e.g. number of cases per week. To compare trends in different countries, consider using the facet_wrap() function. If you are unsure on how ggplot() works, read the EpiRhandbook chapter on Epidemic curves.\nTo create a heatmap, you will need to create a table of counts by country and week of notification. You can do this using the functions group_by() and summarise() from {dplyr}. If you are unsure on how to do this, review the Grouping data chapter of the EpiRhandbook. Then, use the geom geom_tile() to create a heat plot. If you’re unsure on how to do this, read the EpiRhanbook section on Heat Plots\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nPrepare your data by creating the new column using mutate() and floor_date():\n\nmpox_linelist &lt;- mpox_linelist %&gt;% \n  # create week column with Monday start \n  mutate(week_date = floor_date(date_of_notification, unit = \"week\", week_start = \"Monday\")) \n\nThe code below creates an epicurve using ggplot() and the geom_bar() function, then applies further formatting. With geom_bar(), you only need to specify the x axis, and the function will visualize the number of rows per unique x axis value.\n\n# Open up the plot production with ggplot() function, specifying object and columns\nepicurve_mpox &lt;- ggplot(data = mpox_linelist,          \n                        aes(x = week_date)) +    \n  \n  geom_bar(fill=\"darkgreen\",                     #colour inside the bins\n                 color=\"white\",                  #outline colour of the bins\n                 alpha=0.8) +                    #transparency of the bins\n  \n  scale_x_date(breaks = \"2 weeks\") +             #set the x axis labels to two week intervals\n\n  labs(title=\"Mpox cases reported in 2022 in Countries A, B, C, D, and E\",\n       subtitle = \"Date as of August 31st 2022\") +  #add a title\n  \n  theme_minimal() +                             #assign a predefined theme\n  \n  theme(axis.text = element_text(size=9),       #define the font size of the axis text\n        axis.title = element_blank(),           #remove the titles of the x and y axis \n        axis.text.x = element_text(angle=90))   #rotate the x axis text\n           \n# Print the epicurve\nepicurve_mpox\n\n\n\n\nTo examine how the outbreak spread by country, add facet_wrap() to your ggplot code. This splits the graph into multiple smaller ones. As shown below, you can even simply add the function to the national epicurve object.\nAn alternative approach would be to create a stacked epicurve, i.e. retain the single epicurve but split each bar into colors per country. You would do this by adding fill = country to the aes() in the epicurve code. However, we don’t recommend this for comparing trends, as stacked bars make it harder to see individual patterns.\n\nepicurve_epox_country &lt;- epicurve_mpox + \n \n   # Facet wrap to make mini-plots, specifying that you want two columns of plots. \n  facet_wrap(.~country,\n             ncol = 1) \n\n# Print the epicurve\nepicurve_epox_country\n\n\n\n\nFinally, if you want to demonstrate this as a weekly heatmap, you can use geom_tile(). First, aggregate the data by week. Then pipe into a ggplot(), as shown below.\n\n# Assign the output of your ggplot code to a new object\nhp_mpox &lt;- mpox_linelist %&gt;% \n  \n  #first count the number of cases by country and notification week\n  count(country, week_date) %&gt;% \n\n  #you can pipe directly into the ggplot\n    ggplot(aes(x = week_date, # notification week along the x axis\n           y = country,       # country along the y axis\n           fill = n)) +       # colour in the heatmap tiles by number\n  \n  # specify that you want this to be a heatmap with geom_tile()\n  geom_tile(colour = \"black\") +   # black is the outline of each tile\n  \n  #define the gradient of the colours\n  scale_fill_gradient(            \n    low = \"lightgreen\",\n    high = \"red\") +\n  \n  #set the x axis labels to two week intervals\n  scale_x_date(breaks = \"2 weeks\") +             \n  \n  # Add titles\n  labs(\n    title= \"Mpox cases by country and week of notification\",\n    fill = \"Number of cases\"                               \n  ) +\n  \n  # Apply an overall theme to your plot\n  theme_bw() +\n  \n  # Customize other appearance details\n  theme(legend.position = \"bottom\",       #legend position to bottom\n        axis.text = element_text(size=9),     #define axis font \n        axis.title = element_blank(),         #remove the axis titles\n        axis.text.x = element_text(angle=90)) #rotate the x axis text\n    \n\n# Print the heatmap\nhp_mpox \n\n\n\n\n\n\n\n\n\nNext, describe the age, gender, and sexual orientation of cases. What is interesting?\nTask:\n\nCreate a single table showing overall distribution of age, gender, and sexual orientation\nCreate an age-gender pyramid showing age as 10-year age bands\n\n\n\n Click to read a hint\n\n\nTo quickly create a presentation-ready table showing the breakdowns for three different columns, consider using the function tbl_summary() from {gtsummary}.\nTo create an age-gender pyramid, first create a new column with the function age_categories() from the {epikit} package. Then explore the function age_pyramid() from the {apyramid} package.You can find more about this function in the EpiRhandbook chapter Demographic pyramids and Likert-scales\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nSee below the code to quickly generate one table with the breakdown of different variables. The function tbl_summary() by default summarizes columns differently depending on their class:\n\nAge is a numeric column, so is summarized with a median and interquartile range.\nGender and sexual orientation are character values, so are described in terms of counts and percentages.\n\nYou can customize this further; explore the documentation by typing ?tbl_summary() in your console.\nNote that tbl_summary() by default does not include NAs in the counts and percentages, allowing you to see the distribution of non-missing values.\n\n# Create table of all three variables\ntab_demographics &lt;- mpox_linelist %&gt;% \n  \n  # select the columns of interest for\n  select(age, gender, sexual_orientation) %&gt;% \n  \n  # use tbl_summary() to create the table\n  tbl_summary() \n\ntab_demographics\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    age\n37 (31, 45)\n        Unknown\n3\n    gender\n\n        Female\n36 (1.8%)\n        Male\n1,960 (98%)\n        Other\n1 (&lt;0.1%)\n        Unknown\n3\n    sexual_orientation\n\n        Bisexual\n7 (0.8%)\n        Heterosexual\n46 (5.2%)\n        MSM/homo or bisexual male\n833 (94%)\n        Unknown\n1,114\n  \n  \n  \n    \n      1 Median (IQR); n (%)\n    \n  \n\n\n\n\nCreate the new age group column as follows. You can add this to the cleaning section of your script (which we covered 4.1).\n\nmpox_linelist &lt;- mpox_linelist %&gt;% \n  # Use the age_categories function to create age categories\n  mutate(age_group = age_categories(age, lower = 0, #set up the lower age\n                                    upper = 70, #set up the upper age\n                                    by = 10)) #set up the age breaks\n\nThen make the age-gender pyramid using the age_pyramid() function. It is a function that builds on ggplot, so you can then continue to add on customization, such as the theme_bw() below.\n\n# Create table of all three variables\nfigure_agesex &lt;- mpox_linelist %&gt;% \n  \n  # Filter to male and female only\n  filter(gender %in% c(\"Male\", \"Female\")) %&gt;% \n  \n  # select the columns of interest for\n  age_pyramid(age_group = \"age_group\",\n              split_by = \"gender\") +\n  \n  # change theme\n  theme_bw()\n\nfigure_agesex\n\n\n\n\n\n\n\n\n\n\n\n\nTest yourself!\n\nWhich demographic group is more affected by Mpox?\n\n Females 60-69 Males 40-49 Females 10-19 Males 30-39\n\n\n\nWhat proportion of mpox cases were homosexual or bisexual men?\n\n 41% 42% 5% 94%\n\n\n\n\n\n\n\n\nThe media is starting to call your office and are asking what symptoms the public should look out for. Just in luck - you can check that out in the data too!\nTasks:\n\nCreate a table with the distribution of different symptoms and outcomes.\n\nNo hints! You should know this one by now!\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Table with number and percentage of cases by outcome\n\ntab_outcome &lt;- mpox_linelist %&gt;% \n  \n  # Select the columns for tabulation\n  select(outcome, clinical_symptoms) %&gt;% \n  \n  # Use tbl_summary() - note that this time we are adding on labels to change how the column name is displayed\n  tbl_summary(label = list(\n    clinical_symptoms = \"Symptoms\",\n    outcome = \"Reported outcome\")) \n\ntab_outcome\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      N = 2,0001\n    \n  \n  \n    Reported outcome\n\n        Alive\n1,405 (100%)\n        Unknown\n595\n    Symptoms\n\n        Lesions\n14 (0.7%)\n        Rash\n257 (13%)\n        Rash, Lesions\n323 (16%)\n        Rash, Systemic symptoms\n676 (34%)\n        Rash, Systemic symptoms, Lesions\n654 (33%)\n        Systemic symptoms\n28 (1.4%)\n        Systemic symptoms, Lesions\n36 (1.8%)\n        Unknown\n12\n  \n  \n  \n    \n      1 n (%)"
  },
  {
    "objectID": "pages/r_practical.html#step-6-reviewing-data-quality",
    "href": "pages/r_practical.html#step-6-reviewing-data-quality",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "You’ve described a lot now, but you want to make sure you understand how timely and complete your mpox linelist is, especially if it will be the basis of making decisions.\nFor example - is it possible that there are very different reporting delays between countries, meaning current case counts are not directly comparable? Oh dear, must check.\n\n\nTasks\n\nCalculate median time from symptom onset to diagnosis and from diagnosis to notification, both overall and by country\nAssess visually the number of cases by calendar period and type of date (onset, diagnosis and notification)\n\n\n\n Click to read a hint\n\n\nTo plot together the different dates you may need to transform your data from “wide” to “long” form. What we call “pivoting” in R. The objective is to have a column with the different date categories (onset, diagnosis and notification) and another column with their date value. If you are unsure on how to do this, have a look at the Pivoting data chapter of the EpiRhandbook. Then, try to plot with the daily values, but if that’s not easy to interpret you may want to aggregate cases by week.\n\n\n\n\n\n\n\n\nTest yourself!\n\nIs there a difference in the delay from diagnosis to notification by country?\n\n Yes No\n\n\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nFirst create the required columns for this analysis.\n\n# Create two columns in linelist to assess delays\ndelay_db &lt;- mpox_linelist %&gt;% \n  \n  # Time between onset and diagnosis (converted to a number)\n  mutate(delay_diag = as.numeric(date_of_diagnosis - date_of_onset)) %&gt;%   \n\n  # Time between diagnosis and notification (converted to a number)\n  mutate(delay_not = as.numeric(date_of_notification - date_of_diagnosis)) \n\nUse the summary function from base R to quickly view the median, mean, interquartile range, and rang.\n\n# Summarize the delays to diagnosis\nsummary(delay_db$delay_diag) \n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n -2.000   4.000   7.000   7.758  10.000  66.000     897 \n\n# Summarize the delays from diagnosis to notification\nsummary(delay_db$delay_not)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max.     NA's \n-46.0000  -2.0000   0.0000  -0.6078   1.0000  23.0000      715 \n\n\nUse group_by() and summarize() to create a table with median delays per country.\n\ndelay_country &lt;- delay_db %&gt;% \n  \n  # Group by country\n  group_by(country) %&gt;% \n  \n  # Create columns for each delay\n  summarise(median_delay_diag = median(delay_diag, na.rm = T),\n            median_delay_not = median(delay_not, na.rm = T))\n\ndelay_country\n\n# A tibble: 5 × 3\n  country  median_delay_diag median_delay_not\n  &lt;chr&gt;                &lt;dbl&gt;            &lt;dbl&gt;\n1 CountryA                 7                0\n2 CountryB                 7                0\n3 CountryC                 6                0\n4 CountryD                 7                0\n5 CountryE                 6                0\n\n\nTo explore how the trends in cases over time differ when using different dates, you can reshape the linelist to create a dataset with one row per date type per case.\n\n# Prepare the data\ndates_longer &lt;- mpox_linelist %&gt;% \n  \n  select(age, gender, sexual_orientation, starts_with(\"date_\")) %&gt;% \n\n  pivot_longer(\n    \n      # all columns starting with \"date_\" will be pivoted from wide to long \n      cols=starts_with(\"date_\"),         \n    \n      # put names of the columns into a single column called \"indicator\"\n      names_to = \"indicator\",   \n      \n      # the date values will be placed in a column called \"date\"\n      values_to = \"date\")                \n\nThe data will then look like this, with three rows per case:\n\n\n\n\n\n\n\nThen tabulate cases by week per indicator\n\n# Create new object\ndates_longer_week &lt;- dates_longer  %&gt;% \n\n  # Create a new week column\n  mutate(week_date = floor_date(date, unit = \"week\", week_start = \"Monday\")) %&gt;%  \n  \n  # Within each combination of indicator and week, calculate the number of cases\n  group_by(indicator, week_date) %&gt;% \n  summarise(n=n()) %&gt;%   \n  \n  # drop the cases with no data on dates  \n  drop_na(week_date)                     \n\nThe data will then look like this, with three rows per case:\n\n\n\n\n\n\n\nFinally, create a plot with ggplot() and geom_line().\n\nplot_date_delay &lt;-   ggplot(data = dates_longer_week,\n                            aes(x = week_date, \n                                y = n, \n                                color=indicator)) +\n  \n  geom_line(linewidth = 1.5) +\n  \n  scale_x_date(breaks = \"2 weeks\")+\n  \n  theme_bw() +\n  \n  theme(legend.position = \"bottom\", \n        axis.text = element_text(size=9),\n        axis.title = element_blank(),\n        axis.text.x = element_text(angle=90),\n        legend.title = element_blank()) +\n  labs(title=\"Mpox cases reported in 2022, by date of onset, diagnosis and notification.\")\n\nplot_date_delay\n\n\n\n\n\n\n\n\n\nFinally, you remember that all-along you’ve had these aggregate counts from routine surveillance. You find out that these numbers are actually already being published.\nBefore you share your own numbers, you’d better check how different they are from already-published statistics!\nTask: Create a plot comparing the number of cases reported to through the case-based flow and through the aggregated flow in each country.\nNOTE: Take into consideration that the column on cases in the aggregated data frame reports the cumulative number of cases.\n\n\n\n\n\n\nTest yourself!\n\nWhich country is not reporting aggregated data?\n\n A B C D E\n\n\n\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nFirst, create a data frame of country totals from the aggregate data.\n\nmpox_agg_country &lt;- mpox_agg %&gt;% \n \n  # as we have cumulative data, we keep only the last week per country \n  group_by(country) %&gt;% \n  filter(date_rep == max(date_rep)) %&gt;% \n  \n  # remove unnecessary columns\n  select(-date_rep, -week_date) %&gt;%     \n\n  # create this column to distinguish the numbers from the linelist data\n  mutate(source = \"aggregated\")         \n\nThen create a data frame of country totals from the case linelist, and append it to the totals from the aggregate data.\n\nmpox_linelist_country &lt;- mpox_linelist %&gt;%\n  \n  # count cases by country, use the same column name as in the aggregate data\n  group_by(country) %&gt;% \n  summarise(cases = n()) %&gt;% \n  \n  # create this column to distinguish the numbers from the linelist data\n  mutate(source = \"case_based\")       \n  \n\n# Append both data frames. Remember this is different from merging\ntotal_data &lt;- bind_rows(mpox_linelist_country, mpox_agg_country)\n\nYou can now use this data to compare the cases reported in both sources, using ggplot().\n\ngraph_comp &lt;- ggplot(data = total_data,\n                     aes(x = source, \n                         y = cases, \n                         fill = source)) +\n  \n  #position dodge puts bars one next to each other, instead of \"stacked\"\n  geom_col(position = \"dodge\") +            \n  \n  # this command gives us one graph per country. The argument scales allows each y axis scales to adjust to the data\n  facet_wrap(~ country, scales = \"free_y\") +  \n\n  # changes the colours, but with the argument \"labels\" we can change the text of each fill.\n  scale_fill_viridis_d(\n    labels = c(\"Aggregated\", \"Case-based\")) +\n  \n  labs(\n    title = \"Number of cases of Mpox reported in 2022 according to source of data\",\n    fill = \"Source\",\n    x = \"\",\n    y = \"Total number of cases\"\n  ) + \n  \n  theme_bw() +\n  \n  # we remove the text of the x axis because it is already present in the legend\n  theme(axis.text.x = element_blank(),   \n        \n   # we also remove the ticks for aesthetic purposes\n        axis.ticks.x = element_blank())    \n\ngraph_comp\n\n\n\n\n\nInteresting! There are some differences - and this probably will be worth flagging with stakeholders and/or explaining in a footnote somewhere."
  },
  {
    "objectID": "pages/r_practical.html#final-thoughts",
    "href": "pages/r_practical.html#final-thoughts",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Well done! Through your analysis you now understand the magnitude of the outbreak so far, where and when it spread, which demographic groups are most affected, and how the disease actually manifests in terms of symptoms and severity. ECDC is very happy with your work.\nBy coding this up in R, this analysis should be reproducible, meaning you can quickly update it with new data and keep monitoring the outbreak.\nOf course, the above data is not real. If you want to see a paper on the actual outbreak that occured in Europe in 2022, you can take a look at this Eurosurveillance paper. This ECDC page on Mpox also publishes updates on the status of mpox in Europe.\nTo further practise reproducible reports, [link to RMarkdown]."
  },
  {
    "objectID": "pages/r_practical.html#case-study-information",
    "href": "pages/r_practical.html#case-study-information",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Authorship\nOriginal authors: Xanthi Andrianou, Gianfranco Spiteri (ECDC EI Group)\nData source: Fictional data provided by ECDC EI Group for training purposes\n\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nOctober 2021\nFirst draft\n1\nXanthi Andrianou\n\n\nJune 2024\nAdapted to case study template\n1.1\nAlberto Mateo Urdiales\n\n\nSeptember 2024\nRevise for case study repository\n1.2\nPaula Blomquist and Alanah Jansen"
  },
  {
    "objectID": "pages/r_practical.html#terms-of-use",
    "href": "pages/r_practical.html#terms-of-use",
    "title": "Descriptive analysis of the 2022 Mpox outbreak in Europe",
    "section": "",
    "text": "Disclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/"
  },
  {
    "objectID": "pages/tbe.html",
    "href": "pages/tbe.html",
    "title": "TBE - Linear regression (ENG)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nTBE - linear regression\n\n\nLanguage:\nEnglish\n\n\nTool:\nR; DAGitty\n\n\nLocation:\nGermany\n\n\nScale:\nNational\n\n\nDiseases:\nTBE\n\n\nKeywords:\nTBE; Linear Regression; R\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nIntermediate\n\n\n\nAuthorship\nOriginal authors: Teresa Nygren (RKI), Alicia Barrasa Blanco (UK FETP), Jan Walter (RKI) and Achim Dörre (RKI)\nData source: Data is fictional and was inspired by Nygren et al. Tick-borne encephalitis: acute clinical manifestations and severity in 581 cases from Germany, 2018-2020. Journal of Infection. 2023 Apr 1;86(4):369-75\nAdapted by: Liese Van Gompel (MediPIET), Joana Gomes Dias (ECDC), Chiara Entradi (ECDC) and Alberto Mateo Urdiales (ISS)\n\n\n\n\n\n\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\n\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\nDisclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/\n\n\n\n\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\nWrite date of first version\nWrite any revisions made to the case study\n\n\n\n\n\n\n\n\nDate\nChanges made\nAuthor\n\n\n\n\n2023\nRevision R code\nLiese Van Gompel (MediPIET)\n\n\n2024\nRevision R code\nJoana Gomes Dias and Chiara Entradi (ECDC)\n\n\n2024\nRevision of content, structure, R code and adaptation of format to Applied Epi’s template of case studies\nAlberto Mateo Urdiales (ISS)\n\n\n\n\n\n\n\n\n\n\nAt the end of the case study, participants should be able to:\n\nUse directed acyclic graphs (DAG) to identify variables suitable to control for confounding;\nTo perform linear regression in R;\nTo write down the associated models and interpret them.\n\n\n\n\nParticipants are expected to be familiar with directed acyclic graphs (DAGs) and the use of DAGitty (a browser-based environment for creating DAGS) for the first part; and with data management as well as descriptive and stratified analysis in R for the second part.\n\n\n\nInclude the steps needed to start replicating the analysis of the case study\nFor example:\n\nDownload folder named tbe_en and extract contents in the local laptop\nCreate an Rstudio project in the folder tbe_en. If you are unsure on how to do that, read the EpiRhandbook Chapter on R projects\nInside the folder “tbe_en”: Subfolder “data” contains a raw data file named tbe.RDS. This is the only data file you will use in this case study.\nSubfolder scripts should be used to save any scripts related to the analysis. Inside “backup” you will find a solution script with the code of the case study named tbe_lr_backup.R. You will also find an image which corresponds to the proposed DAG solution.\nSubfolder “outputs” could be used to store all outputs (tables, graphs, documents) that are the result of the analysis\nYou will also find inside “tbe_en” a word document called starter_guide_DAGitty.docx in case you need help using this website"
  },
  {
    "objectID": "pages/tbe.html#overview",
    "href": "pages/tbe.html#overview",
    "title": "TBE - Linear regression (ENG)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nTBE - linear regression\n\n\nLanguage:\nEnglish\n\n\nTool:\nR; DAGitty\n\n\nLocation:\nGermany\n\n\nScale:\nNational\n\n\nDiseases:\nTBE\n\n\nKeywords:\nTBE; Linear Regression; R\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nIntermediate\n\n\n\nAuthorship\nOriginal authors: Teresa Nygren (RKI), Alicia Barrasa Blanco (UK FETP), Jan Walter (RKI) and Achim Dörre (RKI)\nData source: Data is fictional and was inspired by Nygren et al. Tick-borne encephalitis: acute clinical manifestations and severity in 581 cases from Germany, 2018-2020. Journal of Infection. 2023 Apr 1;86(4):369-75\nAdapted by: Liese Van Gompel (MediPIET), Joana Gomes Dias (ECDC), Chiara Entradi (ECDC) and Alberto Mateo Urdiales (ISS)"
  },
  {
    "objectID": "pages/tbe.html#instructions",
    "href": "pages/tbe.html#instructions",
    "title": "TBE - Linear regression (ENG)",
    "section": "",
    "text": "There are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\n\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\nDisclaimer: The information presented in this exercise and the associated data files have been deliberately changed so as to facilitate the acquisition of the learning objectives for fellows of EPIET, EUPHEM and EPIET-associated programmes. This case study was first introduced in 2022 (see Copyright and Licence agreement for more information).\nYou are free:\n\nto Share: to copy and distribute the work\nto Remix: to adapt and build upon the material\n\nUnder the following conditions:\n\nAttribution: You must attribute the work in the manner specified by the author or licensor (but not in any way that suggests that they endorse you or your use of the work). The best way to do this is to keep as it is the list of contributors: sources, authors and reviewers.\nShare Alike: If you alter, transform, or build upon this work, you may distribute the resulting work only under the same or similar license to this one. Your changes must be documented. Under that condition, you are allowed to add your name to the list of contributors.\nNotification: If you use the work in the manner specified by the author or licensor, Walter@rki.de\nYou cannot sell this work alone but you can use it as part of a teaching.\n\nWith the understanding that:\n\nWaiver: Any of the above conditions can be waived if you get permission from the copyright holder.\nPublic Domain: Where the work or any of its elements is in the public domain under applicable law, that status is in no way affected by the license.\nOther Rights: In no way are any of the following rights affected by the license:\n\nYour fair dealing or fair use rights, or other applicable copyright exceptions and limitations;\nThe author’s moral rights;\nRights other persons may have either in the work itself or in how the work is used, such as publicity or privacy rights.\n\nNotice: For any reuse or distribution, you must make clear to others the license terms of this work by keeping together this work and the current license.\n\nThis licence is based on http://creativecommons.org/licenses/by-sa/3.0/\n\n\n\n\n\nYou can write feedback and suggestions on this case study at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\nWrite date of first version\nWrite any revisions made to the case study\n\n\n\n\n\n\n\n\nDate\nChanges made\nAuthor\n\n\n\n\n2023\nRevision R code\nLiese Van Gompel (MediPIET)\n\n\n2024\nRevision R code\nJoana Gomes Dias and Chiara Entradi (ECDC)\n\n\n2024\nRevision of content, structure, R code and adaptation of format to Applied Epi’s template of case studies\nAlberto Mateo Urdiales (ISS)"
  },
  {
    "objectID": "pages/tbe.html#guidance",
    "href": "pages/tbe.html#guidance",
    "title": "TBE - Linear regression (ENG)",
    "section": "",
    "text": "At the end of the case study, participants should be able to:\n\nUse directed acyclic graphs (DAG) to identify variables suitable to control for confounding;\nTo perform linear regression in R;\nTo write down the associated models and interpret them.\n\n\n\n\nParticipants are expected to be familiar with directed acyclic graphs (DAGs) and the use of DAGitty (a browser-based environment for creating DAGS) for the first part; and with data management as well as descriptive and stratified analysis in R for the second part.\n\n\n\nInclude the steps needed to start replicating the analysis of the case study\nFor example:\n\nDownload folder named tbe_en and extract contents in the local laptop\nCreate an Rstudio project in the folder tbe_en. If you are unsure on how to do that, read the EpiRhandbook Chapter on R projects\nInside the folder “tbe_en”: Subfolder “data” contains a raw data file named tbe.RDS. This is the only data file you will use in this case study.\nSubfolder scripts should be used to save any scripts related to the analysis. Inside “backup” you will find a solution script with the code of the case study named tbe_lr_backup.R. You will also find an image which corresponds to the proposed DAG solution.\nSubfolder “outputs” could be used to store all outputs (tables, graphs, documents) that are the result of the analysis\nYou will also find inside “tbe_en” a word document called starter_guide_DAGitty.docx in case you need help using this website"
  },
  {
    "objectID": "pages/tbe.html#goal-1-draw-a-directed-acyclic-graph-dag",
    "href": "pages/tbe.html#goal-1-draw-a-directed-acyclic-graph-dag",
    "title": "TBE - Linear regression (ENG)",
    "section": "Goal 1: Draw a Directed Acyclic Graph (DAG)",
    "text": "Goal 1: Draw a Directed Acyclic Graph (DAG)\nSince you are interested in a causal question, please draw a DAG. If you want to use a computer, you may try http://www.dagitty.net/.\n\nWhich variables would you need to adjust for?\n\nIf you are new to DAGitty you can find a few helpful information in the document called starter_guide_DAGitty.docx present in the “tbe_en” folder you have downloaded in your laptop (See Preparation for the case study).\nOnce you have drawn your DAG, click on the solution below to see the DAG we propose.\n\n\nClick to see the solution\n\n\nWhen planning the study, the epidemiologist considered this DAG:\n\n\n\n\nAccording to this DAG, if you want to explore the association between hypertension and severe TBE, you should adjust for:\n\nTBE diagnosis\nTBE vaccination\nage\nlarge tick (=large viral load)\nmonophasic course\nother comorbidities\nsex.\n\nTBE Diagnosis is controlled by design (only cases are included).\nProbably your DAG will look differently. This is absolutely fine, since there is not only one possible DAG. But you should be able to justify your DAG based on the existing evidence."
  },
  {
    "objectID": "pages/tbe.html#goal-2-perform-linear-regression-in-r",
    "href": "pages/tbe.html#goal-2-perform-linear-regression-in-r",
    "title": "TBE - Linear regression (ENG)",
    "section": "Goal 2: Perform linear regression in R",
    "text": "Goal 2: Perform linear regression in R\nNow we will work on the data frame provided which includes data for 523 patients who have been hospitalized with TBE in the years 2018 to 2020 in Germany and for whom data were collected.\nThe following variables are provided:\nTable 1: Data dictionary for the dataframe “tbe.RDS”:\n\n\n\nVariable\nDescription\nValues\n\n\n\n\nage\nage in years\ncontinuous\n\n\nhyper\nhypertension\n1= yes, 0=no\n\n\nvac\nvaccinated against TBE\n1= yes, 0=no\n\n\nmono\nmonophasic disease course\n1= yes, 0=no\n\n\nother\nother comorbidities\n1= yes, 0=no\n\n\ntick\nlarge tick at removal\n1= yes, 0=no\n\n\nsex\nsex\n1= female, 0= male\n\n\nhospd\nlength of hospitalisation in days\ncontinuous\n\n\n\n\nStep 1: Set up\n\nStep 1.1: Create a new R script\nOnce you have created an Rproject inside the “tbe_en” folder (as specified in the second point of the section Preparation for the case study). Create a new script with the name tbe_lr and save it in the subfolder “scripts”. If you are familiar with Rmarkdown, you may decide to use this type of file instead of a standard R script.\n\n\nStep 1.2: Define R language\nDepending on where you are and how you carried out R installation, your language “locale” might be different from the language of the graphs that you want to produce. For example, a french person might have a french “locale”. If that is the case, when creating a graph by day of the week, Monday will be displayed as “lundi”. If that french person wants to create an English report, as for this case study, the language “locale” should be changed.\nTask: Ensure your “locale” is in English and change it into English if it is not. If you don’t know how to do this try finding it online (searching for online help is an important skill for R users!). Otherwise, see the solution below\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# To see your language locale\nSys.getlocale()\n\n# To change it into English\nSys.setlocale(\"LC_ALL\", \"English\")\n\n\n\n\n\nStep 1.3: Install/load packages\nInstall and load the following packages: rio, skimr, janitor, gtsummary, broom, rstatix, ggfortify and tidyverse.\nYou can find more about installing/loading packages in the Packages section of the EpiRhandbook.\n\n\n Click to read a hint\n\n\nYou may end up using a long list of packages. Unfortunately different packages have functions with the same name. For example, the package {dplyr} (already installed with {tidyverse}) has a function called select() which we frequently use to subset columns of a data frame. But other packages such as {MASS} do also have a function called select(). This could create headaches if you want to subset columns using dplyr’s select() but R thinks you’re calling MASS’s select() (we call this masking - dplyr::select() is masked by MASS::select()). Given that you are more likely to use functions from {tidyverse}, ensure that this is the last package in your p_load() list so that functions from {tidyverse} (including {dplyr} functions) will always “prevail”.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  rio,        # importing data  \n  skimr,      # get overview of data\n  janitor,    # data cleaning and tables\n  gtsummary,  # summary statistics, tests and regressions \n  broom,      # to generate tidy tibbles of regression analysis\n  rstatix,    # for statistics, including statistical tests\n  ggfortify,  # data visualisation for statistical analysis results\n  tidyverse  # data management and visualization\n)\n\n\n\n\n\n\nStep 2: Import and explore data\n\nStep 2.1: Import the data and brief exploration\nImport the data frame called “tbe.RDS” inside the “data” subfolder. If you are working within a project, finding the path to the dataframe should be relatively straightfoward. An “.RDS” file is an R object file. You can import this dataframe using the readRDS() function from {base}. However, we recommend that you use the import() function from {rio} because, as you may remember, this function will recognise the file type and import it whether the file is from R, Stata, excel or many others. If you have any doubts about importing review the Import and export chapter of the EpiRhandbook.\nThen, explore the data trying to answer the following questions:\n\nQUESTION: How many columns does the dataframe have?\n\n 523 2 8 6\n\n\n\nQUESTION: How many rows have missing the column ‘Other comorbidities’?\n\n 958 22 18 39\n\n\n\nQUESTION: How many cases have hypertension?\n\n 119 191 332 285\n\n\n\nQUESTION: How many character columns does the dataframe have?\n\n 0 6 2 8\n\n\n\nQUESTION: What is the difference between a column of class ‘character’ and a column of class ‘factor’?\n\n There is no difference between these classes, those are synonims Character columns contain text, whereas factors contain numbers Factors are used when we have more than 5 categories of data Both classes contain text, but factors are used when there are a limited number of unique character strings and they often represent categorical data\n\n\n\n\n Click to read a hint\n\n\nAn efficient way to explore data is to use the function skim() from the {skimr} package, as it gives you all the information needed with only one command. Of course, there are several alternatives.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Import the data\n\ntbe &lt;- import(\"data/tbe.RDS\")\n\n\n# Explore the dataframe\nskim(tbe)\n\n\nData summary\n\n\nName\ntbe\n\n\nNumber of rows\n523\n\n\nNumber of columns\n8\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n6\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nhyper\n0\n1.00\nFALSE\n2\nno: 404, yes: 119\n\n\nvac\n0\n1.00\nFALSE\n2\nno: 503, yes: 20\n\n\nmono\n0\n1.00\nFALSE\n2\nno: 285, yes: 238\n\n\nother\n22\n0.96\nFALSE\n2\nyes: 317, no: 184\n\n\ntick\n0\n1.00\nFALSE\n2\nno: 345, yes: 178\n\n\nsex\n0\n1.00\nFALSE\n2\nmal: 332, fem: 191\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nage\n18\n0.97\n47.30\n18.97\n1\n34\n47.0\n61\n90\n▂▅▇▆▂\n\n\nhospd\n39\n0.93\n42.23\n13.59\n3\n33\n41.5\n52\n86\n▁▆▇▃▁\n\n\n\n\n\n\n\nNormally, at this point we would start cleaning our data. Fortunately for you, the tbe data has already been cleaned, so you can jump directly to the fun part. However, feel free to rename/recode or change any aspect of the dataframe to accomodate it to your personal preferences.\n\n\nStep 2.2: Inspect factor columns\nAs we saw before, we have 6 factor columns representing categorical variables in our dataframe. Although we looked at them with the skim() function, explore them further with the tabyl() function from the {janitor} package.\n\n\n Click to read a hint\n\n\nTo save time, try to always use functions that allow you to apply the same function to many different objects (e.g., multiple columns) simultaneously. You can achieve this using several approaches, such as loops, lapply or purrr. Here we give the solution with purrr, so if you want to explore further purrr have a look at the dedicated section in the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n### One by one\ntabyl(tbe, hyper)\n\n\n\n\n\nhyper\nn\npercent\n\n\n\n\nno\n404\n0.7724665\n\n\nyes\n119\n0.2275335\n\n\n\n\n\ntabyl(tbe, vac)\n\n\n\n\n\nvac\nn\npercent\n\n\n\n\nno\n503\n0.9617591\n\n\nyes\n20\n0.0382409\n\n\n\n\n\n#### All at once\ntbe %&gt;% \n  \n  select(where(is.factor)) %&gt;% #we first select only the columns that are of class 'factor'\n  \n  map(.f = tabyl)              #inside map() from {purrr} we specify the function we want to apply to the entire dataframe\n\n$hyper\n .x[[i]]   n   percent\n      no 404 0.7724665\n     yes 119 0.2275335\n\n$vac\n .x[[i]]   n    percent\n      no 503 0.96175908\n     yes  20 0.03824092\n\n$mono\n .x[[i]]   n   percent\n      no 285 0.5449331\n     yes 238 0.4550669\n\n$other\n .x[[i]]   n    percent valid_percent\n      no 184 0.35181644     0.3672655\n     yes 317 0.60611855     0.6327345\n    &lt;NA&gt;  22 0.04206501            NA\n\n$tick\n .x[[i]]   n   percent\n      no 345 0.6596558\n     yes 178 0.3403442\n\n$sex\n .x[[i]]   n   percent\n    male 332 0.6347992\n  female 191 0.3652008\n\n\n\n\n\n\nStep 2.3: Histogram with length of hospitalisation\nOur outcome variable will be length of hospitalisation in days (column hospd). It might be worth it looking at this column in more detail, as its characteristics may influence how will carry out the analysis later on. One important aspect is to check its distribution and ascertain, at least visually, if it follows a normal distribution.\nTask Create an histogram with the distribution of hospd. Try adding the normal curve to this histogram.\n\nQUESTION: Does lenght of hospitalisation look like normally distributed?\n\n Yes No\n\n\n\n\n Click to read a hint\n\n\nThere are many ways to create an histogram in R, but try using the package {ggplot2}. You can have a look at the ggplot basics chapter of the EpiRhandbook if you struggle.\nAdding the normal curve to the histogram may prove quite challenging. Do not worry if you don’t manage. One hint is that, in the histogram, you will need to display the density and not the frequency count. Try asking a search engine or any AI platform for help. Most of us use these tools on a daily basis to ask for help in R.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%                                  #we call the data first and we pass it into ggplot with the pipe operator\n  \n  ggplot(mapping = aes(x = hospd)) +     #when drawing an histogram we only need to specify the x-axis   \n  \n  geom_histogram(aes(y = ..density..)) + #here we are telling ggplot2 to display the density and not the freq count\n  \n  #the function below will add the normal curve. \n  stat_function(fun = dnorm,  #In the fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(tbe$hospd, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(tbe$hospd, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) # Identify the colour and line width of the normal curve\n\n\n\n\nNow, that was tough! But we’re here to push you out of your comfort zone. Let’s go into more detail about what we have done.\nBy now, you should feel comfortable creating a basic histogram using ggplot, so let’s focus on the new things. We have added another aesthetic to the geom_histogram() in which we specify that we want the density plotted and not the frequency count. Why is that? Displaying the density is more appropriate when we want to focus on the shape of the data, as we can see the underlying probability distribution more clearly.\nBut, what is actually the density? The density represents the relative frequency, what we do is scale the y-axis so that the area under the histogram equals 1, normalising the histogram to represent probabilities (density) rather than raw counts. In fact, look at how the y-axis changes when you represent the density and when you represent the counts.\nFinally, why are we putting two dots before and after density in the aes()? The double dots before and after ..density.. are a special syntax used within ggplot2. They indicate an internal variable that ggplot2 calculates during the plotting process. So, ggplot2 normally calculates the density for histograms, but it does not display it unless you specify it (with this syntax).\n\n\n\n\nStep 2.4: Create a cross-table and calculate a statistical test\nLet’s say that we now want to explore whether sex is associated with hypertension. To find this out, create a cross-table displaying these two variables and calculate the appropriate statistical test to know if there is a statistical association between them.\n\n\n Click to read a hint\n\n\nThere are several ways in which you can do this. You could, for example, create the cross-table with tabyl() and then separately calculate the statistical test. The easiest way would be to use the tbl_summary() function from the {gtsummary} package, which allows you to do both, the cross tabulation and the statistical tests, in the same command. You should be familiar with this package by now, but if you need a little refresher have a look at the dedicated chapter of the EpiRhandbook.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;% \n  select(hyper, sex) %&gt;%   #we select the columns we are interested in\n  tbl_summary(by = hyper) %&gt;% #we specify that we want by hypertension status\n  add_p()                     # adding this command will calculate the most appropriate statistical test\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      no, N = 4041\n      yes, N = 1191\n      p-value2\n    \n  \n  \n    sex\n\n\n0.5\n        male\n253 (63%)\n79 (66%)\n\n        female\n151 (37%)\n40 (34%)\n\n  \n  \n  \n    \n      1 n (%)\n    \n    \n      2 Pearson’s Chi-squared test\n    \n  \n\n\n\n\n\n\nAs you can see, there isn’t a significant association between sex and hypertension.\n\n\n\nStep 3: Check if there is a linear association between length of hospitalisation and age\nAge is a potential confounder for a more severe course of TBE involving a longer stay in hospital, for which we would like to adjust. Since age is a continuous variable, we could include it in the regression model in various ways (e.g. as a continuous variable, in categories, by transforming it, etc.). In order to decide this, we need to analyze the association of age with the length of hospitalisation.\n\nStep 3.1: Inspect a potential linear association\nPlease first have a look at the relationship between age and length of hospitalisation using a scatterplot.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n    \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\nWhat do you think? Is there a linear association? How can you be sure? Add a linear model trend line to help you with the interpretation.\n\n\n Click to read a hint\n\n\nFor the trend line, you can add a geom_smooth() geometry. Look up the documentation for geom_smooth (you can type ?geom_smooth() in the console and press “Enter”) and search for the methods option.For a linear trend line you can assign the methods argument to “lm” (linear model). \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\nNow you have visual evidence of a linear association of age with the duration of hospitalisation. Therefore, it seems reasonable to include age as a continuous variable in the analysis.\n\n\nStep 3.2: Check if the association between age and length of hospitalisation varies by sex\nNow, check visually whether the association between age and hospd differs by sex\n\n\n Click to read a hint\n\n\nYou may choose to create separate graphs adding a facet_grid() to your ggplot() (try looking up the syntax yourself). You may also decide to use colour coding to differentiate between factor levels of sex. For the latter, where do you think you should specify the colour, inside or outside the aes()? Read this section of the EpiRhandbook if you have doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# As separate graphs\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd)) +    \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  facet_grid(~sex)  +                   # adding this function will generate a separate graph for each category of sex\n\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n# Same graphs with different colours\ntbe %&gt;%    \n  \n  ggplot(mapping = aes(x = age,        # we put length of hospitalisation on the y-axis because this axis usually contains the dependent variable; and here we want to know if hospd depends on age\n                       y = hospd,\n                       colour = sex )) + #we add the colour in the aes so that it varies according to the categories of sex   \n  \n  geom_point() +                       # this geometry will create a scatterplot\n  \n  geom_smooth(method = lm) +           # this geometry will add a trend line. \"lm\" is for \"linear model\"\n  \n  scale_x_continuous(name = \"Age\" , limits = c(0,100)) +          # Format the x-axis to a range between 0 and 100 \n  \n  scale_y_continuous(limits = c(0,70)) +                          # Format the y axis to a range between 0 and 70\n  \n  labs(\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\"\n  ) + \n  \n  \n  theme_bw()                            # Add a pre-defined theme for formatting\n\n\n\n\n\n\n\nWhat do you observe? The lines for female and male patients have different slopes, indicating that the association between age and hospitalisation days is modified by sex.\nWhy does this matter? Since there are different effects of age on the length of the hospitalisation by sex, you may want to control for this.\n\n\n\n\nStep 4: Check if there is a difference in length of hospitalisation by hypertension status\nLet’s now focus on our exposure of interest (hypertension). We can check whether TBE cases with high blood pressure have a significant longer stay in hospital compared with those without high blood pressure either by carrying out a simple statistical test or through univariate regression. We will do both.\n\nStep 4.1: Simple statistical test\nOne way would be to carry out a simple statistical test which enables us to ascertain if there is a significant difference between groups. As you know, the choice of statistical test will be determined by the number of groups that we have and the nature of the variables. If you don’t remember well how to choose the appropriate statistical test have a look at this BMJ article\n\nQUESTION: What simple statistical tests do you think would be most appropriate in this case?\n\n Chi-square or Fisher's exact test ANOVA or Kruskal Wallis Student t test or Mann Whitney/Wilcoxon rank-sum test McNemar's test or Spears Rank\n\n\nIn this case we have independent data, two groups (hypertension yes/no) and a quantitative dependent outcome (length of hospitalisation), so we will choose either a t-test or Wilcoxon rank-sum test (also known as Mann-Whitney). We’d do a t-test if length of hospitalisation is normally distributed and Wilcoxon if it isn’t. We already checked in Step 2.3 that the distribution of hospd looked normal from the histogram, let’s now have a look if the distribution looks normal for both categories of hypertension: hypertension-yes and hypertension-no.\nTask Check visually whether length of hospitalisation is normally distributed for both categories of hypertension.\n\n\n Click to read a hint\n\n\nA simple way to check this is by generating a histogram for hospd using ggplot(). Stratification by hyper is easily done by adding a facet_grid() to your ggplot, as we have already seen with the scatterplot. Looking at the frecuency distribution of “hospd” by “hypertension” can already give us an idea of whether the data is normally distributed or not.However, if we want to to add a normal curve, we’d need to plot the density and not the frequency, as you may rembember from the histogram we built above in Sep 2.3. \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Plot the frecuency distribution by hypertension status\ntbe %&gt;%\n  \n  ggplot(mapping = aes(x = hospd)) +\n  \n    geom_histogram() + \n    \n    facet_grid(~hyper) # add facet_grid() to get a graph for each hyper status\n\n\n\n# Plot the density and add a  normal curve\ntbe %&gt;%                                  \n  \n  ggplot(mapping = aes(x = hospd, fill = hyper)) +  #remember that for bars, fill is is the interior colour     \n  \n    geom_histogram(aes(y = ..density..)) + #here we are telling ggplot2 to display the density and not the freq count\n  \n    facet_grid(~hyper) + # add facet_grid() to get a graph for each hyper status\n\n    #the function below will add the normal curve. \n    stat_function(fun = dnorm,  #The fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(tbe$hospd, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(tbe$hospd, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) + # Identify the colour and line width of the normal curve\n    labs(\n      x = \"Length of hospitalisation in days\",\n      y = \"Density\"\n      )\n\n\n\n\n\n\nFrom the graphs looks like the data is normally distributed, but there is a test we can do to determine this: Shapiro-Wilk test.\nTask: Use the Shapiro test to determine whether length of hospitalisation comes from a normally-distributed population.\n\n\n Click to read a hint\n\n\nYou can choose {base} functions to carry out the Shapiro test (and any other statistical test) or functions from the {rstatix} package. Here we will use the latter, because {rstatix} has a syntax that is compatible with {dplyr}, which can be an advantage. In any case the name of the functions are very similar. For example, using {base} the Shapiro test function is shapiro.test(); and with {rstatix} the same function is written as: shapiro_test(). If you want to know more about how to carry out simple statistical tests in R, read the EpiRhandbook Chapter on Simple statistical tests\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%\n  \n  group_by(hyper) %&gt;% #we first group by out independent or exposure variable\n  \n  \n  shapiro_test(hospd)  # this function performs the Shapiro-Wilk test for all groups separately\n\n\n\n\n\nhyper\nvariable\nstatistic\np\n\n\n\n\nno\nhospd\n0.9946512\n0.2207864\n\n\nyes\nhospd\n0.9870178\n0.3632204\n\n\n\n\n\n\n\n\nThe null hypothesis of Shapiro’test is that data is normally distributed. As our results were non-significant (p&gt;0.05) we cannot reject such null hypothesis. In other words, we can conclude that data is normally distributed.\nNote: A statistician, or an epidemiologist pretending to be a statistician, would feel very uncomfortable by the conclusion written above. When we are unable to reject a null hypothesis we cannot conclude that this hypothesis (in this case that data is normally distributed) is true, but we should simply state that we were unable to reject it. However, in practical terms, by carrying out a Shapiro test we are assuming that the data is normally distributed….so word it as you like, but the consequences are the same.\n\nQUESTION: Now that we know that the data is normally distributed, what test should we carry out?\n\n Student t-test Wilcoxon rank-sum test\n\n\nTask Carry out the statistical test to ascertain if there are significant differences in length of hospitalisation according to hypertension status\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ntbe %&gt;%\n  \n  t_test(hospd ~ hyper)  # we write first our dependent variable and then the exposure one\n\n\n\n\n\n.y.\ngroup1\ngroup2\nn1\nn2\nstatistic\ndf\np\n\n\n\n\nhospd\nno\nyes\n373\n111\n-2.706422\n181.4443\n0.00745\n\n\n\n\n\n\n\n\n\nQUESTION: Is there a significant difference in the length of hospitalisation among patients with and without hypertension?\n\n Yes No\n\n\nNote: In large datasets (as this one) the Shapiro-Wilk test may reject the normality hypothesis even though the deviation from the normal distribution is rather small. In this case you may still choose to calculate the t-test despite the violation of the normality assumption. This is acceptable because for large samples, the error imposed by t-test approximation is negligible.\n\n\nStep 4.2: Univariate linear regression between hospd and hyper\nThe other way was through univariate regression. Carry out a univariate linear regression with length of hospitalisation (hospd) as the dependent variable and hypertension as the independent variable. Assign this model to an object named: hyper_hospd_lm. If this is the first time doing regression in R, have a look at the Univariate regression chapter of the EpiRhandbook.\n\n\n Click to read a hint\n\n\nAs we are doing linear regression, we could use the lm() function from {base} to analyse the association between hypertension and length of hospitalisation. The syntax is: lm(outcome/dependent variable ~ exposure/independent variable, data = dataframe) You can print the model output in a subsequent command using the summary() function. However, the tidy() function from {broom} provides an overview which can be more easily compiled and used in down-stream analyses if needed.\nThe approach described above is the {base} R approach. You can also perform univariate regression analysis using the function tbl_uvregression() from the {gtsummary} package. If you want to explore this alternative approach further read the dedicated EpiRhandbook chapter \n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#First we write the formula (lm stands for linear model) and assign the model to an object\nhyper_hospd_lm &lt;- lm(hospd ~ hyper, data= tbe)\n\n# Prin the detailed output of the model\nsummary(hyper_hospd_lm)\n\n\nCall:\nlm(formula = hospd ~ hyper, data = tbe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-38.324  -9.324  -1.261   9.676  44.676 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  41.3244     0.6991  59.113  &lt; 2e-16 ***\nhyperyes      3.9369     1.4598   2.697  0.00724 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 13.5 on 482 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.01487,   Adjusted R-squared:  0.01282 \nF-statistic: 7.273 on 1 and 482 DF,  p-value: 0.007243\n\n# Print the main model parameters\nresults_hyper_hospd_lm &lt;- tidy(hyper_hospd_lm) \nresults_hyper_hospd_lm\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n41.324397\n0.6990698\n59.113403\n0.0000000\n\n\nhyperyes\n3.936864\n1.4597610\n2.696924\n0.0072434\n\n\n\n\n\n\n\n\n\nQUESTION: Based on the results of this model, is there a significant association between hypertension and hospd?\n\n Yes No\n\n\n\nQUESTION: How much does having hypertension increase length of hospitalisation (in days)?\n\n 3.9 41.3 1.46 1.2\n\n\n\nQUESTION: What % of the variability in length of hospitalisation can be explained by hypertension?\n\n 2.7% 41% 1.3% 3%\n\n\nIn this case the regression equation is: \\(hopsd(predicted)=41.3+3.9⋅hyper\\).\nNow let’s go through the outputs of the model. We will first review the outputs of the tidy() function. As explained before, the advantage of using tidy() is that we can save the output as an object for further manipulation. Also, tidy() keeps what we would normally need in epidemiology for the interpretation of the model:\nWe have four columns and two rows. The rows refer to the intercept (baseline value for hospd when there is NO hypertension: 41.3) and our only exposure/predictor (hypertension). In most cases, you will be interested in the row for hypertension (your predictor) and in the columns estimate and p.value. The estimate of hypertension tells us the estimated change in hospd when you go from NO hypertension to YES hypertension (that’s why it is written as hyperyes). In this case, it’s approximately 3.94, meaning that having hypertension is associated with an increase in 3.94 days of hospitalisation. The column p.value tells us whether this estimate is statistically significant.\nThis is most of what you need to know to be a functional epidemiologist. But if you want to know more, we will explain the rest of the elements of the output of both functions:\nThe column std.error provides an estimate of the variability or uncertainty associated with the estimate. In this case it means that the estimated effect of hypertension on hospitalisation duration (hospd) is expected to vary by about 1.46 days (on average) due to sampling variability. Finally, the statistic column gives us a value of the statistical test (t-test in this case) used to ascertain if the estimate is significantly different from 0 (normally you can ignore this column).\nYou may have noticed that the summary() output has more information than the tidy() output. Here we leave a brief explanation on what each part of the summary() output means:\nCall: This line shows the formula used for the regression model.\nResiduals: These are the differences between the actual hospd values and the predicted values from the regression model. The summary provides statistics like minimum, median, and maximum residuals.\nCoefficients: This is the bit that interests us the most:\n\nIntercept: The estimated intercept (baseline value) for hospd when age is zero. In this case, it’s approximately 41.3.\nhyperyes: The estimated change in hospd when you go from NO hypertension to YES hypertension. Here, it’s approximately 3.9. The t-value and p-value indicate whether this coefficient is statistically significant.\n\nSignificance Codes: Indicate whether the p-value is highly significant (*** p &lt; 0.001) or only marginally significant (* p&lt;0.05)\nResidual Standard Error: This measures the average deviation of the observed hospd values from the regression line. In this case, it’s approximately 13.5.\nMultiple and Adjusted R-squared: These values (0.01487 and 0.01282) represent the proportion of variance in hospd explained by the linear relationship with hypertension. Higher values indicate better fit. The adjusted one is adjusted for the number of predictors. In this case we could say that around 1.3% of the variability in hospd is explained by hypertension.\nF-statistic and p-value: The F-statistic tests whether the overall model (including all predictors) is significant. A low p-value (like this one, &lt; 0.007243) indicates that the model is significant.\n\n\n\nStep 5: Check if there is a linear association between length of hospitalisation and the rest of variables\nWe also need to know if there is a univariate association between our outcome (length of hospitalisation) and the confounders we may want to introduce in our model. We will do this in two steps, first with the continous variable and then with the factor variables.\n\nStep 5.1: Univariate linear regression between length of hospitalisation and age\nTry running a linear regression model with length of hospitalisation (hospd) as the dependent variable and age as the independent variable.\n\nQUESTION: Based on the results of the model, is there a significant association between age and hospd?\n\n Yes No\n\n\n\nQUESTION: How much does an additional year of age increase length of hospitalisation (in days)?\n\n 0.372 24.5 0.271 1.42\n\n\n\nQUESTION: What % of the variability in length of hospitalisation can be explained by age?\n\n 65% 15% 27% 99%\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Run the linear regression and assign the output to age_hospd_lm\nage_hospd_lm &lt;- lm(hospd ~ age, data = tbe)\n\n# Print the results using the summary() function\nsummary(age_hospd_lm)\n\n\nCall:\nlm(formula = hospd ~ age, data = tbe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-35.676  -7.893   0.521   6.844  32.279 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  24.5314     1.4233   17.24   &lt;2e-16 ***\nage           0.3722     0.0278   13.39   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.61 on 482 degrees of freedom\n  (39 observations deleted due to missingness)\nMultiple R-squared:  0.2711,    Adjusted R-squared:  0.2696 \nF-statistic: 179.2 on 1 and 482 DF,  p-value: &lt; 2.2e-16\n\n# Print the results of the regression analysis with the tidy() function\ntidy(age_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.5314214\n1.4232588\n17.23609\n0\n\n\nage\n0.3722366\n0.0278031\n13.38831\n0\n\n\n\n\n\n\n\n\n\n\nStep 5.2: Univariate linear regression between length of hospitalisation and factor variables\nNow carry out a similar univariate model for each of the factor variables that we have not explored to ascertain which ones are significantly associated with length of hospitalisation.\n\nQUESTION: What of the following variables have a significant linear association with length of hospitalisation?\n\n Sex Other comorbidities Monophasic disease Large tick at removal Vaccinated against TBE\n\n\n\n\n Click to read a hint\n\n\nWe have seen already how to write a linear formula with lm(). You can then use either summary() from {base} or tidy() from {broom} to look at the coefficient and significance value. You could write these functions for each variable, but you could also try to use map() from {purrr} to do them all at once, as we did in the Step 2.2. Have a look at the Chapter on Purrr in the EpiRhandbook if you have any doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n## Here we write one by one for each variable the formula and tidy\n\n#sex\nsex_hospd_lm &lt;- lm(hospd ~ sex, data = tbe)\ntidy(sex_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n46.66559\n0.6932755\n67.31176\n0\n\n\nsexfemale\n-12.41704\n1.1595927\n-10.70810\n0\n\n\n\n\n\n#other comorbidities\nother_hospd_lm &lt;- lm(hospd ~ other, data = tbe)\ntidy(other_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n39.590909\n1.014237\n39.035162\n0.0000000\n\n\notheryes\n4.142857\n1.271413\n3.258466\n0.0011993\n\n\n\n\n\n#vaccination\nvac_hospd_lm    &lt;- lm(hospd ~ vac, data = tbe)\ntidy(vac_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n42.556989\n0.6262426\n67.956072\n0.0000000\n\n\nvacyes\n-8.399095\n3.1607381\n-2.657321\n0.0081379\n\n\n\n\n\n#monophasic course\nmono_hospd_lm   &lt;- lm(hospd ~ mono, data = tbe)\ntidy(mono_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n40.690566\n0.8291071\n49.077574\n0.0000000\n\n\nmonoyes\n3.396192\n1.2325691\n2.755377\n0.0060844\n\n\n\n\n\n#large tick at removal\ntick_hospd_lm   &lt;- lm(hospd ~ tick, data = tbe) \ntidy(tick_hospd_lm)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n42.731250\n0.7593945\n56.270158\n0.0000000\n\n\ntickyes\n-1.487348\n1.3045725\n-1.140103\n0.2548094\n\n\n\n\n\n## Here we do it with {purrr} all in one command\ntbe %&gt;% \n  select(-hyper, -age, -hospd) %&gt;% # we first remove the columns we have explored before + the outcome column\n  map(.f = ~lm(hospd ~ .x, data = tbe)) %&gt;%   # here we carry out the model for each column. \".x\" represents all the column\n  map(tidy)                                   # finally we do tidy on each model\n\n$vac\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    42.6      0.626     68.0  4.69e-249\n2 .xyes          -8.40     3.16      -2.66 8.14e-  3\n\n$mono\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    40.7      0.829     49.1  1.30e-189\n2 .xyes           3.40     1.23       2.76 6.08e-  3\n\n$other\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    39.6       1.01     39.0  2.43e-151\n2 .xyes           4.14      1.27      3.26 1.20e-  3\n\n$tick\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)    42.7      0.759     56.3  5.50e-214\n2 .xyes          -1.49     1.30      -1.14 2.55e-  1\n\n$sex\n# A tibble: 2 x 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)     46.7     0.693      67.3 2.99e-247\n2 .xfemale       -12.4     1.16      -10.7 3.79e- 24\n\n\n\n\nStatistically significant (\\(p&lt;0.05\\)) univariable effects were observed for sex, other comorbidities, vaccination and monophasic disease course but not for large tick at removal. Women had on average 12.4 days shorter hospitalisation stay than men, other comorbidities increase the hospitalisation by 4.1 days, vaccination reduced it by 8.4 days, and monophasic disease course increased it by 3.4 days.\n\n\n\nStep 6: Multivariable analysis\nTo control for possible confounding, we will adjust for the minimum adjustment set that we have identified through the DAG analysis (Goal 1), adding each variable in different steps.\n\nStep 6.1: Add age as a covariate to the model\nAs a first step, add age to the model of the main effect.\n\n\n Click to read a hint\n\n\nWe have already calculated and saved the model of our main effect (hyper_hospd_lm). You can add age to this model by creating a new model and adding the additional variable, so that the formula syntax is model &lt;- lm(hospd ~ hyper + newVariable, data = data)\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Creating a new model and adding age \nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age, data = tbe)\n\ntidy(hyper_hospd_adj_lm)                                   \n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n23.6918245\n1.437666\n16.479366\n0.0000000\n\n\nhyperyes\n3.8015966\n1.245007\n3.053473\n0.0023875\n\n\nage\n0.3715581\n0.027567\n13.478364\n0.0000000\n\n\n\n\n\n\n\n\nAdjusting for age, we find that hypertension is associated with a 3.8 increase in the days of hospitalisations. This is only a slight change compared to the effect in the unadjusted model (where the coefficient was 3.9).\n\n\nStep 6.2: Compare the performance of both models\nWe have the univariate model (hyper_hospd_lm), and now we have one adjusted by age (hyper_hospd_adj_lm). Compare both models calculating model performance metrics such as AIC and logLik.\n\n\n Click to read a hint\n\n\nOne easy function to calculate performance metrics is glance() from the {broom} package. But, of course, there are many others. Choose the one you feel most comfortable with!\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nglance(hyper_hospd_lm)   # performance metrics of the univariate model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.0148657\n0.0128219\n13.50128\n7.273399\n0.0072434\n1\n-1945.512\n3897.024\n3909.57\n87861.17\n482\n484\n\n\n\n\n\nglance(hyper_hospd_adj_lm) # performance metrics of the adjusted model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.2849348\n0.2819616\n11.51466\n95.83297\n0\n2\n-1867.974\n3743.948\n3760.676\n63774.52\n481\n484\n\n\n\n\n\n\n\n\nThe output of glance() gives us several performance metrics. If you want to focus on the important bits, you can look at the logLik and the AIC. A larger logLik means a better model; and a lower AIC means also a better model. So, if your logLik goes up and your AIC goes down, you are going in the right direction. The advantage of the logLik is that there is a formal test (Likelihood Ratio Test) to test if two logLiks are significantly different. However, AIC takes more things into consideration than the logLik, such as the complexity of the model (favours simpler models), so it is very useful to compare models with different numbers of parameters (like this case).\nIf you want to know more, let’s review each element of the output given by glance():\n\nr.squared: This is a measure of how well the independent variables in your model explain the variability of the dependent variable. Think of it as a percentage that tells you how much of the changes in the outcome can be predicted by the model. So, in our example, hypertension alone explains 1.5% of the variability in length of hospitalisation, but when we add age, that model explains 28.5% of the variability in our dependent variable.\nadj.r.squared: The adjusted R-squared value adjusts the R-squared value based on the number of predictors in the model. It is a more accurate measure when comparing models with different numbers of predictors (as in this case). Here, it is slightly lower at 28.2%, indicating a small adjustment for the number of predictors.\nsigma: This represents the residual standard error, which is the standard deviation of the residuals.\nstatistic: This is the F-statistic value for the overall significance of the model. It tests whether at least one predictor variable has a non-zero coefficient.\np.value: The p-value associated with the F-statistic.\ndf: Degrees of freedom associated with the model which typically correspond to the number of predictors (1 in the first case and 2 in the second).\nlogLik: The log-likelihood of the model, which is a measure of the model fit. Higher values indicate a better fit. As stated above, one of the main performance measures that you will use as an epidemiologist.\nAIC: Akaike Information Criterion, which is used for model comparison. Lower values indicate a better model.\nBIC: Bayesian Information Criterion, similar to AIC but with a higher penalty for models with more parameters.\ndeviance: This is a measure of the goodness of fit of the model. Lower values indicate a better fit.\ndf.residual: The residual degrees of freedom, which is the number of observations minus the number of parameters estimated.\nnobs: The number of observations used in the model, which is 484 in our case\n\n\n\n(Optional) Step 6.3: Plotting effects of age by hypertension\nIf you remember, in Step 3.2 we checked visually for the association between age and length of hospitalisation by sex. Now, we will do something similar by plotting the estimated effect of age on length of hospitalisation by hypertension status. The final output that we want is a scatterplot displaying the raw data (length of hospitalisation by age), and then the fitted regression line by hypertension status.\n\n\n Click to read a hint\n\n\nIn order to plot the estimated effects of age by hypertension we have to extract the fitted values of “hospd” from the model and add them to the dataframe. This only works if we have removed all rows that contain NAs in the hospd column before. we can then pass this new dataframe to a ggplot() with a geom_point() for the scatterplot and geom_line() for the fitted trend lines. Note: Previously, we used geom_smooth() to add trend lines to the scatterplot. Because we want to plot the predicted values of an existing model and not fit a linear model within ggplot(), we have to use geom_line() and use the predicted values of our model as input for y.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# First add a variable with the fitted values to the dataframe\ndata_fitted &lt;- tbe %&gt;%\n  \n  filter(!is.na(hospd)) %&gt;%   # Remove NAs \n\n  mutate(fit = predict(hyper_hospd_adj_lm)) # Create a new variable with the predicted values\n\n### You can now open data_fitted to have a look at the new column\n\n# Now we do the plot with the fitted lines\n\nplot_fitted &lt;- data_fitted %&gt;%\n  \n  ggplot(mapping = aes(x = age, \n                       y = hospd,\n                       colour = hyper)) +\n  \n  geom_point() +                            # Adding a scatter plot\n  \n  geom_line(aes(y = fit)) +                 # we add a line specifying that we want the fitted and not the observed values in the y-axis\n\n  labs(\n    title = \"Effect of age on TBE hospital stay length for people\\nwith and without hypertension \", # Title of the plot, note that \"\\n\" breaks the title into the next line\n    x = \"Age\",\n    y = \"Length of hospitalisation in days\",\n    colour = \"Hypertension\"\n  ) +\n  \n  theme_bw()                                  # we add a predefined theme\n\nplot_fitted\n\n\n\n\n\n\nWe did not allow the association between age and hospitalisation to vary by levels of hypertension. So we force them to be parallel by the way we specify the model. If we want the slopes to differ, we would need to allow for an interaction term. We’ll get to that later.\n\n\nStep 6.4: Continue building the adjusted model and check quality of the model\nContinue adding the rest of variables that we have to adjust for according to the DAG (TBE vaccination, age, large tick, monophasic course, other comorbidities and sex) to the model hyper_hospd_adj_lm one by one. Check, after you add each variable, model performance metrics to ensure that the model is improving.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n### We first add sex to the previous model\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n28.9661798\n1.3613386\n21.277718\n0.000000\n\n\nhyperyes\n3.4659484\n1.1079250\n3.128324\n0.001865\n\n\nage\n0.3452202\n0.0246333\n14.014368\n0.000000\n\n\nsexfemale\n-11.0377082\n0.9762639\n-11.306071\n0.000000\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4353144\n0.4317851\n10.24315\n123.3435\n0\n3\n-1810.837\n3631.674\n3652.584\n50362.62\n480\n484\n\n\n\n\n\n### TBE vaccination\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n29.0192105\n1.3398585\n21.658415\n0.0000000\n\n\nhyperyes\n3.5401913\n1.0905445\n3.246260\n0.0012512\n\n\nage\n0.3507362\n0.0242813\n14.444681\n0.0000000\n\n\nsexfemale\n-10.9105961\n0.9613218\n-11.349577\n0.0000000\n\n\nvacyes\n-9.6218919\n2.3644063\n-4.069475\n0.0000551\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.454185\n0.4496271\n10.08105\n99.6467\n0\n4\n-1802.611\n3617.223\n3642.315\n48679.6\n479\n484\n\n\n\n\n\n### Monophasic disease course\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n27.9231475\n1.3632885\n20.482199\n0.0000000\n\n\nhyperyes\n3.7307298\n1.0799901\n3.454411\n0.0006006\n\n\nage\n0.3443505\n0.0240868\n14.296254\n0.0000000\n\n\nsexfemale\n-11.0608203\n0.9517650\n-11.621378\n0.0000000\n\n\nvacyes\n-9.8262935\n2.3391721\n-4.200757\n0.0000317\n\n\nmonoyes\n3.1330906\n0.9155900\n3.421936\n0.0006751\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4672362\n0.4616634\n9.970207\n83.84164\n0\n5\n-1796.755\n3607.509\n3636.784\n47515.6\n478\n484\n\n\n\n\n\n### Large tick at removal\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono + tick, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n28.1079813\n1.3968015\n20.1231029\n0.0000000\n\n\nhyperyes\n3.7655402\n1.0821694\n3.4796219\n0.0005482\n\n\nage\n0.3446256\n0.0241066\n14.2959288\n0.0000000\n\n\nsexfemale\n-11.0239613\n0.9542624\n-11.5523372\n0.0000000\n\n\nvacyes\n-9.7114161\n2.3481138\n-4.1358371\n0.0000418\n\n\nmonoyes\n3.0852716\n0.9194697\n3.3554903\n0.0008555\n\n\ntickyes\n-0.5959864\n0.9677751\n-0.6158315\n0.5382995\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4676595\n0.4609634\n9.976687\n69.8405\n0\n6\n-1796.562\n3609.124\n3642.581\n47477.85\n477\n484\n\n\n\n\n\n### Other comorbidities\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + age + sex + vac + mono + tick + other, data = tbe) # formula\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n27.1251926\n1.4596417\n18.5834593\n0.0000000\n\n\nhyperyes\n2.7139444\n1.1769928\n2.3058292\n0.0215491\n\n\nage\n0.3406119\n0.0240754\n14.1477140\n0.0000000\n\n\nsexfemale\n-11.0797108\n0.9506756\n-11.6545649\n0.0000000\n\n\nvacyes\n-9.2298015\n2.3484899\n-3.9301005\n0.0000975\n\n\nmonoyes\n3.0995048\n0.9157172\n3.3847838\n0.0007712\n\n\ntickyes\n-0.7577796\n0.9665465\n-0.7840074\n0.4334257\n\n\notheryes\n2.3008449\n1.0351078\n2.2228070\n0.0266978\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4731284\n0.4653803\n9.935728\n61.06371\n0\n7\n-1794.063\n3606.126\n3643.765\n46990.1\n476\n484\n\n\n\n\n\n\n\n\nAs you can see, adding the rest of variables identified through the DAG improved the model, according to the metrics. The only exception was the addition of the variable “Large tick at removal”. However, the choice of covariates should be made conceptually through the DAG, and not based on whether there are small differences in AIC/logLik. If you concluded that this variable was a possible confounder, then keep it in the model.\n\n\nStep 6.5: Adding an interaction term\nWe saw in Step 3.2 that the effect of age on length of hospitalisation is modified by sex. We knew this because the slopes were different for males and females. Given this information add now to the model an interaction term between sex and age.\n\n\n Click to read a hint\n\n\nAn interaction can be written as a multiplication of two variables in your regression formula, e.g. age*sex. There is no need to keep both variables individually in the formula AND in the interaction term. By just including the interaction term, R will estimate the coefficients independently for both variables and with the interaction.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nhyper_hospd_adj_lm &lt;- lm(hospd ~ hyper + vac + mono + tick + other + age*sex, data = tbe) # formula\n\ntidy(hyper_hospd_adj_lm)                                # estimates\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n24.2675847\n1.6629366\n14.5932114\n0.0000000\n\n\nhyperyes\n2.9092258\n1.1650411\n2.4971014\n0.0128588\n\n\nvacyes\n-9.0596630\n2.3224314\n-3.9009389\n0.0001096\n\n\nmonoyes\n2.9451978\n0.9064522\n3.2491487\n0.0012396\n\n\ntickyes\n-0.6988387\n0.9557594\n-0.7311868\n0.4650256\n\n\notheryes\n2.2322254\n1.0235851\n2.1807913\n0.0296887\n\n\nage\n0.3998845\n0.0293313\n13.6333864\n0.0000000\n\n\nsexfemale\n-3.0511594\n2.5045520\n-1.2182456\n0.2237354\n\n\nage:sexfemale\n-0.1728331\n0.0499755\n-3.4583541\n0.0005924\n\n\n\n\n\nglance(hyper_hospd_adj_lm)                              # performance metrics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.4860689\n0.4774132\n9.823278\n56.15605\n0\n8\n-1788.045\n3596.09\n3637.911\n45835.97\n475\n484\n\n\n\n\n\n\n\n\n\nQUESTION: Looking at the estimates of the model, for females, what is the net effect of age on length of hospitalisation?\n\n +0.4 +0.23 -0.17 +0.17 -0.32\n\n\nLooking at the results, we see that age has an estimate of 0.4, which means that one additional year of age is associated with an increase in 0.4 days of hospitalisation. We also see that being female is associated with a decrease in 3 days of hospitalisation (compared with being male). The interaction estimate of -0.17 means that for females, each additional year of age is associated with a decrease in 0.17 days of hospitalisation. So, to sum up, all things equal, a female of 41 years, compared to one of 40 years, would have +0.4 days of hospitalisation for having one extra year, but as she is female you have to add the interaction term (-0.17), so the net effect would be +0.23. For males, one extra year would still be associated with +0.4 days of hospitalisation.\nWe also see that adding the interaction term changes the effect estimate for hypertension and improves the model, so we may want to keep the interaction in the model, assuming that this ensures a better control for confounding of “age” and “sex”.\nNote: You should also look for other effect interaction terms in the dataframe. In the interest of the exercise, we will skip this step.\n\n\n\nStep 7: Model diagnostics\nThe multiple linear regression model that we just run, like all models, has several assumptions:\n\nLinearity: The relationship between the independent variables and the dependent variable (length of hospitalisation) is linear. This means that the change in the dependent variable is proportional to the change in the independent variables.\nIndependence: The observations are independent of each other.\nNormality of Residuals: The residuals of the model are normally distributed, meaning that they are symmetrically distributed around 0 and the residuals’ distribution is bell-shaped.\nHomoscedasticity: The residuals (difference between the observed values and the values estimated from the model) have constant variance at every level of the independent variables. This means that the spread of the residuals should be roughly the same across all levels of the independent variables.\nNo Multicollinearity: The independent variables are not highly correlated with each other. High multicollinearity can make it difficult to determine the individual effect of each independent variable on the dependent variable.\nNo Autocorrelation: The residuals are not correlated with each other. This is particularly important in time series data where observations are ordered in time.\n\nIn this case study we will learn how to check two of these assumptions: Normality of residuals and Homoscedasticity\n\nStep 7.1: Normality assumption\nTo assess whether the residuals of our model are normally distributed or not we will do these three things:\n\nGenerating some diagnostic plots of the model\nCreating an histogram with the residuals\nCarrying out a Shapiro-test\n\nTask: Generate diagnostic plots of the model hyper_hospd_adj_lm. Don’t worry if you don’t find out how to do these. If you struggle, have a look at the hint below.\n\n\n Click to read a hint\n\n\nYou can retrieve diagnostic plots by using the function autoplot() from {ggfortify}. Pass your model object to the autoplot() function to generate three basic diagnostic plots. You can also specify which plot autoplot() needs to generate using the which = argument. In the solution, we specify that we want the first 2 plots.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nautoplot(hyper_hospd_adj_lm, which = 1:2)\n\n\n\n\n\n\nLet’s interpret each graph:\n\nResiduals vs. fitted plot (plot 1, upper left corner): You see a scatterplot with a blue line (LOESS smoothing curve). If the residuals are normally distributed around zero, then the blue line will be horizontal (parallel to the zero line).\nQ-Q plot (plot 2, upper right plot): The Q-Q plot shows you whether or not the residuals are normally distributed. On the X-axis you can see the quantiles of a normal distribution, while on the Y-axis you can see the standardized residuals (residuals dived by their standard deviation). If the residuals are normally distributed, in general, most of the points need to lie on the diagonal.\n\n\nQUESTION: Are residuals normally distributed, based on these plots?\n\n No Yes\n\n\nTask: Build an histogram with the residuals of the hyper_hospd_adj_lm model.\n\n\n Click to read a hint\n\n\nTo do an histogram of the residuals you can follow a similar approach than the one we used in the step 6.3. That is, first adding to the tbe dataframe the residuals as a column (you can use the function resid()) and then using ggplot() to plot this column in the x-axis of a geom_histogram()\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# First add a variable with the residual values to the dataframe\ndata_res &lt;- tbe %&gt;%\n  \n  filter(!is.na(hospd)) %&gt;%   # Remove NAs \n  \n  mutate(res = resid(hyper_hospd_adj_lm)) # Create a new variable with the residual values\n\n### You can now open data_res to have a look at the new column\n\n# Now we do the histogram with the residuals\n\ndata_res %&gt;%\n  \n  ggplot(mapping = aes(x = res)) +\n  \n  geom_histogram(aes(y = ..density..)) +                        # Adding a histogram with density\n  \n  #the function below will add the normal curve. \n  stat_function(fun = dnorm,  #The fun = argument we are specifying that we want the normal curve         \n                args = list(mean = mean(data_res$res, na.rm = T), #to draw a normal curve we need to give the mean and standard deviation of our column\n                            sd   = sd(data_res$res, na.rm = T)),  \n                \n                col = \"darkblue\", lwd = 1) +\n  labs(\n    title = \"Histogram of Residuals\", \n    x = \"Residuals\",\n    y = \"Frequency\"\n  ) +\n  \n  theme_bw()                                  # we add a predefined theme\n\n\n\n\n\n\n\nQUESTION: Does the histogram suggest a normal distribution of residuals?\n\n No Yes\n\n\nTask: Carry out a Shapiro-Wilk test to ascertain statistically if residuals are normally distributed\n\n\n Click to read a hint\n\n\nReview Step 4.1 and/or have a look at the EpiRhandbook Chapter on Simple statistical tests if you still have doubts.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\ndata_res %&gt;%\n  shapiro_test(res)\n\n\n\n\n\nvariable\nstatistic\np\n\n\n\n\nres\n0.9960796\n0.2762974\n\n\n\n\n\n\n\n\n\nQUESTION: How do you interpret the result of the Shapiro-Wilk test?\n\n Residuals do not follow a normal distribution We cannot reject the null hypothesis that residuals follow a normal distribution We accept the alternative hypothesis\n\n\n\n\nStep 7.2: Homoscedasticity\nNow we will check the other assumption, that the variance of the residuals (i.e., the magnitude of their distance to 0) does not depend on the predictor.\nCheck homoscedasticity by running plot number 3 (Scale-location plot) in the autoplot() function.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Generate the scale-location plot\nautoplot(hyper_hospd_adj_lm, which = 3)\n\n\n\n\n\n\nThis plot is called a scale-location plot and represents the fitted values vs. the square root of the standardized residuals. With this plot it is possible to check for heteroscedasticity (i.e., heterogeneity of variance - the opposite of homoscedasticity). Ideally, the residuals should be evenly spread out across all levels of fitted values. This means there should be no clear pattern or systematic change in the spread of residuals as you move along the x-axis. If the residuals form a funnel shape (narrow at one end and wide at the other), this indicates heteroscedasticity, meaning the variance of the residuals is not constant. The blue smoothed line should be roughly horizontal and close to zero. Significant deviations from this line can indicate issues with homoscedasticity.\n\nQUESTION: Based on this graph, do you think that the assumption of homoscedasticity is reasonably met?\n\n No Yes\n\n\n\n\n\nStep 8: Public health relevance\n\nDiscuss the PH relevance of your findings and any next steps and recommendations that may result from this.\n\n\n\nClick to see a solution (try it yourself first!)\n\n\nThis is the first time that an association between hypertension and severity of TBE (based on the duration of hospitalisation) has been detected. Even though you suspected such an association in the beginning (and thus tested for it) and even though you think that it may be causal based on your DAGs and the control for confounding, there are more steps involved in establishing a causal relationship. We should follow the Bradford Hill Criteria to argue causality. For example, the biological basis for this effect needs to be established. Similar effects have been seen for other infections (e.g., SARS-CoV-2), which may have promoted the hypothesis in the beginning. You may also decide to look for a possible dose-response relationship in further analysis of our data. Furthermore, these results should be repeated independently to rule out a chance finding. Once the causal relationship between hypertension and TBE is further corroborated, vaccination against TBE for persons with hypertension could be recommended."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html",
    "href": "pages/openxlsx2_tutorial-en_revised.html",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nopenxlsx2 tutorial\n\n\nLanguage:\nEnglish\n\n\nTools:\nR, MS Excel\n\n\nLocation:\nN/A\n\n\nScale:\nN/A\n\n\nDiseases:\nN/A\n\n\nKeywords:\nR, Excel, Report, Export, Format, openxls2, Tutorial\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nIntermediate\n\n\n\nAuthorship\nOriginal authors: Leonel Lerebours and Alberto Mateo Urdiales\nData source: None (Example data will be generated with R)\n\n\n\n\n\n\nThere are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\n\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\n\nYou may use the tutorial to learn how to generate reports using R by creating tables and exporting them in MS Excel for visualization with the openxlsx2 package, for educational purposes, and to apply the learned techniques to your personal or professional projects. This tutorial might be freely translated, copied, or distributed. No warranty is made or implied for use of the software for any particular purpose.\n\n\n\n\n\nYou can write feedback and suggestions on this tutorial at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nJuly 2023\nFirst draft\n1.0\nLeonel Lerebours\n\n\nSeptember 2024\nRevision first draft\n1.1\nAlberto Mateo Urdiales\n\n\n\n\n\n\n\n\n\n\n\n\nThe main focus of the tutorial is to use the core functions of the openxlsx2 up to the version 1.8. You must have install MS Excel (or software equivalent like OpenOffice) to visualize the output tables.\nThe data for this tutorial will be generated randomly (any resemble with real data is totally coincidence).\n\n\n\n\n\n\n\n\n\nThe goal of this tutorial is to introduce you in the use of openxlsx2 to export formatted tables in MS Excel.\n\n\n\n\nIt is recommended to have intermediate R skills and have at least a basic knowledge of dplyr (from tidyverse package) like pipe operators and data wrangling. Here some reference.\nEpidemiological experience (e.g., knowledge of how to design outputs tables for reporting purpose)\n\n\n\n\n\nCreate a folder named “openxls2_tutorial” in your laptop\nCreate an Rstudio project in the folder openxls2_tutorial. If you are unsure on how to do that, read the EpiRhandbook on R projects\nYou must have installed MS Excel (or software equivalent like OpenOffice) in your laptop to visualize the output tables."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#overview",
    "href": "pages/openxlsx2_tutorial-en_revised.html#overview",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "",
    "text": "Case study characteristics\n\n\n\n\n\nName:\nopenxlsx2 tutorial\n\n\nLanguage:\nEnglish\n\n\nTools:\nR, MS Excel\n\n\nLocation:\nN/A\n\n\nScale:\nN/A\n\n\nDiseases:\nN/A\n\n\nKeywords:\nR, Excel, Report, Export, Format, openxls2, Tutorial\n\n\nTechnical complexity:\nIntermediate\n\n\nMethodological complexity:\nIntermediate\n\n\n\nAuthorship\nOriginal authors: Leonel Lerebours and Alberto Mateo Urdiales\nData source: None (Example data will be generated with R)"
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#instructions",
    "href": "pages/openxlsx2_tutorial-en_revised.html#instructions",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "",
    "text": "There are several ways to get help:\n\nLook for the “hints” and solutions (see below)\nPost a question in Applied Epi Community with reference to this case study\n\n\n\n\nHere is what the “helpers” look like:\n\n\n\n\n Click to read a hint\n\n\nHere you will see a helpful hint!\n\n\n\n\n\nClick to see the solution\n\n\n\nebola_linelist %&gt;% \n  filter(\n    age &gt; 25,\n    district == \"Bolo\"\n  )\n\nHere is more explanation about why the solution works.\n\n\n\n\n\n\n\n… description here about posting in Community… TO BE COMPLETED BY APPLIED EPI\n\n\n\n\nYou may use the tutorial to learn how to generate reports using R by creating tables and exporting them in MS Excel for visualization with the openxlsx2 package, for educational purposes, and to apply the learned techniques to your personal or professional projects. This tutorial might be freely translated, copied, or distributed. No warranty is made or implied for use of the software for any particular purpose.\n\n\n\n\n\nYou can write feedback and suggestions on this tutorial at the GitHub issues page\nAlternatively email us at: contact@appliedepi.org\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nChanges made\nVersion\nAuthor\n\n\n\n\nJuly 2023\nFirst draft\n1.0\nLeonel Lerebours\n\n\nSeptember 2024\nRevision first draft\n1.1\nAlberto Mateo Urdiales\n\n\n\n\n\n\n\n\n\n\n\n\nThe main focus of the tutorial is to use the core functions of the openxlsx2 up to the version 1.8. You must have install MS Excel (or software equivalent like OpenOffice) to visualize the output tables.\nThe data for this tutorial will be generated randomly (any resemble with real data is totally coincidence)."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#guidance",
    "href": "pages/openxlsx2_tutorial-en_revised.html#guidance",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "",
    "text": "The goal of this tutorial is to introduce you in the use of openxlsx2 to export formatted tables in MS Excel.\n\n\n\n\nIt is recommended to have intermediate R skills and have at least a basic knowledge of dplyr (from tidyverse package) like pipe operators and data wrangling. Here some reference.\nEpidemiological experience (e.g., knowledge of how to design outputs tables for reporting purpose)\n\n\n\n\n\nCreate a folder named “openxls2_tutorial” in your laptop\nCreate an Rstudio project in the folder openxls2_tutorial. If you are unsure on how to do that, read the EpiRhandbook on R projects\nYou must have installed MS Excel (or software equivalent like OpenOffice) in your laptop to visualize the output tables."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#why-use-ms-excel-for-reporting",
    "href": "pages/openxlsx2_tutorial-en_revised.html#why-use-ms-excel-for-reporting",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Why use MS Excel for reporting ?",
    "text": "Why use MS Excel for reporting ?\nExcel is one of the most popular softwares for data analysis, data visualization and many other capabilities, since Excel’s formatting options allow users to adjust fonts, colors, borders, and alignment to create visually appealing reports. MS excel use is very common in many areas, including epidemiological tasks that involve creating reports.\nIn some ways, MS Excel -as other point-and-click tools- is easy to learn, since it lets you “interact” with the data. For example, if we want to do quick calculations and produce a summary table, or if we want to modify a graph and to compare it with other previous reports.\nIf you have experience working in routine reporting -such as working with epidemiological surveillance-, probably you or a co-worker use or have used at some point Excel or any other spreadsheet software like OpenOffice to present tables and summaries.\nHowever, even with all the great perks that Excel has, it is somewhat hard to automate a report with Excel even using a template with a pre-designed format. Is also time consuming to edit a spreadsheet every time you create a table or a graph (specially without knowledge of macros) and prone to errors."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#automating-a-report-in-excel-with-r-using-openxlsx2-package",
    "href": "pages/openxlsx2_tutorial-en_revised.html#automating-a-report-in-excel-with-r-using-openxlsx2-package",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Automating a report in Excel with R using openxlsx2 package",
    "text": "Automating a report in Excel with R using openxlsx2 package\nAs described in the CRAN documentation of the openxlsx2 package the main purpose of this package is to simplify the creation of ‘xlsx’ files by providing a high level interface to writing, styling and editing worksheets.\nIn this short tutorial we are going to create and format a summary report from scratch in R without touching Excel or any other spreadsheet software."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#step-1-getting-ready",
    "href": "pages/openxlsx2_tutorial-en_revised.html#step-1-getting-ready",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Step 1: Getting ready",
    "text": "Step 1: Getting ready\n\nStep 1.1: Create a new R script\nOnce you have created an Rproject inside the “openxls2_tutorial” folder (as specified in the second point of the section Preparation for the case study). Create a new script with the name openxls2_tutorial.R and save it in the subfolder “openxls2_tutorial”.\n\n\nStep 1.2: Install/load packages\nAs you probably know, the first part of our script (besides including -commented- some information about the aim, author, date last updated and contact details) is to install and load packages. Fortunately, there is a package that does this task very effectively: {pacman}. The function p_load() from this package will install any packages listed not already installed and will load them. If a listed package had already been installed, it will just load it. You can find more about installing/loading packages in the Packages section of the EpiRhandbook.\nUsing this approach, try to install and load the following packages: janitor, openxlsx2 and tidyverse.\n\n\n Click to read a hint\n\n\nYou may end up using a long list of packages. Unfortunately different packages have functions with the same name. For example, the package {dplyr} (already installed with {tidyverse}) has a function called select() which we frequently use to subset columns of a data frame. But other packages such as {MASS} do also have a function called select(). This could create headaches if you want to subset columns using dplyr’s select() but R thinks you’re calling MASS’s select() (we call this masking - dplyr’s select() is masked by MASS’s select()). Given that you are more likely to use functions from {tidyverse}, ensure that this is the last package in your p_load() list so that functions from {tidyverse} (including {dplyr} functions) will always “prevail”.\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# Ensures the package \"pacman\" is installed\nif (!require(\"pacman\")) {\n     install.packages(\"pacman\") }\n\n# install (if necessary) from CRAN and load packages to be used\npacman::p_load(\n  janitor,    # data cleaning and tables\n  openxlsx2,  # create xlsx files \n  lubridate,  # to manage dates\n  tidyverse  # data management and visualization\n)"
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#step-2-create-a-fake-dataset",
    "href": "pages/openxlsx2_tutorial-en_revised.html#step-2-create-a-fake-dataset",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Step 2: Create a fake dataset",
    "text": "Step 2: Create a fake dataset\nBefore start using the functions of openxlsx2, we need to decide on what we want into the exported report (i.e., how many tables, what type of tables and which data inside.)\nFor this tutorial, the scenario is to do a summary of the microbiology laboratories’ production, including:\n\nhow many samples were received\nhow many were confirmed\nwhen the samples had a confirmed diagnosis\n\nWe will not be using real data for this tutorial, but we will create it with R. Feel free to create it with MS Excel if you feel more comfortable.\nTask: Generate a fake data frame of 1000 observations using the following variables:\n\ndate: from January 01, 2022 to December 31st, 2024.\nlaboratories: a categorical variable with five categories that have values from “A” to “E”.\nn_samples: The Number of samples received. a numerical variable (ramdon number from 0 to 100).\nn_confirmed: The Number of samples with positive results. A numerical variable calculated as a proportion of the previous variable, with a range that goes from 0.02 to 0.05.\npct_confirmed: The percentage of samples confirmed (n_samples/n_confirmed)\n\n\n\n Click to read a hint\n\n\nOne way to create the dataframe is to use the function tibble() which comes when you install {tidyverse}. Inside tibble() you can create the specified columns using the sample() function which will create values randomly once you have specified the date rage, the categories etc. If you are unsure on how to do this read the documentation of the tibble function or this post from R bloggers which may help you. If you still do not manage, have a look at the solution. Don’t worry, this may be the most difficult part of the case study!\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# we first use this function which will ensure that, even though the dataframe will be generated with random values, we will ALWAYS get the same values if we rerun the script. The number inside is not relevant\n\nset.seed(1300) \n\n# Create dataframe for the example using 1000 observations\n\ndb &lt;- tibble(\n  \n  # Random dates\n  \n   date=sample(seq(as.Date(\"2022-01-01\"),           # the begging of the period\n                  as.Date(\"2024-12-31\"),            # the end of the period\n                  by=\"day\"),                        # the time interval\n              replace = TRUE,                          # setting replace to true means that each date can be chosen to be in the sample more than once.\n              1000),                                # this is the number of observations\n  \n   # Random Labs (A to E)\n   \n   laboratories=sample(LETTERS[c(1:5)],             # we are saying that this columns should have letters from A (1) to E (5)       \n                      replace = TRUE, \n                      1000),\n  \n  # Random samples (by day and lab)\n  \n   n_samples=sample(c(1:100),                       \n                   replace = TRUE, \n                   1000)) %&gt;% \n  \n  #Random confirmed samples\n  mutate(\n    \n    n_confirmed=round(sample(seq(                  # the round function is used to round numbers. In this case the multiplication may mean that we have decimals\n      from=0.01, \n      to=0.05, \n      by=0.001), \n      replace=T,\n      1000)*n_samples,\n      digits = 0),                                 # setting digits to 0 is saying to round it to the nearest whole number\n    \n    # % positivity\n    pct_confirmed=n_confirmed/n_samples       \n    \n    )\n\n#take a look at the db created\nhead(db)\n\n# A tibble: 6 × 5\n  date       laboratories n_samples n_confirmed pct_confirmed\n  &lt;date&gt;     &lt;chr&gt;            &lt;int&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n1 2024-09-04 B                   15           0        0     \n2 2023-02-27 C                   90           1        0.0111\n3 2023-06-02 A                   88           4        0.0455\n4 2023-02-19 B                    1           0        0     \n5 2024-06-15 D                   29           1        0.0345\n6 2022-06-24 C                   43           2        0.0465\n\n\n\n\n Note: If you want to create a dummy database with Excel, (or you want to use your data) you will have to import your file. To do that, you can use the function import() from the {rio} package or the function read_xlsx() from the openxlsx2 package. Read the dedicated EpiRhandbook Chapter for importing data if you have any doubts."
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#step-3-transfoming-the-data-and-creating-the-summary-tables",
    "href": "pages/openxlsx2_tutorial-en_revised.html#step-3-transfoming-the-data-and-creating-the-summary-tables",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Step 3: Transfoming the data and creating the summary tables",
    "text": "Step 3: Transfoming the data and creating the summary tables\nAs you can see, with this simple fake dataframe of 5 columns we may want to know :\n\nHow many samples were reported by month each year.\nWhat is the proportion of the confirmed samples by month.\nWhat is the proportion of samples reported by each laboratory.\nThe overall positivity rate by laboratory.\n\n\nStep 3.1: Summary table with number of samples received\nLet’s start by creating a table with the number of samples received by month each year. We want a table that has a nice formatting to put in a report, so we want a column with the year of reporting and one column for each calendar month. We also want to add the totals.\nWe will achieve this in to separate steps:\nTask 1: Aggregate the number of samples received (column n_samples) by month and year. Called the new column with the aggregated number tot_samples\n\n\n Click to read a hint\n\n\nThere are different ways to aggregate data. We suggest you use the function group_by() alongside summarise(), which is a {dplyr} approach. If you have never used this approach or if you have doubts, read the this section of the EpiRhandbook. In any case, you’ll need to create columns for the month and the year using the date column. You can do this using the functions month() and year().\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#table for samples by year and months\n\n#table for samples by year and months\n\ntotal_sample_tab &lt;- db %&gt;% \n  \n  # we first create a column named months and years which are the respective months and years of the column \"date\"\n  \n  mutate(months = month(date,label=T), # the argument label = T ensures that months have the name of the month and not their number\n         years  = year(date)) %&gt;% \n  \n  # we group by the new columns\n  group_by(years, months) %&gt;% \n  \n  # and we create a new column by adding by each group the number of samples received\n  summarise(tot_samples = sum(n_samples), .groups = \"drop\") # the .groups = \"drop\" argument ungroup the data, which is always advisable after we finish aggregation\n\n\n\n#check the new data frame\nhead(total_sample_tab)\n\n# A tibble: 6 × 3\n  years months tot_samples\n  &lt;dbl&gt; &lt;ord&gt;        &lt;int&gt;\n1  2022 Jan           1587\n2  2022 Feb           1439\n3  2022 Mar           1099\n4  2022 Apr            906\n5  2022 May           1378\n6  2022 Jun           1646\n\n\n\n\nThe new dataframe has one column for years, one for months and one for the total number of samples received. We call this the long format. This format is useful for further analysis/visualisation (creating a plot), but here what we want is to export this into a nice formatted table, so we want to the months to go in the columns. In other words, we want our dataframe in a wide format.\nTask 2: Pivot your data from long to wide so that you have one column per month. Add the totals.\n\n\n Click to read a hint\n\n\nOne way to do this is to use the function pivot_wider(). You need to specify where the names of the columns will come from and where the values will come from. If you are not familiar with pivoting or if you have doubts, spend some time in this section of the EpiRhandbook\n\n\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# pivoting from long to wide\ntotal_sample_tab &lt;- total_sample_tab %&gt;% \n  pivot_wider(names_from = months,        # so the names of the columns will be the months\n              values_from = tot_samples,  # and the values the number of samples\n              values_fill = 0) %&gt;%        # if there were no data in a given month, it would fill it with 0\n  \n  adorn_totals(c(\"col\", \"row\"))           # we add the sum of the totals both by row and column\n\ntotal_sample_tab\n\n years  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec Total\n  2022 1587 1439 1099  906 1378 1646 1640  799  955 1422  976 1550 15397\n  2023 1391 1488 1390 1870 1236 1488 1737 1097 1320 1642 1581 1647 17887\n  2024 1339 1156 1526 1188 1274 1609 1455 1342 1974 1226 1626 1974 17689\n Total 4317 4083 4015 3964 3888 4743 4832 3238 4249 4290 4183 5171 50973\n\n\n\n\n\n\nStep 3.2: Summary table with positivity percentage\nTask: Following a similar approach to the one we used in the previous step, create a table showing, by year and month, the positivity percentage of the samples\n\n\nClick to see a solution (try it yourself first!)\n\n\n\npositivity_tab &lt;- db %&gt;% \n  \n  # we first create a column named months and years which are the respective months and years of the column \"date\"\n  \n  mutate(months = month(date,label=T), # the argument label = T ensures that months have the name of the month and not their number\n         years  = year(date)) %&gt;% \n  \n  # we group by the new columns\n  group_by(years, months) %&gt;% \n  \n  # we create  new columns by adding by each group the number of samples received, the total confirmed and calcualte percentage\n  summarise(tot_samples = sum(n_samples),\n          tot_confirmed = sum(n_confirmed),\n          pct = round(tot_confirmed/tot_samples, digits = 5), .groups = \"drop\") %&gt;% # round can be use to round up numbers. in this case to only 1 decimal\n  \n  #we select the columns we're interested\n  select(years, months, pct) %&gt;% \n  \n  # we pivot them into wide format\n  pivot_wider(names_from = months,\n              values_from = pct, \n              values_fill = 0)\n\npositivity_tab\n\n# A tibble: 3 × 13\n  years    Jan    Feb    Mar    Apr    May    Jun    Jul    Aug    Sep    Oct\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1  2022 0.0309 0.0285 0.0319 0.0342 0.0268 0.0316 0.0287 0.0263 0.0304 0.0267\n2  2023 0.0367 0.0289 0.0295 0.0267 0.0235 0.0343 0.0311 0.0228 0.0296 0.0311\n3  2024 0.0261 0.0294 0.0242 0.0295 0.0330 0.0311 0.0261 0.0276 0.0314 0.0286\n# ℹ 2 more variables: Nov &lt;dbl&gt;, Dec &lt;dbl&gt;\n\n\n\n\n\n\nStep 3.3: Summary table by laboratory\nTask: Create a summary table for each year and laboratory with the total number of samples, the number of samples confirmed and the positivity percentage, as well as the total by rows.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nlaboratory_summary &lt;- db %&gt;% \n  \n  # we first create a column named years which are the years of the column \"date\"\n  \n  mutate(years=year(date)) %&gt;% \n  \n  # we group by year and laboratory \n  group_by(years, laboratories) %&gt;% \n  \n  # we create new columns with the total number of samples received and the total number of samples confirmed\n  summarise(total_samples=sum(n_samples),\n          total_confirmed=sum(n_confirmed), .groups = \"drop\") %&gt;% \n  \n  # we arrange our table by laboratory and year\n  arrange(laboratories, years) %&gt;% \n  \n  # we add the totals by row\n  adorn_totals(\"row\") %&gt;% \n  \n  # we add a column with the positivity percentage by row\n  mutate(positivity_rate= round(total_confirmed/total_samples, digits = 5)) %&gt;% \n  \n  # we rename the columns\n  rename(\"Years of reporting\"=years,\n         \"Laboratories\"=laboratories,\n         \"Total Samples\"=total_samples,\n         \"Confirmed Samples\"=total_confirmed,\n         \"% of confirmed samples\"=positivity_rate)\n\nlaboratory_summary \n\n Years of reporting Laboratories Total Samples Confirmed Samples\n               2022            A          4202               135\n               2023            A          3638                98\n               2024            A          3997               110\n               2022            B          2795                76\n               2023            B          3759               104\n               2024            B          3800               119\n               2022            C          2441                68\n               2023            C          3425               111\n               2024            C          3045                90\n               2022            D          2730                75\n               2023            D          2863                83\n               2024            D          3553               106\n               2022            E          3229               102\n               2023            E          4202               127\n               2024            E          3294                93\n              Total            -         50973              1497\n % of confirmed samples\n                0.03213\n                0.02694\n                0.02752\n                0.02719\n                0.02767\n                0.03132\n                0.02786\n                0.03241\n                0.02956\n                0.02747\n                0.02899\n                0.02983\n                0.03159\n                0.03022\n                0.02823\n                0.02937"
  },
  {
    "objectID": "pages/openxlsx2_tutorial-en_revised.html#step-4-export-the-tables-to-excel",
    "href": "pages/openxlsx2_tutorial-en_revised.html#step-4-export-the-tables-to-excel",
    "title": "Creating reports with R and MS Excel: a tutorial using the openxls2 package (EN)",
    "section": "Step 4: Export the tables to excel",
    "text": "Step 4: Export the tables to excel\nNow that we have the tables for the summary report, lets do a overview of the main functions of openxlsx2 :\n\nMain functions\n\nwb_workbook(): to create a new workbook\nwb_add_worksheet(): to add worksheets (name, zoom level and gridlines)\nwb_add_data(): to add either a dataframe, a table, text string a single value\nwb_save(): to export the workbook to a file (Excel format)\nwb_open(): really handy to open right away the workbook in Excel (to see the results of the code)\n\n\n\nRelated to formating\n\nwb_add_font(): to specify font type for a region\nwb_add_border(): to add borders to a region\nwb_add_cell_style(): to add specific style to a region (wrap, vertical/horizontal/left/right alignment,\nwb_add_numfmt(): to add specific number style to a cells range\nwb_add_fill(): to add fill color to a region\nwb_set_col_widths(): to setup the width of specific columns\nwb_set_row_heights(): to setup the heights of specific rows\nwb_merge_cells(): to merge a range of cells\n\nSome of the functions use parameters to specify coordinates in the spreadsheet numbers (as columns and row) and others use dimensions, which are the combination of letters for columns and number for letters.\n\n\nRelated to location / dimension / coordenates\nThe following functions are really important to get the coordinates of where to apply specific formatting, since most of the functions to add format/style need a range (in row and columns). With these we can obtain Excel type coordinates of specify a region (such as A1:B30) based on the size of the tables and the location in the worksheet of the table that we want to export.\nrowcol_to_dims(): to create a vector with Excel’s coordinates from the rows and columns numbers you provide (example: rowcol_to_dims(col= c(1:3), row= c(1:3)) will result in “A1:C3”). You can combine the use of nrow() and ncol() to get the size of a table and get the Excel’s coordinates, depending where you want to place the table. Is important to know beforehand the position of a table (number of row and column) since the starting row and column so the format we want to apply fall in the range we want.\nwb_dims(): to get a dimension of an object (dataframe or matrix) for the spreadsheet. It start from coordinate A1 by default, for example if we use wb_dims(x=mtcars) will return the value of “A1:K33”. This helper function\n\n\n\nGeneral approach, you can create an example with all the formats in excel first too\n\n\nNow that we have some dataframes and see a quick overview of the main or most used functions from openxlsx, lets start to create the following output in excel:\n\nFirst lets see the “style” we want to add to the outputs and also the dimensions of the columns and rows where we want to apply a format. Like we do when we work in Excel directly.\nFor example in the picture above we can observe the title in the middle of each tables is at the center and the text is larger and bold. The first row with the column’s title has also bold text and has top and bottom borders. The last row, Total, has also bold text and has top bolder. In all of the cells the text is centered. In the top table the text of the body is in number format and the last row too. In the bottom table, the text of the body and last row are in percentage format.\nWe have to take into account these details to start creating the vectors with the ranges. There are formats that applies for the whole table, others for just part of the tables.\nAlso, it is very important to know where the tables are going to be located and if it is a table that is going to grow over time (from top to bottom or left to right).\n\n\nStep 4.1: Set the table positions\nThe first column and row of the top table (Table with the total samples received) in the exercise have the starting position at cell B3, whilst the bottom table (The positivity table) starts at cell B10.\nThe first row of the first table (Table with the total samples received) goes from cell B3 to cell O3, the last row range is B7 to O7. For the bottom table (The positivity table) the first row range is B10 to N10 and the bottom row is from B14 to N14.\nFor the vector with the ranges, we have to specify the columns as number instead of letters. the rowcol_to_dims() function will convert the numbers coordinates to the Excel range format (such as A1:B3). With the combination of the function ncol() and nrow() and the starting position we can get the range of the whole table.\nTask: Use the function rowcol_to_dims() to obtain the range of the columns for the Table with the total samples received (B3:07). Assign this range to the name tab1_dimres. Get also the range for the same table’s title (B2:B2)\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#get the table 1 (Table with the total samples received) dimensions\n\ntab1_dimres &lt;- rowcol_to_dims(\n  # 1:ncol() will give us the a range for the number of columns of the table. By adding one we are specifying that we want the range to start in B instead of A\n  \n  col = 1:ncol(total_sample_tab) + 1, \n  \n  # similarly 3:nrow() will create a range that starts in 3 and will go until nrow() + 3, whic is 7\n  row = 3:(nrow(total_sample_tab) + 3))\n\ntab1_dimtitle &lt;- rowcol_to_dims(col = 2, row = 2)\n\n#get the first row and last row dimension of table 1 (Table with the total samples received)\ntab1_dimfirstrow&lt;- rowcol_to_dims(col = 1:ncol(total_sample_tab)+1, row=3)\n\ntab1_dimlastrow &lt;- rowcol_to_dims(col = 1:ncol(total_sample_tab)+1, row=nrow(total_sample_tab)+3)\n\n\n\nSince the table1 (Table with the total samples received) is a table that will grow over the time from top to bottom, the starting position of the table 2 (The positivity table) must depend on the starting position of the first table, if we want to dinamically setup the starting position of The positivity table.\nLets create a vector with the sequence of numbers of the rows where the The positivity table will be located based on the position of table 1 and the spaces between Table with the total samples received and The positivity table.\nTable with the total samples received has 7 rows nrow(total_sample_tab) and the starting position in the spreadsheet is row 3, and we want the second table starts two rows after the end of table 1. So, the starting position row of table 2 is the sum of the numbers of rows of table 1 + the starting position of table 1 + the rows after table 1 last row.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n#vector with the row for the starting position of table 2\n\ntab2_row_start &lt;- nrow(total_sample_tab)+3+3\n\ntab2_row_seq &lt;- seq(from= nrow(total_sample_tab)+3+3, \n                    to=nrow(positivity_tab)+nrow(total_sample_tab)+3+3, \n                    by=1)\n\n\ntab2_dimres &lt;- rowcol_to_dims(col = 1:ncol(positivity_tab)+1, #the columns are not affected\n                              row = tab2_row_seq)\n\n#get the first row and last row dimension of table 2 (The positivity table)\n\ntab2_dimtitle &lt;- rowcol_to_dims(\n  col = 2, \n  row = tab2_row_start - 1)\n\ntab2_dimfirstrow&lt;- rowcol_to_dims(\n  col =1:ncol(positivity_tab) + 1, \n  row=tab2_row_start)\n\ntab2_dimlastrow &lt;- rowcol_to_dims(\n  col = 1:ncol(positivity_tab) + 1, \n  row  =nrow(positivity_tab) + tab2_row_start)\n\ntab2_dimbody_pct &lt;- rowcol_to_dims(\n  col = 2:ncol(positivity_tab) + 1, \n  row = 1:nrow(positivity_tab) + tab2_row_start)\n\n\n\nWe are getting there!, now that we have the ranges where we just need to format it.\n\n\nStep 4.2: Creating the table in MS Excel\nThe next step now is to start using the {openxlsx2} functions to:\n\nAdd the tables\nAdd the formatting/styles\nSave the workbook into a Excel file\n\nTask: Looking at the {openxlsx2} functions described in the section Main functions have a go at:\n\nCreate a workbook object\nAdd at least one worksheet\n\nBelow you have one possible solution that we propose\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nwb_main &lt;- wb_workbook(\"Laboratory_Data\") %&gt;%                    #creating the workbook\n\nwb_add_worksheet(\"lab summary\",                                  #adding the first worksheet\n\ngridLines = FALSE,                                               #remove gridlines\n\ntabColor = \"red\") %&gt;%                                            #tab color in the spreadsheet\n\nwb_add_data(x=\"Samples reported by month and year (All laboratories)\", #adding the title of table 1\n\nstart_col = 2,                                                   #title starting in col 2 or \"B\" \n\nstart_row = 2) %&gt;%                                               #title starts in row 2 as well\n\nwb_merge_cells(rows=2,                                           #merging the row above Table 1\n\ncols = 1:ncol(total_sample_tab) + 1) %&gt;%                         #column range of Table 1            \n \nwb_add_data(x=total_sample_tab, start_col  = 2,                  #data starts in column 2 and row 3\n\nstart_row =3 ) %&gt;%\n\nwb_add_data(x=\"Sample positivity by month and year (All Laboratories)\", #add title of table 2\n            start_col = 2,\n            \n            start_row = tab2_row_start-1) %&gt;%\n  \nwb_merge_cells(rows= tab2_row_start - 1, #merging the row above Table 2\n\ncols = 1:ncol(positivity_tab)+1) %&gt;%\n\nwb_add_data(x=positivity_tab, start_col  = 2,\n\nstart_row =tab2_row_start, na.strings =\"-\" )\n\nwb_open(wb_main) # to see a preview\n\n\n\nWe can see so far what we have, almost done! \n\n\nStep 4.3: Formatting the table\nNo we just need to format the table.\nTask: Format the table using the {openxlsx2} functions described in the Related to formatting section.\nBelow is our proposed solution.\n\n\nClick to see a solution (try it yourself first!)\n\n\n\nwb_main_w_styles &lt;- wb_main %&gt;%\n  \n  wb_add_cell_style( 1, tab1_dimres,\n\n  vertical = \"center\",\n  \n  horizontal = \"center\",\n  \n  wrapText = 1) %&gt;%\n  \n  wb_add_cell_style(1 , tab1_dimtitle,\n  \n  vertical = \"center\",\n  \n  horizontal = \"center\") %&gt;% \n  \n  wb_add_border(1, tab1_dimfirstrow, \n                \n                top_border = \"thick\", \n                \n                top_color = wb_color(hex = \"000000\"),\n                \n                bottom_border = \"thick\", \n                \n                bottom_color = wb_color(hex = \"000000\"),\n                \n                inner_vgrid = \"none\",\n                \n                left_border=\"none\",\n                \n                right_border = \"none\") %&gt;% \n    \n  wb_add_font(dims = tab1_dimfirstrow,\n              \n              bold = \"double\") %&gt;% \n  \n  wb_add_font(dims = tab1_dimtitle,\n              \n              size=12,\n                \n              bold = \"double\") %&gt;% \n  \n  wb_add_font(dims = tab1_dimlastrow,\n              \n              size=11,\n              \n              bold=\"dobule\") %&gt;% \n  \n  wb_add_border(dims = tab1_dimlastrow,\n                \n                top_border = \"thick\",\n                \n                top_color = wb_color(hex = \"000000\"),\n                \n                inner_vgrid = \"none\",\n                \n                left_border=\"none\",\n                \n                right_border = \"none\",\n                \n                bottom_border = \"none\") %&gt;% \n\n  wb_add_numfmt(dims = tab1_dimlastrow,\n                \n                numfmt = \"#,0\") %&gt;% \n  \n  wb_add_numfmt(dims = rowcol_to_dims(row=1:nrow(total_sample_tab)+3,\n                                      col=ncol(total_sample_tab)+1),\n                \n                numfmt = \"#,0\") %&gt;% \n  \n wb_set_col_widths(cols = 2, widths = 20) %&gt;% \n  \n wb_set_row_heights(rows = c(2, 3,nrow(total_sample_tab)+3), heights = 30 )  %&gt;%\n  \n  wb_add_cell_style(1, tab2_dimres,\n\n  vertical = \"center\",\n\n  horizontal = \"center\",\n\n  wrapText = 1) %&gt;%\n\n  wb_add_cell_style(1, tab2_dimtitle,\n\n  vertical = \"center\",\n\n  horizontal = \"center\") %&gt;% \n\n  wb_add_border(1, tab2_dimfirstrow, \n              \n              top_border = \"thick\", \n              \n              top_color = wb_color(hex = \"000000\"),\n              \n              bottom_border = \"thick\", \n              \n              bottom_color = wb_color(hex = \"000000\"),\n              \n              inner_vgrid = \"none\",\n              \n              left_border=\"none\",\n              \n              right_border = \"none\") %&gt;% \n  \n  wb_add_font(dims=tab2_dimfirstrow,\n\n              bold=\"double\") %&gt;%\n\n  wb_add_font(dims=tab2_dimtitle,\n\n              size=12,\n\n              bold=\"double\") %&gt;%\n  \n  wb_add_border(dims = tab2_dimlastrow,\n\n                top_border = \"none\",\n\n                inner_vgrid = \"none\",\n\n                left_border=\"none\",\n\n                right_border = \"none\",\n\n                bottom_color = wb_color(hex = \"000000\"),\n\n                bottom_border = \"thick\") %&gt;%\n  \n\n   \n  wb_add_numfmt(dims = tab2_dimbody_pct,\n\n                numfmt = \"#.0%\") %&gt;%\n\n wb_set_col_widths(cols = 2, widths = 20) %&gt;%\n\n wb_set_row_heights(rows = c(tab2_row_start-1,tab2_row_start,nrow\n                             (positivity_tab)+tab2_row_start), heights = 30 ) %&gt;%\n \n  wb_open() # to open a temporary file \n\n\n\nIf all functions and commands are correct, you should see something like the following output:\n\n\n\nThe formated tables in Excel\n\n\n\n\nStep 4.4: Exporting the table\nThe last step is to export the table and save it in our computer.\nTask: Using the function wb_save(), export the tables into your local computer\n\n\nClick to see a solution (try it yourself first!)\n\n\n\n# save the output into an excel file\nwb_save(wb_main_w_styles, \"Lab_tables.xlsx\", overwrite = T) \n\n\n\n\nDepending on how complex your report is, you can add more worksheets. Iteration (like using map() from purrr) could help you to automate several reports from different provinces or geographical units. Have a look at the Iteration chapter of the EpiRhandbook if you want to explore this further.\n\nThere are other great functions to add more details or expand the format of the tables, for example adding conditional format, or sparklines. This tutorial covers just the main functions and what is the most common task to add formats. For more in depth formatting, please review the openxlsx2 following vignette\nThanks for reading this tutorial!"
  },
  {
    "objectID": "pages/under_construction.es.html",
    "href": "pages/under_construction.es.html",
    "title": "Sitio web en construcción",
    "section": "",
    "text": "Sitio web en construcción\n\n\n\n\n\n\nEN CONSTRUCCIÓN\n\n\n\nEsta página está en desarrollo. El contenido y la URL cambiarán.\n\n\nPara obtener instrucciones sobre cómo utilizar nuestros estudios de casos, consulte nuestra How-to Guide. Agradecemos sus comentarios y sugerencias en contact@appliedepi.org. También puede debatir sobre el estudio de caso o conceptos relacionados en la Applied Epi Community."
  }
]